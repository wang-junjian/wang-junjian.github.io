---
layout: single
title:  "llama.cpp 实战指南（Jetson Thor 平台）：从源码编译到 GGUF 模型部署与性能基准测试"
date:   2025-10-15 08:00:00 +0800
categories: Jetson LLM
tags: [Jetson, Thor, Benchmark, llama.cpp, llama-server, gguf, Qwen3, gpt-oss, LLM, NVIDIA]
---

本文将介绍如何在 Jetson Thor 平台上编译、部署和测试 llama.cpp 项目中的 GGUF 格式的大模型。

<!--more-->

## [源码编译](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md)

### 克隆 [llama.cpp](https://github.com/ggml-org/llama.cpp)

```
git clone https://github.com/ggml-org/llama.cpp.git
cd llama.cpp
```

### [CUDA GPU Compute Capability（计算能力）](https://developer.nvidia.com/cuda-gpus)

`计算能力（CC）`定义了每种 NVIDIA GPU 架构的**硬件特性**和**支持的指令**。在下表中查找您的GPU的计算能力。

![](/images/2025/Jetson/llama-server/cc-110.png)

### 编译

```bash
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES="110"
cmake --build build --config Release -j $(nproc)
```


## 模型部署
### 运行 llama-server

#### [Qwen3-Coder-30B-A3B-Instruct-GGUF](https://www.modelscope.cn/models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF)

```bash
./build/bin/llama-server \
    --model /models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf \
    --alias qwen3 \
    --host 0.0.0.0 \
    --port 8000 \
    --jinja \
    --reasoning-format none \
    --gpu-layers -1 \
    --ctx-size 0 \
    --threads $(nproc) \
    --flash-attn on \
    --no-kv-offload \
    --no-op-offload \
    --no-mmap \
    --mlock
```

#### [gpt-oss-120b-GGUF](https://www.modelscope.cn/models/ggml-org/gpt-oss-120b-GGUF)
- [gpt-oss-120b & gpt-oss-20b Model Card](https://www.modelscope.cn/papers/2508.10925)
- [guide : running gpt-oss with llama.cpp #15396](https://github.com/ggml-org/llama.cpp/discussions/15396)
- [llama : add gpt-oss #15091](https://github.com/ggml-org/llama.cpp/pull/15091)

```bash
./build/bin/llama-server \
    --model /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
    --alias qwen3 \
    --host 0.0.0.0 \
    --port 8000 \
    --jinja \
    --gpu-layers -1 \
    --ctx-size 0 \
    --parallel $(nproc) \
    --threads $(nproc) \
    --flash-attn on \
    --no-kv-offload \
    --no-op-offload \
    --no-mmap \
    --mlock
```

### llama-server 命令行参数解释

#### 模型路径与别名设置

*   **`.llama-server`**
    *   这是 `llama.cpp` 项目中用于启动 REST API 服务的可执行文件 [i]。
*   **`--model /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf`**
    *   使用此选项（`-m, --model FNAME`）指定要加载的模型文件的路径。
*   **`--alias qwen3`**
    *   使用此选项（`-a, --alias STRING`）**为模型名称设置别名**，该别名将用于 REST API。

#### 服务配置

*   **`--host 0.0.0.0`**
    *   指定服务器要监听的 IP 地址。默认是 `127.0.0.1`。
*   **`--port 8000`**
    *   指定服务器要监听的端口。默认是 `8080`。
*   **`--jinja`**
    *   启用此标志，**在聊天中将使用 Jinja 模板**。默认情况下，此功能是禁用的。
*   **`--reasoning-format none`**
    *   此选项（`-rf, --reasoning-format FORMAT`）用于**控制是否允许以及如何从模型的响应中提取“思维标签”**。
    *   当将此参数设置为 none 时，模型生成的思维内容（thoughts）将保持未解析状态，并保留在 message.content 字段中。 默认值是 `none`。

#### 资源与并发控制

*   **`--gpu-layers -1`**
    *   此选项（`-ngl, --gpu-layers, --n-gpu-layers N`）设置**存储在 VRAM 中的最大层数**。
    *   设置为 `-1` 通常意味着尝试将所有可能的层加载到 VRAM 中。
*   **`--ctx-size 0`**
    *   此选项（`-c, --ctx-size N`）设置提示上下文的大小。
    *   **当设置为 `0` 时，上下文大小将从模型中加载**。默认值是 4096。
*   **`--parallel $(nproc)`**
    *   此选项（`-np, --parallel N`）设置**并行解码的序列数量**。
    *   `$(nproc)` 是一个 shell 命令，表示将并行序列数设置为系统可用的处理器核心数。默认值是 1。
*   **`--threads $(nproc)`**
    *   此选项（`-t, --threads N`）设置**在生成过程中使用的线程数量**。
    *   `$(nproc)` 意味着将线程数设置为系统可用的处理器核心数。

#### 性能优化与内存管理

*   **`--flash-attn on`**
    *   此选项（`-fa, --flash-attn [on|off|auto]`）**设置 Flash Attention 的使用**。
    *   在此配置中，它被显式设置为“开启”（`on`）。
*   **`--no-kv-offload`**
    *   此选项（`-nkvo, --no-kv-offload`）用于**禁用 KV (Key/Value) 卸载**。
*   **`--no-op-offload`**
    *   此选项用于**禁用将主机张量操作卸载到设备**（默认值为 `false`）。
*   **`--no-mmap`**
    *   此选项用于**不进行内存映射模型**。这会导致加载速度变慢，但如果未使用 `mlock`，则可能减少换页。
*   **`--mlock`**
    *   此选项用于**强制系统将模型保留在 RAM 中**，而不是进行交换或压缩。


## 性能测试

### 模型性能分析

#### 模型规格对比

![](/images/2025/Jetson/llama-server/models.png)

#### PP512 性能对比

![](/images/2025/Jetson/llama-server/pp512-performance.png)

#### TG128 性能对比

![](/images/2025/Jetson/llama-server/tg128-performance.png)

#### 模型大小 vs PP512 性能分析

![](/images/2025/Jetson/llama-server/model-size_pp512-performance.png)

#### 模型大小 vs TG128 性能分析

![](/images/2025/Jetson/llama-server/model-size_tg128-performance.png)

### gpt-oss-120b-GGUF

- **MXFP4**

```bash
./build/bin/llama-bench -m /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf
```

```bash
  Device 0: NVIDIA Thor, compute capability 11.0, VMM: yes
| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | CUDA       |  99 |           pp512 |        533.06 ± 7.20 |
| gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | CUDA       |  99 |           tg128 |         39.95 ± 0.03 |
```

### Qwen3-Coder-30B-A3B-Instruct-GGUF

- **Q4_K_M**

```bash
./build/bin/llama-bench -m /models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf
```

```bash
| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3moe 30B.A3B Q4_K - Medium |  17.28 GiB |    30.53 B | CUDA       |  99 |           pp512 |        959.77 ± 7.22 |
| qwen3moe 30B.A3B Q4_K - Medium |  17.28 GiB |    30.53 B | CUDA       |  99 |           tg128 |         52.95 ± 0.08 |
```

- **Q5_K_M**

```bash
./build/bin/llama-bench -m /models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf
```

```bash
| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3moe 30B.A3B Q5_K - Medium |  20.23 GiB |    30.53 B | CUDA       |  99 |           pp512 |        946.36 ± 4.90 |
| qwen3moe 30B.A3B Q5_K - Medium |  20.23 GiB |    30.53 B | CUDA       |  99 |           tg128 |         54.55 ± 0.08 |
```

- **Q6_K**

```bash
./build/bin/llama-bench -m /models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf
```

```bash
| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3moe 30B.A3B Q6_K          |  23.36 GiB |    30.53 B | CUDA       |  99 |           pp512 |        863.80 ± 5.21 |
| qwen3moe 30B.A3B Q6_K          |  23.36 GiB |    30.53 B | CUDA       |  99 |           tg128 |         47.76 ± 0.05 |
```

### Qwen3-8B-GGUF

- **Q4_K_M**

```bash
./build/bin/llama-bench -m /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q4_K_M.gguf
```

```bash
| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3 8B Q4_K - Medium         |   4.68 GiB |     8.19 B | CUDA       |  99 |           pp512 |       1256.90 ± 0.40 |
| qwen3 8B Q4_K - Medium         |   4.68 GiB |     8.19 B | CUDA       |  99 |           tg128 |         40.07 ± 0.02 |
```

- **Q5_0**

```bash
./build/bin/llama-bench -m /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_0.gguf
```

```bash
| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3 8B Q5_0                  |   5.32 GiB |     8.19 B | CUDA       |  99 |           pp512 |       1353.25 ± 0.66 |
| qwen3 8B Q5_0                  |   5.32 GiB |     8.19 B | CUDA       |  99 |           tg128 |         39.22 ± 0.02 |
```

- **Q5_K_M**

```bash
./build/bin/llama-bench -m /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_K_M.gguf
```

```bash
| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3 8B Q5_K - Medium         |   5.44 GiB |     8.19 B | CUDA       |  99 |           pp512 |       1239.98 ± 0.97 |
| qwen3 8B Q5_K - Medium         |   5.44 GiB |     8.19 B | CUDA       |  99 |           tg128 |         36.13 ± 0.02 |
```

- **Q6_K**

```bash
./build/bin/llama-bench -m /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf
```

```bash
| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3 8B Q6_K                  |   6.26 GiB |     8.19 B | CUDA       |  99 |           pp512 |       1121.56 ± 0.30 |
| qwen3 8B Q6_K                  |   6.26 GiB |     8.19 B | CUDA       |  99 |           tg128 |         31.35 ± 0.97 |
```

- **Q8_0**

```bash
./build/bin/llama-bench -m /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q8_0.gguf
```

```bash
| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3 8B Q8_0                  |   8.11 GiB |     8.19 B | CUDA       |  99 |           pp512 |       1413.13 ± 0.80 |
| qwen3 8B Q8_0                  |   8.11 GiB |     8.19 B | CUDA       |  99 |           tg128 |         28.29 ± 0.25 |
```


## 参考资料
- [ggml-org 模型库](https://www.modelscope.cn/organization/ggml-org?tab=model)
- [Unsloth AI 模型库](https://huggingface.co/unsloth/models)
