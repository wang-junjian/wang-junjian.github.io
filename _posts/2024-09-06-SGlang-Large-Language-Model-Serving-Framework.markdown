---
layout: post
title:  "SGLang å¤§æ¨¡å‹æœåŠ¡æ¡†æ¶"
date:   2024-09-06 08:00:00 +0800
categories: SGlang LLM
tags: [SGlang, vLLM, FlashInfer, CUDA]
---

## SGLang
SGLang is a fast serving framework for large language models and vision language models. It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.

[SGLang](https://github.com/sgl-project/sglang) æ˜¯ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„å¿«é€ŸæœåŠ¡æ¡†æ¶ã€‚é€šè¿‡ååŒè®¾è®¡åç«¯è¿è¡Œæ—¶å’Œå‰ç«¯è¯­è¨€ï¼Œä½¿æ‚¨ä¸æ¨¡å‹çš„äº¤äº’æ›´å¿«é€Ÿã€æ›´å¯æ§ã€‚

The core features include:

æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š
- **Fast Backend Runtime**: Efficient serving with RadixAttention for prefix caching, jump-forward constrained decoding, continuous batching, token attention (paged attention), tensor parallelism, FlashInfer kernels, and quantization (AWQ/FP8/GPTQ/Marlin).
- **å¿«é€Ÿåç«¯è¿è¡Œæ—¶**ï¼šé€šè¿‡ RadixAttention å®ç°é«˜æ•ˆçš„æœåŠ¡ï¼Œæ”¯æŒå‰ç¼€ç¼“å­˜ï¼ˆprefix cachingï¼‰ã€å—é™è·³è½¬å‰ç¼€è§£ç ï¼ˆjump-forward constrained decodingï¼‰ã€è¿ç»­æ‰¹å¤„ç†ï¼ˆcontinuous batchingï¼‰ã€ä»¤ç‰Œæ³¨æ„åŠ›(åˆ†é¡µæ³¨æ„åŠ›)ï¼ˆtoken attention (paged attention)ï¼‰ã€å¼ é‡å¹¶è¡Œï¼ˆtensor parallelismï¼‰ã€FlashInfer å†…æ ¸å’Œé‡åŒ–ï¼ˆAWQ/FP8/GPTQ/Marlinï¼‰ã€‚
- **Flexible Frontend Language**: Enables easy programming of LLM applications with chained generation calls, advanced prompting, control flow, multiple modalities, parallelism, and external interactions.
- **çµæ´»çš„å‰ç«¯è¯­è¨€**ï¼šé€šè¿‡é“¾å¼ç”Ÿæˆè°ƒç”¨ï¼ˆchained generation callsï¼‰ã€é«˜çº§æç¤ºï¼ˆadvanced promptingï¼‰ã€æ§åˆ¶æµï¼ˆcontrol flowï¼‰ã€å¤šæ¨¡æ€ï¼ˆmultiple modalitiesï¼‰ã€å¹¶è¡Œï¼ˆparallelismï¼‰å’Œå¤–éƒ¨äº¤äº’ï¼ˆexternal interactionsï¼‰ï¼Œè½»æ¾ç¼–ç¨‹ LLM åº”ç”¨ã€‚


## å®‰è£…
```bash
python -m venv env
source env/bin/activate

pip install --upgrade pip
pip install "sglang[all]"

# Install FlashInfer CUDA kernels
pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/
```

å¯¹äºå›½å†…è®¿é—® GitHub æœ‰å›°éš¾çš„ç”¨æˆ·ï¼Œæ¥ä¸Šæ¢¯å­ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸‹è½½å¹¶å®‰è£…ï¼š

```bash
wget https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl
pip install flashinfer-0.1.6+cu121torch2.4-cp310-cp310-linux_x86_64.whl
```


## åç«¯ï¼šSGLang Runtime (SRT)
- sglang v0.3.0

éƒ¨ç½² Qwen2-7B-Instruct æ¨¡å‹

```bash
python -m sglang.launch_server \
    --model-path /data/models/llm/qwen/Qwen2-7B-Instruct \
    --served-model-name Qwen2-7B \
    --port 30000 \
    --tensor-parallel-size 4 \
    --mem-fraction-static 0.66
```
- `--served-model-name`: æ¨¡å‹åç§°ã€‚
- `--tensor-parallel-size 4` `--tp-size`: ä½¿ç”¨ 4 å¡å¼ é‡å¹¶è¡Œã€‚
- `--mem-fraction-static 0.66`: é™ä½é™æ€å†…å­˜åˆ†é…æ¯”ä¾‹ã€‚
- `--disable-cuda-graph`: ç¦ç”¨CUDAå›¾ã€‚è¿™å¯èƒ½ä¼šç¨å¾®é™ä½æ€§èƒ½ï¼Œä½†å¯ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- `--enable-torch-compile`: å¯ç”¨ Torch ç¼–è¯‘å™¨ã€‚
- `--max-num-reqs`: é™åˆ¶å¹¶å‘è¯·æ±‚æ•°ã€‚

éƒ¨ç½²ä½¿ç”¨ 2 å¡å¼ é‡å¹¶è¡Œï¼ˆ`--tp 2`ï¼‰å’Œ 2 å¡æ•°æ®å¹¶è¡Œï¼ˆ`--dp 2`ï¼‰ã€‚

```bash
python -m sglang.launch_server \
    --model-path /data/models/llm/qwen/Qwen2-7B-Instruct \
    --served-model-name Qwen2-7B \
    --port 30000 \
    --tp 2 --dp 2 \
    --mem-fraction-static 0.66
```

`2 å¡å¼ é‡å¹¶è¡Œ`çš„é€Ÿåº¦æ¯ç§’ç”Ÿæˆ `31 Tokens`ã€‚

ä½¿ç”¨ curl è°ƒç”¨ OpenAI å…¼å®¹ API

- completions
```bash
curl http://127.0.0.1:30000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen2-7B",
        "prompt": "ä½ æ˜¯è°ï¼Ÿ",
        "temperature": 0.3
    }'|jq
```

- chat/completions
```bash
curl http://127.0.0.1:30000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen2-7B", 
        "messages": [{
            "role": "user", 
            "content": "ä½ æ˜¯è°ï¼Ÿ"
        }], 
        "temperature": 0.3
    }'|jq
```

éƒ¨ç½² Qwen2-72B-Instruct-GPTQ-Int4 æ¨¡å‹

```bash
python -m sglang.launch_server \
    --model-path /data/models/llm/qwen/Qwen2-72B-Instruct-GPTQ-Int4 \
    --quantization gptq \
    --served-model-name Qwen2-72B \
    --port 30000 \
    --tensor-parallel-size 4 \
    --disable-cuda-graph
```

| æ¨¡å‹å‚æ•°ï¼ˆBï¼‰ | mem-fraction-static | disable-cuda-graph | enable-torch-compile | å¡æ•° | æ˜¾å­˜ä½¿ç”¨ | è¾“å…¥ï¼ˆTokensï¼‰ | è¾“å‡ºï¼ˆTokensï¼‰ | æ¯ç§’ç”Ÿæˆï¼ˆTokensï¼‰ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 7 | 0.66 | âŒ | âŒ | 4 | 14647MiB / 15360MiB |   19 | 781 | 53.54 |
| 7 | 0.66 | âŒ | âŒ | 4 | 14647MiB / 15360MiB | 1495 | 264 | 51.47 |
| 7 | 0.85 | âœ… | âŒ | 4 | 13953MiB / 15360MiB |   19 | 566 | 45.33 |
| 7 | 0.85 | âŒ | âœ… | 4 | âŒ | âŒ | âŒ | âŒ |
| 7 | 0.85 | âœ… | âœ… | 4 | 13875MiB / 15360MiB |   19 | 559 | 47.79 |
| 72 | 0.85 | âœ… | âŒ | 4 | 13851MiB / 15360MiB |  19 | 633 | 16.50 |

- `7B`: Qwen2-7B-Instruct
- `72B`: Qwen2-72B-Instruct-GPTQ-Int4


## åç«¯ï¼švLLM Runtime
- vllm v0.6.0

éƒ¨ç½² Qwen2-7B-Instruct æ¨¡å‹

```bash
python -m vllm.entrypoints.openai.api_server \
    --model /data/models/llm/qwen/Qwen2-7B-Instruct \
    --served-model-name Qwen2-7B \
    --port 30000 \
    --tensor-parallel-size 4 \
    --dtype float16
```
- `--tensor-parallel-size 4`: ä½¿ç”¨ 4 å¡å¼ é‡å¹¶è¡Œ
- `--dtype float16`: T4 ä¸æ”¯æŒ bfloat16

éƒ¨ç½² Qwen2-72B-Instruct-GPTQ-Int4 æ¨¡å‹

```bash
python -m vllm.entrypoints.openai.api_server \
    --model /data/models/llm/qwen/Qwen2-72B-Instruct-GPTQ-Int4 \
    --quantization gptq \
    --served-model-name Qwen2-72B \
    --port 30000 \
    --tensor-parallel-size 4 \
    --gpu-memory-utilization 0.99 \
    --max_model_len 8192 \
    --dtype float16
```

| æ¨¡å‹å‚æ•°ï¼ˆBï¼‰ | å¡æ•° | æ˜¾å­˜ä½¿ç”¨ | è¾“å…¥ï¼ˆTokensï¼‰ | è¾“å‡ºï¼ˆTokensï¼‰ | æ¯ç§’ç”Ÿæˆï¼ˆTokensï¼‰ |
| --- | --- | --- | --- | --- | --- |
| 7 | 4 | 11233MiB / 15360MiB | 19 | 573 | 43.60 |
| 7 | 4 | 11233MiB / 15360MiB | 1495 | 208 | 40.29 |
| 72 | 4 | 13983MiB / 15360MiB | 19 | 683 | 16.88 |


**SGLang æ¯” vLLM çš„æ€§èƒ½å¿« ğŸš€ `20%` ä»¥ä¸Šã€‚**


## å‚è€ƒèµ„æ–™
- [SGLang](https://github.com/sgl-project/sglang)
- [Guide on Hyperparameter Tuning](https://github.com/sgl-project/sglang/blob/main/docs/en/hyperparameter_tuning.md)
- [FlashInfer - Kernel Library for LLM Serving](https://github.com/flashinfer-ai/flashinfer)
- [è´¾æ‰¬æ¸…ç‚¹èµï¼š3K staré‡çš„SGLangä¸Šæ–°ï¼ŒåŠ é€ŸLlama 405Bæ¨ç†ç§’æ€vLLMã€TensorRT-LLM](https://mp.weixin.qq.com/s/FYwguU3USf12Wb5HXaHH3A)
- [Qwen2-7B-Instruct](https://www.modelscope.cn/models/qwen/Qwen2-7B-Instruct)
