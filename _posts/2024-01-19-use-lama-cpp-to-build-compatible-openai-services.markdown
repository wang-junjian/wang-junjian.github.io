---
layout: single
title:  "ä½¿ç”¨ llama.cpp æ„å»ºå…¼å®¹ OpenAI API æœåŠ¡"
date:   2024-01-19 08:00:00 +0800
categories: llama.cpp
tags: [llama.cpp, Quantization, LLM, Qwen, DeepSeek, llama-cpp-python, OpenAI, å›°æƒ‘åº¦, PPL, Perplexity, curl, Metal, MPS, cuBLAS, CUDA, TeslaT4, MacBookProM2Max]
---

## [llama.cpp][llama.cpp]
- [ä½¿ç”¨ llama.cpp æ„å»ºæœ¬åœ°èŠå¤©æœåŠ¡]({% post_url 2023-12-16-building-a-local-chat-service-using-llama-cpp %})

## æ¨¡å‹é‡åŒ–
### é‡åŒ–ç±»å‹
```shell
./quantize --help
```
```
Allowed quantization types:
   2  or  Q4_0    :  3.56G, +0.2166 ppl @ LLaMA-v1-7B
   3  or  Q4_1    :  3.90G, +0.1585 ppl @ LLaMA-v1-7B
   8  or  Q5_0    :  4.33G, +0.0683 ppl @ LLaMA-v1-7B
   9  or  Q5_1    :  4.70G, +0.0349 ppl @ LLaMA-v1-7B
  19  or  IQ2_XXS :  2.06 bpw quantization
  20  or  IQ2_XS  :  2.31 bpw quantization
  10  or  Q2_K    :  2.63G, +0.6717 ppl @ LLaMA-v1-7B
  21  or  Q2_K_S  :  2.16G, +9.0634 ppl @ LLaMA-v1-7B
  12  or  Q3_K    : alias for Q3_K_M
  11  or  Q3_K_S  :  2.75G, +0.5551 ppl @ LLaMA-v1-7B
  12  or  Q3_K_M  :  3.07G, +0.2496 ppl @ LLaMA-v1-7B
  13  or  Q3_K_L  :  3.35G, +0.1764 ppl @ LLaMA-v1-7B
  15  or  Q4_K    : alias for Q4_K_M
  14  or  Q4_K_S  :  3.59G, +0.0992 ppl @ LLaMA-v1-7B
  15  or  Q4_K_M  :  3.80G, +0.0532 ppl @ LLaMA-v1-7B
  17  or  Q5_K    : alias for Q5_K_M
  16  or  Q5_K_S  :  4.33G, +0.0400 ppl @ LLaMA-v1-7B
  17  or  Q5_K_M  :  4.45G, +0.0122 ppl @ LLaMA-v1-7B
  18  or  Q6_K    :  5.15G, -0.0008 ppl @ LLaMA-v1-7B
   7  or  Q8_0    :  6.70G, +0.0004 ppl @ LLaMA-v1-7B
   1  or  F16     : 13.00G              @ 7B
   0  or  F32     : 26.00G              @ 7B
```

#### å›°æƒ‘åº¦ï¼ˆPPL, Perplexityï¼‰
"PPL" æ˜¯ "Perplexity" çš„ç¼©å†™ï¼Œä¸­æ–‡é€šå¸¸ç¿»è¯‘ä¸ºâ€œå›°æƒ‘åº¦â€ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼Œå›°æƒ‘åº¦æ˜¯ä¸€ç§è¡¡é‡æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­è¨€æ¨¡å‹ä¸­ã€‚

å›°æƒ‘åº¦åŸºäºæ¨¡å‹å¯¹æµ‹è¯•é›†æ•°æ®çš„æ¦‚ç‡ï¼Œå®ƒçš„å€¼è¶Šå°ï¼Œè¯´æ˜æ¨¡å‹çš„æ€§èƒ½è¶Šå¥½ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹çš„å›°æƒ‘åº¦ä¸º Pï¼Œé‚£ä¹ˆå½“è¿™ä¸ªæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ—¶å€™ï¼Œå®ƒçš„ä¸ç¡®å®šæ€§ï¼ˆæˆ–è€…è¯´â€œå›°æƒ‘åº¦â€ï¼‰å°±ç›¸å½“äºåœ¨ P ä¸ªè¯ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªè¯ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹çš„å›°æƒ‘åº¦ä¸º 10ï¼Œé‚£ä¹ˆè¿™ä¸ªæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„ä¸ç¡®å®šæ€§å°±ç›¸å½“äºåœ¨ 10 ä¸ªè¯ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªè¯ã€‚å¦‚æœå¦ä¸€ä¸ªæ¨¡å‹çš„å›°æƒ‘åº¦ä¸º 5ï¼Œé‚£ä¹ˆè¿™ä¸ªæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„ä¸ç¡®å®šæ€§å°±ç›¸å½“äºåœ¨ 5 ä¸ªè¯ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªè¯ã€‚å› æ­¤ï¼Œå›°æƒ‘åº¦è¶Šå°ï¼Œæ¨¡å‹çš„æ€§èƒ½å°±è¶Šå¥½ã€‚

### ç¼–è¯‘

å…‹éš†ä»£ç 

```shell
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

#### Metal (MPS)
```shell
make
```

#### cuBLAS (CUDA)
éœ€è¦å®‰è£… [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit)

```shell
make LLAMA_CUBLAS=1
```

### é‡åŒ–
```shell
# install
pip install -r requirements.txt

# convert to gguf
python convert.py [--outtype {f32,f16,q8_0}] [--pad-vocab] [--vocab-type {spm,bpe,hfft}] [--outfile OUTFILE] model
python convert-hf-to-gguf.py [--outtype {f32,f16}] [--outfile OUTFILE] model

# quantize
./quantize model.gguf [model-quant.gguf] quant-type
```

### æµ‹è¯•
#### Metal (MPS)
```shell
./main -m ~/HuggingFace/wangjunjian/gguf/deepseek-llm-7b-chat.Q5_K_M.gguf \
    --n-gpu-layers -1 -e -p "å†™ä¸€ç¯‡1000å­—çš„ä½œæ–‡ï¼šã€Š2024å›å®¶è¿‡å¹´ã€‹"
```

#### cuBLAS (CUDA)
```shell
CUDA_VISIBLE_DEVICES=0 ./main -m deepseek-llm-7b-chat.Q5_K_M.gguf \
    --n-gpu-layers 15000 -e -p "å†™ä¸€ç¯‡1000å­—çš„ä½œæ–‡ï¼šã€Š2024å›å®¶è¿‡å¹´ã€‹"
```

`--n-gpu-layers` è®¾ç½® -1 æ²¡æœ‰æ•ˆæœï¼Œè®¾ç½®å¤§ä¸€ç‚¹çš„æ•°å­—å³å¯ï¼Œå¦‚ï¼š15000

### é€Ÿåº¦
- MacBook Pro M2 Max
  - CPUï¼šé€Ÿåº¦æ¯ç§’ `17 Tokens`
  - GPUï¼šé€Ÿåº¦æ¯ç§’ `45 Tokens`
- NVIDIA Tesla T4
  - GPUï¼šé€Ÿåº¦æ¯ç§’ `36 Tokens`
- Intel Xeon Silver 4216ï¼ˆ64 æ ¸ï¼‰
  - CPUï¼šé€Ÿåº¦æ¯ç§’ `9 Tokens`

## æ¨¡å‹

å¯ä»¥ä» [TheBloke](https://huggingface.co/TheBloke) ä¸‹è½½æ›´å¤šä¸åŒé‡åŒ–çš„ GGUF æ¨¡å‹ã€‚

### [Qwen][Qwen]
- [Qwen/Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat)
- [Merge qwen to llama cpp](https://github.com/ggerganov/llama.cpp/pull/4281)
- [qwen.cpp](https://github.com/QwenLM/qwen.cpp)
- [llama.cpp][llama.cpp]

1. è½¬æ¢ GGUF
```shell
python convert-hf-to-gguf.py \
    --outtype f32 \
    --outfile ~/HuggingFace/wangjunjian/gguf/qwen-7b-chat-f32.gguf \
    ~/HuggingFace/Qwen/Qwen-7B-Chat
```

2. é‡åŒ– Q5_K_M
```shell
./quantize \
    ~/HuggingFace/wangjunjian/gguf/qwen-7b-chat-f32.gguf \
    ~/HuggingFace/wangjunjian/gguf/qwen-7b-chat.Q5_K_M.gguf \
    Q5_K_M
```

3. æµ‹è¯•
```shell
./main -m ~/HuggingFace/wangjunjian/gguf/qwen-7b-chat.Q5_K_M.gguf \
    -n 256 -e -p "è§£é‡Šå‘½ä»¤: make -j"
```

```
è§£é‡Šå‘½ä»¤: make -j4
æ‰§è¡Œç»“æœ:
make -j4
ç”±äºæˆ‘ä»¬æ²¡æœ‰æä¾›ä»»ä½•é€‰é¡¹æˆ–å‚æ•°ç»™makeå‘½ä»¤ï¼Œmakeå°†ä½¿ç”¨é»˜è®¤çš„å‚æ•°ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œmakeå‘½ä»¤ä¼šè‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿä¸­å¯ç”¨çš„CPUæ•°é‡ï¼Œå¹¶åœ¨æ‰§è¡Œæ—¶ä½¿ç”¨è¿™äº›CPUè¿›è¡Œå¹¶è¡ŒåŒ–ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦æ‰‹åŠ¨æŒ‡å®šä½¿ç”¨çš„CPUæ•°é‡ï¼Œåˆ™å¯ä»¥ä½¿ç”¨-jå‚æ•°æ¥å®ç°ã€‚
ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥æŒ‡å®šä½¿ç”¨4ä¸ªCPUè¿›è¡Œå¹¶è¡Œå¤„ç†ï¼š

make -j4

è¿™å°†ä½¿makeå‘½ä»¤åŒæ—¶ä½¿ç”¨4ä¸ªCPUæ¥æ„å»ºé¡¹ç›®ã€‚æ³¨æ„ï¼Œä½¿ç”¨å¤šä¸ªCPUè¿›è¡Œå¹¶è¡ŒåŒ–å¯èƒ½ä¼šå¢åŠ ç³»ç»Ÿçš„è´Ÿè½½ï¼Œå¹¶å¯èƒ½å¯¼è‡´å…¶ä»–ç¨‹åºæˆ–æœåŠ¡çš„æ€§èƒ½ä¸‹é™ã€‚å› æ­¤ï¼Œåœ¨æ‰§è¡Œè¿™ç§æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç³»ç»Ÿæœ‰è¶³å¤Ÿçš„èµ„æºæ¥æ”¯æŒå¹¶è¡ŒåŒ–ã€‚
æ€»çš„æ¥è¯´ï¼Œmakeå‘½ä»¤æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬è‡ªåŠ¨åŒ–å„ç§ä»»åŠ¡ã€‚é€šè¿‡æŒæ¡makeå‘½ä»¤çš„åŸºæœ¬è¯­æ³•å’Œé€‰é¡¹ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æœ‰æ•ˆåœ°ç®¡ç†å’Œæ„å»ºé¡¹ç›®ã€‚[PAD151643] [end of text]
```

### [DeepSeek][DeepSeek]
- [deepseek-ai/deepseek-llm-7b-chat](https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat)
- [DeepSeek æ¨¡å‹é‡åŒ–](https://github.com/deepseek-ai/deepseek-coder/?tab=readme-ov-file#7-qa)
- [llama.cpp][llama.cpp]

1. è½¬æ¢ GGUF
```shell
python convert.py \
    --outtype f32 --vocab-type bpe --pad-vocab \
    --outfile ~/HuggingFace/wangjunjian/gguf/deepseek-llm-7b-chat-f32.gguf \
    ~/HuggingFace/deepseek-ai/deepseek-llm-7b-chat
```

2. é‡åŒ– Q5_K_M
```shell
./quantize \
    ~/HuggingFace/wangjunjian/gguf/deepseek-llm-7b-chat-f32.gguf \
    ~/HuggingFace/wangjunjian/gguf/deepseek-llm-7b-chat.Q5_K_M.gguf \
    Q5_K_M
```

3. æµ‹è¯•
```shell
./main -m ~/HuggingFace/wangjunjian/gguf/deepseek-llm-7b-chat.Q5_K_M.gguf \
    -n 256 -e -p "è§£é‡Šå‘½ä»¤: make -j"
```
```
è§£é‡Šå‘½ä»¤: make -j4
ç¼–è¯‘æ—¶ï¼Œmake ä¼šæ£€æŸ¥æºä»£ç æ–‡ä»¶æ˜¯å¦å‘ç”Ÿäº†æ”¹å˜ã€‚å¦‚æœå‘ç”Ÿæ”¹å˜äº†ï¼Œé‚£ä¹ˆå®ƒå°±ä¼šé‡æ–°ç¼–è¯‘é‚£äº›ç›¸å…³çš„ç›®æ ‡æ–‡ä»¶ã€‚å¦‚æœæ²¡æœ‰å‘ç”Ÿå˜åŒ–ï¼Œé‚£ä¹ˆå®ƒä¼šè·³è¿‡ç¼–è¯‘è¿‡ç¨‹ã€‚
-j4 å‚æ•°å‘Šè¯‰ make å¹¶è¡Œè¿è¡Œå¤šä¸ªä»»åŠ¡ï¼ˆæœ€å¤šå››ä¸ªï¼‰ä»¥åŠ å¿«æ„å»ºé€Ÿåº¦ã€‚ [end of text]
```


## [llama-cpp-python][llama-cpp-python]

- [Python Bindings for llama.cpp](https://llama-cpp-python.readthedocs.io/en/latest/)
- [Local Copilot replacement](https://llama-cpp-python.readthedocs.io/en/latest/server/#code-completion)
- [Function Calling support](https://llama-cpp-python.readthedocs.io/en/latest/server/#function-calling)
- [Vision API support](https://llama-cpp-python.readthedocs.io/en/latest/server/#multimodal-models)
- [Multiple Models](https://llama-cpp-python.readthedocs.io/en/latest/server/#configuration-and-multi-model-support)

### å®‰è£…

- [Getting Started](https://llama-cpp-python.readthedocs.io/en/latest/)
- [Development](https://llama-cpp-python.readthedocs.io/en/latest/#development)

åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```shell
conda create -n llama-cpp-python python
conda activate llama-cpp-python
```

#### [Metal (MPS)](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/)
```shell
CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install "llama-cpp-python[server]"
```

#### [cuBLAS (CUDA)](https://llama-cpp-python.readthedocs.io/en/latest/#cublas)
```shell
CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python[server]
```

### è¿è¡Œå…¼å®¹ OpenAI æœåŠ¡
- [OpenAI Compatible Server](https://llama-cpp-python.readthedocs.io/en/latest/server/)

#### ä¸»è¦å‚æ•°
- `--model MODEL`             The path to the model to use for generating completions.
- `--model_alias MODEL_ALIAS` The alias of the model to use for generating completions.
- `--n_gpu_layers N_GPU_LAYERS` The number of layers to put on the GPU. The rest will be on the CPU. Set -1 to move all to GPU.
- `--n_ctx N_CTX`             The context size. (default: 2048)
- `--chat_format CHAT_FORMAT` Chat format to use. (default: llama-2)
- `--host HOST`               Listen address (default: localhost)
- `--port PORT`               Listen port (default: 8000)
- `--api_key API_KEY`         API key for authentication. If set all requests need to be authenticated.

#### Metal (MPS)
```shell
python -m llama_cpp.server \
    --model ~/HuggingFace/wangjunjian/gguf/deepseek-llm-7b-chat.Q5_K_M.gguf \
    --model_alias gpt-3.5-turbo \
    --host 0.0.0.0 --port 8080 \
    --n_gpu_layers -1 \
    --chat_format chatml
```

`--n_gpu_layers` è¦æ”¾ç½®åœ¨ GPU ä¸Šçš„å±‚æ•°ã€‚ å…¶ä½™çš„å°†åœ¨ CPU ä¸Šè¿›è¡Œã€‚ è®¾ç½® `-1` å°†å…¨éƒ¨ç§»è‡³ GPUï¼Œå¦‚æœä¸ä½¿ç”¨ `--n_gpu_layers` å‚æ•°å°†ä½¿ç”¨ CPU è¿è¡Œã€‚

#### cuBLAS (CUDA)
```shell
python -m llama_cpp.server \
    --model deepseek-llm-7b-chat.Q5_K_M.gguf \
    --model_alias gpt-3.5-turbo \
    --n_gpu_layers -1 \
    --host 0.0.0.0 --port 8080 \
    --chat_format chatml
```

å¯ä»¥ä½¿ç”¨ `CUDA_VISIBLE_DEVICES` ç¯å¢ƒå˜é‡æŒ‡å®š GPUã€‚

æˆ‘åœ¨ macOS ä¸Šé‡åŒ–çš„æ¨¡å‹ï¼Œç„¶ååœ¨ Linux ä¸Šè¿è¡Œå‡ºç°ä»¥ä¸‹é”™è¯¯ï¼ˆGitHub Copilot: å¯èƒ½æ˜¯å› ä¸º macOS å’Œ Linux çš„æµ®ç‚¹æ•°è®¡ç®—ç²¾åº¦ä¸ä¸€è‡´å¯¼è‡´çš„ã€‚ï¼‰

```shell
Llama.generate: prefix-match hit
CUDA error: invalid argument
  current device: 0, in function ggml_backend_cuda_buffer_get_tensor at /tmp/pip-install-ulfkej7c/llama-cpp-python_7f8116c5437340c7b46a6e712c01894b/vendor/llama.cpp/ggml-cuda.cu:10317
  cudaMemcpy(data, (const char *)tensor->data + offset, size, cudaMemcpyDeviceToHost)
GGML_ASSERT: /tmp/pip-install-ulfkej7c/llama-cpp-python_7f8116c5437340c7b46a6e712c01894b/vendor/llama.cpp/ggml-cuda.cu:231: !"CUDA error"
Aborted (core dumped)
```

æˆ‘é‡æ–°åœ¨ Linux ä¸Šå®‰è£…äº† llama.cppï¼Œé‡åŒ–æ¨¡å‹ï¼Œç„¶åè¿è¡ŒæˆåŠŸäº†ã€‚

- [warning: failed to mlock NNN-byte buffer (after previously locking MMM bytes)](https://github.com/abetlen/llama-cpp-python/issues/708)

#### [é…ç½®å¤šæ¨¡å‹](https://llama-cpp-python.readthedocs.io/en/latest/server/#configuration-and-multi-model-support)

æœåŠ¡å™¨æ”¯æŒé€šè¿‡ JSON é…ç½®æ–‡ä»¶è¿›è¡Œé…ç½®ï¼Œå¯ä»¥ä½¿ç”¨ `--config_file` å‚æ•°æˆ– `CONFIG_FILE` ç¯å¢ƒå˜é‡ä¼ é€’è¯¥æ–‡ä»¶ã€‚

åˆ›å»ºé…ç½®æ–‡ä»¶ `config.json`ï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨å½“å‰ç›®å½•ä¸­ï¼š

```json
{
    "host": "0.0.0.0",
    "port": 8080,
    "models": [
        {   
            "model": "/Users/junjian/HuggingFace/wangjunjian/gguf/qwen-7b-chat.Q5_K_M.gguf",
            "model_alias": "gpt-3.5-turbo",
            "chat_format": "chatml",
            "n_gpu_layers": -1, 
            "offload_kqv": true,
            "n_threads": 12, 
            "n_batch": 512,
            "n_ctx": 2048
        },  
        {   
            "model": "/Users/junjian/HuggingFace/wangjunjian/gguf/deepseek-llm-7b-chat.Q5_K_M.gguf",
            "model_alias": "gpt-4",
            "chat_format": "chatml",
            "n_gpu_layers": -1, 
            "offload_kqv": true,
            "n_threads": 12, 
            "n_batch": 512,
            "n_ctx": 2048
        }   
    ]   
}
```

**ç›®å‰åªæœ‰å•ä¸ªæ¨¡å‹åŠ è½½åˆ°å†…å­˜ä¸­ï¼ŒæœåŠ¡å™¨å°†æ ¹æ®éœ€è¦è‡ªåŠ¨åŠ è½½å’Œå¸è½½æ¨¡å‹ã€‚**

åŒæ—¶éƒ¨ç½²ä¸¤ä¸ªä¸€æ ·çš„æ¨¡å‹ï¼Œä¹Ÿä¸ä¼šæé«˜æ€§èƒ½ï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªæ¨¡å‹ä¼šè¢«åŠ è½½åˆ°å†…å­˜ä¸­ã€‚

```json
{
    "models": [
        {   
            "model": "/Users/junjian/HuggingFace/wangjunjian/gguf/qwen-7b-chat.Q5_K_M.gguf",
            "model_alias": "gpt-3.5-turbo",
        },  
        {   
            "model": "/Users/junjian/HuggingFace/wangjunjian/gguf/qwen-7b-chat.Q5_K_M.gguf",
            "model_alias": "gpt-3.5-turbo",
        }   
    ]   
}
```

è¿è¡ŒæœåŠ¡

```shell
python -m llama_cpp.server --config_file config.json
```

### curl è°ƒç”¨ API
- [Text generation models](https://platform.openai.com/docs/guides/text-generation?lang=curl)
- [OpenAI Completions API (Legacy)](https://platform.openai.com/docs/api-reference/completions)

#### æ¨¡å‹åˆ—è¡¨
```shell
curl http://localhost:8080/v1/models | jq
```
```json
{
  "object": "list",
  "data": [
    {
      "id": "gpt-3.5-turbo",
      "object": "model",
      "owned_by": "me",
      "permissions": []
    },
    {
      "id": "gpt-4",
      "object": "model",
      "owned_by": "me",
      "permissions": []
    }
  ]
}
```

#### `POST` `/v1/completions`
```shell
curl http://localhost:8080/v1/completions \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "gpt-3.5-turbo",
    "max_tokens": 2048,
    "prompt": "å†™ä¸€ç¯‡1000å­—çš„ä½œæ–‡ï¼šã€Š2024å›å®¶è¿‡å¹´ã€‹"
  }'|jq
```
```json
{
  "id": "cmpl-cd79289e-d323-441a-97ad-561df9726ad5",
  "object": "text_completion",
  "created": 1705671912,
  "model": "gpt-3.5-turbo",
  "choices": [
    {
      "text": "\n\né¢˜ç›®è¦æ±‚å†™ä¸€ç¯‡1000å­—çš„ä½œæ–‡ï¼Œä¸»é¢˜ä¸ºâ€œ2024å›å®¶è¿‡å¹´â€ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å®½æ³›çš„ä¸»é¢˜ï¼Œå¯ä»¥æ¶‰åŠå„ç§è§’åº¦å’Œæƒ…æ„Ÿã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¯èƒ½çš„å†™ä½œæ–¹å‘ï¼š\n\n2024å¹´ï¼Œæˆ‘ç»ˆäºæœ‰äº†æœºä¼šå›å®¶è¿‡å¹´ã€‚æˆ‘å·²ç»åœ¨å¤–å·¥ä½œäº†äº”å¹´ï¼Œæ¯å¹´éƒ½åœ¨åŸå¸‚é‡Œåº¦è¿‡æ˜¥èŠ‚ï¼Œåªæœ‰ä»Šå¹´ï¼Œæˆ‘å¯ä»¥å›åˆ°å®¶ä¹¡ï¼Œä¸å®¶äººä¸€èµ·æ¬¢åº¦ä½³èŠ‚ã€‚\n\nå›å®¶çš„è·¯é€”å¹¶ä¸é¥è¿œï¼Œåªéœ€å‡ ä¸ªå°æ—¶çš„é£è¡Œï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œè¿™å´æ˜¯ä¸€æ¬¡å¿ƒçµçš„æ—…ç¨‹ã€‚æˆ‘çœ‹åˆ°äº†çª—å¤–çš„å±±å·ã€æ²³æµå’Œç”°é‡ï¼Œå®ƒä»¬åœ¨é˜³å…‰ä¸‹æ˜¾å¾—ç”Ÿæœºå‹ƒå‹ƒï¼Œä»¿ä½›åœ¨æ¬¢è¿æˆ‘çš„å½’æ¥ã€‚æˆ‘æ„Ÿåˆ°äº†ç©ºæ°”ä¸­çš„æ¸…æ–°å’Œæ¹¿æ¶¦ï¼Œé‚£æ˜¯å®¶ä¹¡ç‰¹æœ‰çš„æ°”æ¯ï¼Œè®©æˆ‘æ„Ÿåˆ°æ— æ¯”çš„èˆ’é€‚å’Œå®‰å®ã€‚\n\nå½“æˆ‘ç»ˆäºåˆ°è¾¾å®¶é—¨å‰çš„æ—¶å€™ï¼Œæˆ‘çœ‹åˆ°çš„æ˜¯ç†Ÿæ‚‰çš„é¢å­”å’Œäº²åˆ‡çš„ç¬‘å®¹ã€‚çˆ¶æ¯å·²ç»å‡†å¤‡å¥½äº†ä¸€æ¡Œä¸°ç››çš„å¹´å¤œé¥­ï¼Œç­‰å¾…ç€æˆ‘å’Œå¦¹å¦¹çš„åˆ°æ¥ã€‚æˆ‘ä»¬å›´ååœ¨é¤æ¡Œå‰ï¼Œäº«å—ç€ç¾é£Ÿçš„åŒæ—¶ï¼Œä¹Ÿåœ¨åˆ†äº«å½¼æ­¤çš„ç”Ÿæ´»æ•…äº‹å’Œå·¥ä½œç»å†ã€‚\n\nåœ¨å®¶ä¹¡è¿‡å¹´ï¼Œæœ€è®©æˆ‘æ„Ÿåˆ°å¹¸ç¦çš„å°±æ˜¯ä¸å®¶äººä¸€èµ·åº¦è¿‡çš„æ—¶é—´ã€‚æˆ‘ä»¬ä¸€èµ·åŒ…é¥ºå­ã€è´´æ˜¥è”ã€çœ‹æ˜¥æ™šï¼Œè¿™äº›ç†Ÿæ‚‰çš„æ´»åŠ¨è®©æˆ‘æ„Ÿåˆ°æ— æ¯”çš„äº²åˆ‡å’Œæ¸©é¦¨ã€‚æˆ‘ä¹Ÿæ„Ÿåˆ°äº†å®¶äººçš„å…³çˆ±å’Œæ”¯æŒï¼Œä»–ä»¬å§‹ç»ˆåœ¨æˆ‘èº«è¾¹ï¼Œç»™æˆ‘åŠ›é‡å’Œå‹‡æ°”å»é¢å¯¹ç”Ÿæ´»ä¸­çš„æŒ‘æˆ˜å’Œå›°éš¾ã€‚\n\né™¤å¤•å¤œï¼Œæˆ‘ä»¬ä¸€å®¶äººå›´ååœ¨ç”µè§†æœºå‰ï¼Œçœ‹ç€æ˜¥æ™šçš„èŠ‚ç›®ï¼Œäº«å—ç€å›¢åœ†çš„ç¾å¥½æ—¶å…‰ã€‚æˆ‘çœ‹åˆ°äº†çˆ¶æ¯çœ¼ä¸­çš„æœŸå¾…å’Œæ»¡è¶³ï¼Œä¹Ÿçœ‹åˆ°äº†å¦¹å¦¹è„¸ä¸Šçš„ç¬‘å®¹å’Œå¿«ä¹ã€‚æˆ‘çŸ¥é“ï¼Œè¿™å°±æ˜¯æˆ‘ä¸€ç›´æƒ³è¦çš„ç”Ÿæ´»ï¼Œè¿™å°±æ˜¯æˆ‘ä¸€ç›´å‘å¾€çš„å®¶çš„æ„Ÿè§‰ã€‚\n\nå›å®¶è¿‡å¹´ï¼Œè®©æˆ‘æ„Ÿåˆ°æ— æ¯”çš„å¹¸ç¦å’Œæ»¡è¶³ã€‚æˆ‘æ˜ç™½äº†ï¼Œæ— è®ºæˆ‘åœ¨å“ªé‡Œï¼Œæ— è®ºæˆ‘åœ¨åšä»€ä¹ˆï¼Œå®¶äººæ°¸è¿œæ˜¯æˆ‘æœ€åšå®çš„åç›¾å’Œæœ€æ¸©æš–çš„æ¸¯æ¹¾ã€‚æˆ‘ä¹Ÿæ˜ç™½äº†ï¼Œåªæœ‰å›åˆ°å®¶ä¹¡ï¼Œæ‰èƒ½çœŸæ­£åœ°æ„Ÿå—åˆ°ç”Ÿæ´»çš„ç¾å¥½å’Œå®¶çš„å‘³é“ã€‚\n\n2024å¹´çš„æ˜¥èŠ‚ï¼Œå¯¹æˆ‘æ¥è¯´æ˜¯ä¸€æ¬¡éš¾å¿˜çš„ç»å†ã€‚å®ƒè®©æˆ‘æ›´åŠ çæƒœä¸å®¶äººåœ¨ä¸€èµ·çš„æ—¶é—´ï¼Œä¹Ÿè®©æˆ‘æ›´æ·±åˆ»åœ°ç†è§£äº†å®¶çš„æ„ä¹‰ã€‚æˆ‘çŸ¥é“ï¼Œæ— è®ºæœªæ¥æˆ‘ä¼šå»å“ªé‡Œï¼Œæˆ‘éƒ½ä¼šè®°ä½è¿™ä¸ªæ˜¥èŠ‚ï¼Œè®°ä½è¿™ä¸ªå›å®¶çš„æ„Ÿè§‰ã€‚\n\nè¿™æ˜¯ä¸€ä¸ªå……æ»¡æ¸©æƒ…å’Œäº²æƒ…çš„ä¸»é¢˜ï¼Œå¯ä»¥å¼•å‘è¯»è€…å¯¹å®¶åº­ã€å›¢åœ†å’Œçˆ±çš„æ€è€ƒã€‚åœ¨å†™ä½œä¸­ï¼Œå¯ä»¥é€šè¿‡æç»˜å®¶ä¹¡çš„é£æ™¯ã€æè¿°å®¶äººçš„é¢å­”å’Œè¡Œä¸ºã€è¡¨è¾¾è‡ªå·±çš„æ„Ÿå—å’Œæ€è€ƒç­‰æ–¹å¼ï¼Œæ¥å±•ç°ä¸»é¢˜çš„æ·±åº¦å’Œå¹¿åº¦ã€‚åŒæ—¶ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å¯¹è¯å’Œå¿ƒç†æå†™ï¼Œæ¥å±•ç¤ºäººç‰©çš„æ€§æ ¼å’Œæƒ…æ„Ÿï¼Œå¢å¼ºæ–‡ç« çš„è‰ºæœ¯æ„ŸæŸ“åŠ›ã€‚æœ€åï¼Œå¯ä»¥ä»¥ä¸€ç§æ„Ÿäººçš„æ–¹å¼ç»“æŸæ–‡ç« ï¼Œå¦‚é€šè¿‡å›å¿†è¿‡å»çš„ç»å†ã€å±•æœ›æœªæ¥çš„å¸Œæœ›æˆ–è€…è¡¨è¾¾å¯¹å®¶äººçš„æ„Ÿè°¢å’Œç¥ç¦ç­‰ï¼Œæ¥è¾¾åˆ°å‡åä¸»é¢˜çš„æ•ˆæœã€‚",
      "index": 0,
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 19,
    "completion_tokens": 535,
    "total_tokens": 554
  }
}
```

#### `POST` `/v1/chat/completions`
```shell
curl http://localhost:8080/v1/chat/completions \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "gpt-3.5-turbo",
    "max_tokens": 2048,
    "messages": [ 
      { "role": "system", "content": "ä½ æ˜¯ä¸€åäºŒæ¬¡å…ƒåŠ©æ‰‹ï¼Œå›ç­”è¦ç²¾ç®€ã€‚" },
      { "role": "user", "content": "æœ€è¿‘æœ‰ä»€ä¹ˆå¥½çœ‹çš„ç•ªå‰§ï¼Ÿ" }
    ]
  }'|jq
```
```json
{
  "id": "chatcmpl-3305fdda-b6ec-4424-83ee-0346bc170d0a",
  "object": "chat.completion",
  "created": 1705672548,
  "model": "gpt-3.5-turbo",
  "choices": [
    {
      "index": 0,
      "message": {
        "content": "æ¨èã€Šè¿›å‡»çš„å·¨äººã€‹ã€ã€Šé¬¼ç­ä¹‹åˆƒã€‹ã€ã€Šä¸œäº¬å–°ç§ã€‹ã€ã€Šè¾‰å¤œå¤§å°å§æƒ³è®©æˆ‘å‘Šç™½ã€‹ç­‰ã€‚è¿™äº›éƒ½æ˜¯è¿‘å¹´æ¥éå¸¸å—æ¬¢è¿çš„ç•ªå‰§ï¼Œå‰§æƒ…ç²¾å½©ï¼Œäººç‰©å½¢è±¡é²œæ˜ï¼Œå€¼å¾—ä¸€çœ‹ã€‚[PAD151645]\n",
        "role": "assistant"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 55,
    "completion_tokens": 54,
    "total_tokens": 109
  }
}
```

#### `POST` `/v1/embeddings`
```shell
curl http://localhost:8080/v1/embeddings \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "input": "Embeddings are a way to represent words as vectors. The vectors are chosen in such a way that they are similar to other words that appear in similar contexts."
  }'|jq
```


## é€Ÿåº¦æµ‹è¯•
- [LLM çš„åŸºå‡†æµ‹è¯•]({% post_url 2024-01-17-LLM-benchmarking %})
- [æµ‹è¯•è„šæœ¬](https://wangjunjian.com/llm/benchmark/2024/01/17/LLM-benchmarking.html#%E6%B5%8B%E8%AF%95%E8%84%9A%E6%9C%AC)

### 1 å¼  GPU

éƒ¨ç½²æœåŠ¡

```shell
CUDA_VISIBLE_DEVICES=0 python -m llama_cpp.server \
    --model qwen-7b-chat.Q5_K_M.gguf \
    --model_alias gpt-3.5-turbo \
    --n_gpu_layers -1 \
    --host 0.0.0.0 --port 8080 \
    --chat_format chatml
```

GPU ä½¿ç”¨æƒ…å†µ

```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       Off | 00000000:43:00.0 Off |                    0 |
| N/A   48C    P0              27W /  70W |   6478MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

æµ‹è¯•ç»“æœ

```shell
ğŸš€ æ¯ç§’ç”Ÿæˆ Tokens: 21.06 	 åˆè®¡ Tokens ï¼ˆ418ï¼‰ = è¾“å…¥ Tokensï¼ˆ20ï¼‰ + è¾“å‡º Tokensï¼ˆ398ï¼‰
ğŸš€ æ¯ç§’ç”Ÿæˆå­—ç¬¦   : 40.33 	 åˆè®¡ç”Ÿæˆå­—ç¬¦ï¼ˆ762ï¼‰
â±ï¸ ç”Ÿæˆè€—æ—¶: 18.90 ç§’
```

### 2 å¼  GPU

éƒ¨ç½²æœåŠ¡

```shell
CUDA_VISIBLE_DEVICES=0,1 python -m llama_cpp.server \
    --model qwen-7b-chat.Q5_K_M.gguf \
    --model_alias gpt-3.5-turbo \
    --n_gpu_layers -1 \
    --host 0.0.0.0 --port 8080 \
    --chat_format chatml
```

GPU ä½¿ç”¨æƒ…å†µ

```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       Off | 00000000:43:00.0 Off |                    0 |
| N/A   55C    P0              28W /  70W |   3222MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla T4                       Off | 00000000:47:00.0 Off |                    0 |
| N/A   41C    P0              27W /  70W |   3522MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

æµ‹è¯•ç»“æœ

```shell
ğŸš€ æ¯ç§’ç”Ÿæˆ Tokens: 18.47 	 åˆè®¡ Tokens ï¼ˆ526ï¼‰ = è¾“å…¥ Tokensï¼ˆ20ï¼‰ + è¾“å‡º Tokensï¼ˆ506ï¼‰
ğŸš€ æ¯ç§’ç”Ÿæˆå­—ç¬¦   : 33.79 	 åˆè®¡ç”Ÿæˆå­—ç¬¦ï¼ˆ926ï¼‰
```

### 3 å¼  GPU

éƒ¨ç½²æœåŠ¡

```shell
CUDA_VISIBLE_DEVICES=0,1,3 python -m llama_cpp.server \
    --model qwen-7b-chat.Q5_K_M.gguf \
    --model_alias gpt-3.5-turbo \
    --n_gpu_layers -1 \
    --host 0.0.0.0 --port 8080 \
    --chat_format chatml
```

GPU ä½¿ç”¨æƒ…å†µ

```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       Off | 00000000:43:00.0 Off |                    0 |
| N/A   55C    P0              28W /  70W |   2186MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla T4                       Off | 00000000:47:00.0 Off |                    0 |
| N/A   54C    P0              28W /  70W |   2174MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  Tesla T4                       Off | 00000000:8E:00.0 Off |                    0 |
| N/A   43C    P0              27W /  70W |   2658MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

æµ‹è¯•ç»“æœ

```shell
ğŸš€ æ¯ç§’ç”Ÿæˆ Tokens: 15.50 	 åˆè®¡ Tokens ï¼ˆ1020ï¼‰ = è¾“å…¥ Tokensï¼ˆ20ï¼‰ + è¾“å‡º Tokensï¼ˆ1000ï¼‰
ğŸš€ æ¯ç§’ç”Ÿæˆå­—ç¬¦   : 29.94 	 åˆè®¡ç”Ÿæˆå­—ç¬¦ï¼ˆ1932ï¼‰
â±ï¸ ç”Ÿæˆè€—æ—¶: 64.54 ç§’
```

### 4 å¼  GPU

éƒ¨ç½²æœåŠ¡

```shell
python -m llama_cpp.server \
    --model qwen-7b-chat.Q5_K_M.gguf \
    --model_alias gpt-3.5-turbo \
    --n_gpu_layers -1 \
    --host 0.0.0.0 --port 8080 \
    --chat_format chatml
```

GPU ä½¿ç”¨æƒ…å†µ

```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       Off | 00000000:43:00.0 Off |                    0 |
| N/A   53C    P0              28W /  70W |   1838MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Tesla T4                       Off | 00000000:47:00.0 Off |                    0 |
| N/A   53C    P0              28W /  70W |   1656MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  Tesla T4                       Off | 00000000:8E:00.0 Off |                    0 |
| N/A   49C    P0              28W /  70W |   1656MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  Tesla T4                       Off | 00000000:92:00.0 Off |                    0 |
| N/A   34C    P0              26W /  70W |   2138MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
```

æµ‹è¯•ç»“æœ

```shell
ğŸš€ æ¯ç§’ç”Ÿæˆ Tokens: 15.57 	 åˆè®¡ Tokens ï¼ˆ655ï¼‰ = è¾“å…¥ Tokensï¼ˆ20ï¼‰ + è¾“å‡º Tokensï¼ˆ635ï¼‰
ğŸš€ æ¯ç§’ç”Ÿæˆå­—ç¬¦   : 30.53 	 åˆè®¡ç”Ÿæˆå­—ç¬¦ï¼ˆ1245ï¼‰
â±ï¸ ç”Ÿæˆè€—æ—¶: 40.77 ç§’
```

### æ€»ç»“

| æ˜¾å¡æ•°é‡ | æ˜¾å­˜ (MiB) | æ¯ç§’ç”Ÿæˆ Tokens | æ¯ç§’ç”Ÿæˆå­—ç¬¦ |
| :---: | ---: | ---: | ---: |
| 1 | 6478 | 21.06 | 40.33 |
| 2 | 6744 | 18.47 | 33.79 |
| 3 | 7018 | 15.50 | 29.94 |
| 4 | 7288 | 15.57 | 30.53 |


[llama.cpp]: https://github.com/ggerganov/llama.cpp
[llama-cpp-python]: https://github.com/abetlen/llama-cpp-python
[Qwen]: https://huggingface.co/Qwen
[DeepSeek]: https://huggingface.co/deepseek-ai
