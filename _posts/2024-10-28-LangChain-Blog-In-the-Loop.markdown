---
layout: single
title:  "LangChain Blog: In the Loop"
date:   2024-10-28 10:00:00 +0800
categories: [LangChain, Agent]
tags: [LangChain, Agent, Agentic, LLM]
---

## [What is an agent?](https://blog.langchain.dev/what-is-an-agent/)

â€œä»€ä¹ˆæ˜¯ä»£ç†ï¼Ÿâ€

å‡ ä¹æ¯å¤©éƒ½ä¼šæœ‰äººé—®æˆ‘è¿™ä¸ªé—®é¢˜ã€‚åœ¨ LangChainï¼Œæˆ‘ä»¬æ„å»ºå·¥å…·æ¥å¸®åŠ©å¼€å‘è€…æ„å»º LLM åº”ç”¨ç¨‹åºï¼Œç‰¹åˆ«æ˜¯é‚£äº›å……å½“æ¨ç†å¼•æ“å¹¶ä¸å¤–éƒ¨æ•°æ®å’Œè®¡ç®—æºäº¤äº’çš„åº”ç”¨ç¨‹åºã€‚è¿™åŒ…æ‹¬é€šå¸¸è¢«ç§°ä¸ºâ€œä»£ç†â€çš„ç³»ç»Ÿã€‚

æ¯ä¸ªäººä¼¼ä¹å¯¹ä»£ç†éƒ½æœ‰ç¨å¾®ä¸åŒçš„å®šä¹‰ã€‚æˆ‘çš„å®šä¹‰å¯èƒ½æ¯”å¤§å¤šæ•°äººæ›´æŠ€æœ¯æ€§ï¼š

ğŸ’¡ ä»£ç†æ˜¯ä¸€ä¸ªä½¿ç”¨ LLM æ¥å†³å®šåº”ç”¨ç¨‹åºæ§åˆ¶æµçš„ç³»ç»Ÿã€‚

å³ä½¿åœ¨è¿™é‡Œï¼Œæˆ‘ä¹Ÿæ‰¿è®¤æˆ‘çš„å®šä¹‰å¹¶ä¸å®Œç¾ã€‚äººä»¬é€šå¸¸è®¤ä¸ºä»£ç†æ˜¯é«˜çº§çš„ã€è‡ªä¸»çš„ã€ç±»äººçš„â€”â€”ä½†å¦‚æœæ˜¯ä¸€ä¸ªç®€å•çš„ç³»ç»Ÿï¼ŒLLM åœ¨ä¸¤ä¸ªä¸åŒè·¯å¾„ä¹‹é—´è¿›è¡Œè·¯ç”±å‘¢ï¼Ÿè¿™ç¬¦åˆæˆ‘çš„æŠ€æœ¯å®šä¹‰ï¼Œä½†ä¸ç¬¦åˆäººä»¬å¯¹ä»£ç†åº”å…·å¤‡èƒ½åŠ›çš„æ™®éçœ‹æ³•ã€‚å¾ˆéš¾å‡†ç¡®å®šä¹‰ä»€ä¹ˆæ˜¯ä»£ç†ï¼

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘éå¸¸å–œæ¬¢ Andrew Ng ä¸Šå‘¨çš„æ¨æ–‡ã€‚åœ¨æ¨æ–‡ä¸­ï¼Œä»–å»ºè®®â€œä¸å…¶äº‰è®ºå“ªäº›å·¥ä½œåº”è¢«åŒ…æ‹¬æˆ–æ’é™¤ä¸ºçœŸæ­£çš„ä»£ç†ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¿è®¤ç³»ç»Ÿå¯ä»¥æœ‰ä¸åŒç¨‹åº¦çš„ä»£ç†æ€§ã€‚â€å°±åƒè‡ªåŠ¨é©¾é©¶æ±½è½¦æœ‰ä¸åŒçš„è‡ªåŠ¨åŒ–çº§åˆ«ä¸€æ ·ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†ä»£ç†èƒ½åŠ›è§†ä¸ºä¸€ä¸ªå…‰è°±ã€‚æˆ‘éå¸¸åŒæ„è¿™ä¸ªè§‚ç‚¹ï¼Œæˆ‘è®¤ä¸º Andrew è¡¨è¾¾å¾—å¾ˆå¥½ã€‚å°†æ¥ï¼Œå½“æœ‰äººé—®æˆ‘ä»€ä¹ˆæ˜¯ä»£ç†æ—¶ï¼Œæˆ‘ä¼šè½¬è€Œè®¨è®ºä»€ä¹ˆæ˜¯â€œä»£ç†æ€§â€ã€‚

### ä»€ä¹ˆæ˜¯ä»£ç†æ€§ï¼ˆagenticï¼‰ï¼Ÿ

å»å¹´æˆ‘åœ¨ TED æ¼”è®²ä¸­è°ˆåˆ°äº† LLM ç³»ç»Ÿï¼Œå¹¶ä½¿ç”¨ä¸‹é¢çš„å¹»ç¯ç‰‡è®¨è®ºäº† LLM åº”ç”¨ç¨‹åºä¸­å­˜åœ¨çš„ä¸åŒè‡ªä¸»çº§åˆ«ã€‚

![](/images/2024/LangChain-Blog-In-the-Loop/Levels-of-autonomous-in-LLM-applications.png)

ä¸€ä¸ªç³»ç»Ÿè¶Šâ€œä»£ç†æ€§â€ï¼ŒLLM å†³å®šç³»ç»Ÿè¡Œä¸ºçš„ç¨‹åº¦å°±è¶Šé«˜ã€‚

ä½¿ç”¨ LLM å°†è¾“å…¥è·¯ç”±åˆ°ç‰¹å®šçš„ä¸‹æ¸¸å·¥ä½œæµä¸­å…·æœ‰ä¸€äº›å°çš„â€œä»£ç†æ€§â€è¡Œä¸ºã€‚è¿™å°†å±äºä¸Šå›¾ä¸­çš„è·¯ç”±å™¨ç±»åˆ«ã€‚

å¦‚æœä½ ä½¿ç”¨å¤šä¸ª LLM è¿›è¡Œå¤šä¸ªè·¯ç”±æ­¥éª¤å‘¢ï¼Ÿè¿™å°†ä»‹äºè·¯ç”±å™¨å’ŒçŠ¶æ€æœºä¹‹é—´ã€‚

å¦‚æœå…¶ä¸­ä¸€ä¸ªæ­¥éª¤å†³å®šæ˜¯å¦ç»§ç»­æˆ–ç»“æŸâ€”â€”æœ‰æ•ˆåœ°å…è®¸ç³»ç»Ÿåœ¨å®Œæˆä¹‹å‰å¾ªç¯è¿è¡Œï¼Ÿè¿™å°†å±äºçŠ¶æ€æœºã€‚

å¦‚æœç³»ç»Ÿæ­£åœ¨æ„å»ºå·¥å…·ï¼Œè®°ä½è¿™äº›å·¥å…·ï¼Œç„¶ååœ¨æœªæ¥çš„æ­¥éª¤ä¸­ä½¿ç”¨å®ƒä»¬ï¼Ÿè¿™ç±»ä¼¼äº Voyager è®ºæ–‡ä¸­å®ç°çš„å†…å®¹ï¼Œå…·æœ‰æé«˜çš„ä»£ç†æ€§ï¼Œå±äºæ›´é«˜çº§çš„è‡ªä¸»ä»£ç†ç±»åˆ«ã€‚

è¿™äº›â€œä»£ç†æ€§â€çš„å®šä¹‰ä»ç„¶ç›¸å½“æŠ€æœ¯æ€§ã€‚æˆ‘æ›´å–œæ¬¢â€œä»£ç†æ€§â€çš„æŠ€æœ¯å®šä¹‰ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºå®ƒåœ¨è®¾è®¡å’Œæè¿° LLM ç³»ç»Ÿæ—¶å¾ˆæœ‰ç”¨ã€‚

### ä¸ºä»€ä¹ˆâ€œä»£ç†æ€§â€æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ¦‚å¿µï¼Ÿ

ä¸æ‰€æœ‰æ¦‚å¿µä¸€æ ·ï¼Œå€¼å¾—é—®ä¸€ä¸‹ä¸ºä»€ä¹ˆæˆ‘ä»¬ç”šè‡³éœ€è¦â€œä»£ç†æ€§â€è¿™ä¸ªæ¦‚å¿µã€‚å®ƒæœ‰ä»€ä¹ˆå¸®åŠ©ï¼Ÿ

äº†è§£ç³»ç»Ÿçš„ä»£ç†æ€§å¯ä»¥æŒ‡å¯¼ä½ åœ¨å¼€å‘è¿‡ç¨‹ä¸­çš„å†³ç­–â€”â€”åŒ…æ‹¬æ„å»ºã€è¿è¡Œã€äº¤äº’ã€è¯„ä¼°ç”šè‡³ç›‘æ§å®ƒã€‚

ç³»ç»Ÿè¶Šä»£ç†æ€§ï¼Œç¼–æ’æ¡†æ¶çš„å¸®åŠ©å°±è¶Šå¤§ã€‚å¦‚æœä½ æ­£åœ¨è®¾è®¡ä¸€ä¸ªå¤æ‚çš„ä»£ç†æ€§ç³»ç»Ÿï¼Œæ‹¥æœ‰ä¸€ä¸ªå…·æœ‰æ­£ç¡®æŠ½è±¡æ¦‚å¿µçš„æ¡†æ¶å¯ä»¥åŠ å¿«å¼€å‘é€Ÿåº¦ã€‚è¿™ä¸ªæ¡†æ¶åº”è¯¥å¯¹åˆ†æ”¯é€»è¾‘å’Œå¾ªç¯æä¾›ä¸€æµçš„æ”¯æŒã€‚

ç³»ç»Ÿè¶Šä»£ç†æ€§ï¼Œè¿è¡Œèµ·æ¥å°±è¶Šå›°éš¾ã€‚å®ƒå°†å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œæœ‰äº›ä»»åŠ¡éœ€è¦å¾ˆé•¿æ—¶é—´æ‰èƒ½å®Œæˆã€‚è¿™æ„å‘³ç€ä½ ä¼šå¸Œæœ›å°†ä½œä¸šä½œä¸ºåå°è¿è¡Œã€‚è¿™ä¹Ÿæ„å‘³ç€ä½ éœ€è¦æŒä¹…çš„æ‰§è¡Œæ¥å¤„ç†ä¸­é€”å‘ç”Ÿçš„ä»»ä½•é”™è¯¯ã€‚

ç³»ç»Ÿè¶Šä»£ç†æ€§ï¼Œä½ å°±è¶Šå¸Œæœ›åœ¨è¿è¡Œæ—¶ä¸ä¹‹äº¤äº’ã€‚ä½ ä¼šå¸Œæœ›èƒ½å¤Ÿè§‚å¯Ÿå†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œå› ä¸ºç¡®åˆ‡çš„æ­¥éª¤å¯èƒ½äº‹å…ˆæœªçŸ¥ã€‚ä½ ä¼šå¸Œæœ›èƒ½å¤Ÿåœ¨ç‰¹å®šæ—¶é—´ç‚¹ä¿®æ”¹ä»£ç†çš„çŠ¶æ€æˆ–æŒ‡ä»¤ï¼Œå¦‚æœå®ƒåç¦»é¢„å®šè·¯å¾„ï¼Œå¯ä»¥å°†å…¶æ‹‰å›æ­£è½¨ã€‚

ç³»ç»Ÿè¶Šä»£ç†æ€§ï¼Œä½ å°±è¶Šå¸Œæœ›æœ‰ä¸€ä¸ªä¸ºè¿™äº›ç±»å‹çš„åº”ç”¨ç¨‹åºæ„å»ºçš„è¯„ä¼°æ¡†æ¶ã€‚ä½ ä¼šå¸Œæœ›å¤šæ¬¡è¿è¡Œè¯„ä¼°ï¼Œå› ä¸ºæœ‰å¤§é‡çš„éšæœºæ€§ã€‚ä½ ä¼šå¸Œæœ›èƒ½å¤Ÿæµ‹è¯•ä¸ä»…æ˜¯æœ€ç»ˆè¾“å‡ºï¼Œè¿˜åŒ…æ‹¬ä¸­é—´æ­¥éª¤ï¼Œä»¥æµ‹è¯•ä»£ç†çš„æ•ˆç‡ã€‚

ç³»ç»Ÿè¶Šä»£ç†æ€§ï¼Œä½ å°±è¶Šå¸Œæœ›æœ‰ä¸€ä¸ªæ–°çš„ç›‘æ§æ¡†æ¶ã€‚ä½ ä¼šå¸Œæœ›èƒ½å¤Ÿæ·±å…¥äº†è§£ä»£ç†é‡‡å–çš„æ‰€æœ‰æ­¥éª¤ã€‚ä½ è¿˜ä¼šå¸Œæœ›èƒ½å¤Ÿæ ¹æ®ä»£ç†é‡‡å–çš„æ­¥éª¤æŸ¥è¯¢è¿è¡Œæƒ…å†µã€‚

ç†è§£å’Œåˆ©ç”¨ç³»ç»Ÿä¸­ä»£ç†èƒ½åŠ›çš„å…‰è°±å¯ä»¥æé«˜å¼€å‘è¿‡ç¨‹çš„æ•ˆç‡å’Œç¨³å¥æ€§ã€‚

### ä»£ç†æ€§æ˜¯æ–°çš„

æˆ‘ç»å¸¸æ€è€ƒçš„ä¸€ä»¶äº‹æ˜¯ï¼Œåœ¨è¿™åœºç‹‚çƒ­ä¸­ï¼Œä»€ä¹ˆæ˜¯çœŸæ­£æ–°çš„ã€‚æˆ‘ä»¬æ˜¯å¦éœ€è¦æ–°çš„å·¥å…·å’ŒåŸºç¡€è®¾æ–½æ¥æ”¯æŒäººä»¬æ„å»ºçš„ LLM åº”ç”¨ç¨‹åºï¼Ÿè¿˜æ˜¯ä»¥å‰çš„é€šç”¨å·¥å…·å’ŒåŸºç¡€è®¾æ–½å°±è¶³å¤Ÿäº†ï¼Ÿ

å¯¹æˆ‘æ¥è¯´ï¼Œä½ çš„åº”ç”¨ç¨‹åºè¶Šä»£ç†æ€§ï¼Œæ‹¥æœ‰æ–°çš„å·¥å…·å’ŒåŸºç¡€è®¾æ–½å°±è¶Šå…³é”®ã€‚è¿™æ­£æ˜¯æˆ‘ä»¬æ„å»º LangGraphï¼ˆä¸€ä¸ªå¸®åŠ©æ„å»ºã€è¿è¡Œå’Œäº¤äº’ä»£ç†çš„ä»£ç†ç¼–æ’å™¨ï¼‰å’Œ LangSmithï¼ˆä¸€ä¸ªç”¨äº LLM åº”ç”¨ç¨‹åºçš„æµ‹è¯•å’Œå¯è§‚å¯Ÿæ€§å¹³å°ï¼‰çš„åŠ¨æœºã€‚éšç€æˆ‘ä»¬åœ¨ä»£ç†æ€§å…‰è°±ä¸Šè¿›ä¸€æ­¥å‘å±•ï¼Œæ•´ä¸ªæ”¯æŒå·¥å…·çš„ç”Ÿæ€ç³»ç»Ÿéœ€è¦é‡æ–°æ„æƒ³ã€‚


## [What is a "cognitive architecture"?](https://blog.langchain.dev/what-is-a-cognitive-architecture/)

æ›´æ–°ï¼šä¸€äº›è¯»è€…æŒ‡å‡ºï¼Œâ€œè®¤çŸ¥æ¶æ„â€ä¸€è¯åœ¨ç¥ç»ç§‘å­¦å’Œè®¡ç®—è®¤çŸ¥ç§‘å­¦ä¸­æœ‰ç€ä¸°å¯Œçš„å†å²ã€‚æ ¹æ®ç»´åŸºç™¾ç§‘ï¼Œâ€œè®¤çŸ¥æ¶æ„æ—¢æŒ‡å…³äºäººç±»å¿ƒæ™ºç»“æ„çš„ç†è®ºï¼Œä¹ŸæŒ‡è¿™ç§ç†è®ºçš„è®¡ç®—å®ç°â€ã€‚è¿™ä¸ªå®šä¹‰ï¼ˆä»¥åŠç›¸å…³çš„ç ”ç©¶å’Œæ–‡ç« ï¼‰æ¯”æˆ‘åœ¨è¿™é‡Œå°è¯•æä¾›çš„å®šä¹‰è¦å…¨é¢å¾—å¤šï¼Œå› æ­¤è¿™ç¯‡åšå®¢åº”è¯¥è¢«è§†ä¸ºæˆ‘åœ¨è¿‡å»ä¸€å¹´ä¸­æ„å»ºå’Œå¸®åŠ©æ„å»ºåŸºäºLLMçš„åº”ç”¨ç¨‹åºçš„ç»éªŒä¸è¿™ä¸€ç ”ç©¶é¢†åŸŸçš„æ˜ å°„ã€‚

åœ¨è¿‡å»å…­ä¸ªæœˆä¸­ï¼Œæˆ‘ç»å¸¸ä½¿ç”¨çš„ä¸€ä¸ªçŸ­è¯­æ˜¯â€œè®¤çŸ¥æ¶æ„â€ï¼Œå¹¶ä¸”å¯èƒ½ä¼šä½¿ç”¨æ›´å¤šã€‚è¿™æ˜¯æˆ‘ç¬¬ä¸€æ¬¡ä»Flo Crivelloé‚£é‡Œå¬åˆ°çš„æœ¯è¯­â€”â€”æ‰€æœ‰çš„åŠŸåŠ³éƒ½å½’äºä»–ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå¾ˆæ£’çš„æœ¯è¯­ã€‚é‚£ä¹ˆæˆ‘åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿ

æˆ‘æ‰€è¯´çš„è®¤çŸ¥æ¶æ„æ˜¯æŒ‡ä½ çš„ç³»ç»Ÿå¦‚ä½•æ€è€ƒâ€”â€”æ¢å¥è¯è¯´ï¼Œå°±æ˜¯ä»£ç /æç¤º/LLMè°ƒç”¨çš„æµç¨‹ï¼Œè¿™äº›æµç¨‹æ¥æ”¶ç”¨æˆ·è¾“å…¥å¹¶æ‰§è¡Œæ“ä½œæˆ–ç”Ÿæˆå“åº”ã€‚

æˆ‘å–œæ¬¢â€œè®¤çŸ¥â€è¿™ä¸ªè¯ï¼Œå› ä¸ºä»£ç†ç³»ç»Ÿä¾èµ–äºä½¿ç”¨LLMæ¥æ¨ç†è¯¥åšä»€ä¹ˆã€‚

æˆ‘å–œæ¬¢â€œæ¶æ„â€è¿™ä¸ªè¯ï¼Œå› ä¸ºè¿™äº›ä»£ç†ç³»ç»Ÿä»ç„¶æ¶‰åŠå¤§é‡ç±»ä¼¼äºä¼ ç»Ÿç³»ç»Ÿæ¶æ„çš„å·¥ç¨‹ã€‚

### å°†è‡ªä¸»çº§åˆ«æ˜ å°„åˆ°è®¤çŸ¥æ¶æ„

å¦‚æœæˆ‘ä»¬å›åˆ°è¿™å¼ å¹»ç¯ç‰‡ï¼ˆæœ€åˆæ¥è‡ªæˆ‘çš„TEDæ¼”è®²ï¼‰å…³äºLLMåº”ç”¨ç¨‹åºä¸­ä¸åŒè‡ªä¸»çº§åˆ«çš„å¹»ç¯ç‰‡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸åŒè®¤çŸ¥æ¶æ„çš„ä¾‹å­ã€‚

![](/images/2024/LangChain-Blog-In-the-Loop/Levels-of-autonomous-in-LLM-applications.png)

é¦–å…ˆæ˜¯çº¯ä»£ç â€”â€”ä¸€åˆ‡éƒ½æ˜¯ç¡¬ç¼–ç çš„ã€‚ç”šè‡³ä¸æ˜¯çœŸæ­£çš„è®¤çŸ¥æ¶æ„ã€‚

æ¥ä¸‹æ¥æ˜¯å•ä¸ªLLMè°ƒç”¨ã€‚ä¸€äº›æ•°æ®é¢„å¤„ç†å‰å’Œ/æˆ–åï¼Œä½†å•ä¸ªLLMè°ƒç”¨æ„æˆäº†åº”ç”¨ç¨‹åºçš„ä¸»è¦éƒ¨åˆ†ã€‚ç®€å•çš„èŠå¤©æœºå™¨äººå¯èƒ½å±äºè¿™ä¸€ç±»ã€‚

æ¥ä¸‹æ¥æ˜¯ä¸€ç³»åˆ—LLMè°ƒç”¨ã€‚è¿™ä¸ªåºåˆ—å¯ä»¥æ˜¯å°†é—®é¢˜åˆ†è§£æˆä¸åŒçš„æ­¥éª¤ï¼Œæˆ–è€…åªæ˜¯æœåŠ¡äºä¸åŒçš„ç›®çš„ã€‚æ›´å¤æ‚çš„RAGç®¡é“å±äºè¿™ä¸€ç±»ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªLLMè°ƒç”¨ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼Œç„¶åä½¿ç”¨ç¬¬äºŒä¸ªLLMè°ƒç”¨ç”Ÿæˆç­”æ¡ˆã€‚

ä¹‹åæ˜¯è·¯ç”±å™¨ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œä½ æå‰çŸ¥é“åº”ç”¨ç¨‹åºå°†é‡‡å–çš„æ‰€æœ‰æ­¥éª¤ã€‚ç°åœ¨ï¼Œä½ ä¸å†çŸ¥é“ã€‚LLMå†³å®šé‡‡å–å“ªäº›è¡ŒåŠ¨ã€‚è¿™å¢åŠ äº†ä¸€äº›éšæœºæ€§å’Œä¸å¯é¢„æµ‹æ€§ã€‚

ä¸‹ä¸€ä¸ªçº§åˆ«æ˜¯æˆ‘ç§°ä¹‹ä¸ºçŠ¶æ€æœºçš„ä¸œè¥¿ã€‚è¿™æ˜¯å°†LLMè¿›è¡Œä¸€äº›è·¯ç”±ä¸å¾ªç¯ç»“åˆèµ·æ¥ã€‚è¿™æ›´åŠ ä¸å¯é¢„æµ‹ï¼Œå› ä¸ºé€šè¿‡å°†è·¯ç”±å™¨ä¸å¾ªç¯ç»“åˆèµ·æ¥ï¼Œç³»ç»Ÿç†è®ºä¸Šå¯ä»¥è°ƒç”¨æ— é™æ•°é‡çš„LLMè°ƒç”¨ã€‚

è‡ªä¸»çº§åˆ«çš„æœ€ç»ˆçº§åˆ«æ˜¯æˆ‘ç§°ä¹‹ä¸ºä»£ç†ï¼Œæˆ–è€…çœŸæ­£çš„â€œè‡ªä¸»ä»£ç†â€ã€‚åœ¨çŠ¶æ€æœºä¸­ï¼Œä»ç„¶å¯¹å¯ä»¥é‡‡å–çš„è¡ŒåŠ¨å’Œåœ¨é‡‡å–è¯¥è¡ŒåŠ¨åæ‰§è¡Œçš„æµç¨‹æœ‰çº¦æŸã€‚åœ¨è‡ªä¸»ä»£ç†ä¸­ï¼Œè¿™äº›æŠ¤æ è¢«ç§»é™¤ã€‚ç³»ç»Ÿæœ¬èº«å¼€å§‹å†³å®šå¯ä»¥é‡‡å–å“ªäº›æ­¥éª¤ä»¥åŠæŒ‡ä»¤æ˜¯ä»€ä¹ˆï¼šè¿™å¯ä»¥é€šè¿‡æ›´æ–°æç¤ºã€å·¥å…·æˆ–ç”¨äºé©±åŠ¨ç³»ç»Ÿçš„ä»£ç æ¥å®Œæˆã€‚

### é€‰æ‹©è®¤çŸ¥æ¶æ„

å½“æˆ‘è°ˆè®ºâ€œé€‰æ‹©è®¤çŸ¥æ¶æ„â€æ—¶ï¼Œæˆ‘æŒ‡çš„æ˜¯é€‰æ‹©ä½ æƒ³é‡‡ç”¨çš„è¿™äº›æ¶æ„ä¸­çš„å“ªä¸€ä¸ªã€‚è¿™äº›æ¶æ„æ²¡æœ‰ä¸€ä¸ªæ˜¯ä¸¥æ ¼â€œä¼˜äºâ€å…¶ä»–çš„â€”â€”å®ƒä»¬éƒ½æœ‰å„è‡ªçš„ç›®çš„ï¼Œé€‚ç”¨äºä¸åŒçš„ä»»åŠ¡ã€‚

åœ¨æ„å»ºLLMåº”ç”¨ç¨‹åºæ—¶ï¼Œä½ å¯èƒ½ä¼šåƒå®éªŒæç¤ºä¸€æ ·é¢‘ç¹åœ°å®éªŒä¸åŒçš„è®¤çŸ¥æ¶æ„ã€‚æˆ‘ä»¬æ­£åœ¨æ„å»ºLangChainå’ŒLangGraphæ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬è¿‡å»ä¸€å¹´çš„å¤§éƒ¨åˆ†å¼€å‘å·¥ä½œéƒ½é›†ä¸­åœ¨æ„å»ºä½çº§ã€é«˜åº¦å¯æ§çš„ç¼–æ’æ¡†æ¶ï¼ˆLCELå’ŒLangGraphï¼‰ä¸Šã€‚

è¿™ä¸æ—©æœŸçš„LangChainæœ‰äº›ä¸åŒï¼Œæ—©æœŸçš„LangChainä¸“æ³¨äºæ˜“äºä½¿ç”¨çš„ç°æˆé“¾æ¡ã€‚è¿™äº›å¯¹äºå…¥é—¨éå¸¸æœ‰ç”¨ï¼Œä½†éš¾ä»¥å®šåˆ¶å’Œå®éªŒã€‚æ—©æœŸè¿™æ²¡é—®é¢˜ï¼Œå› ä¸ºæ¯ä¸ªäººéƒ½åªæ˜¯æƒ³å¼€å§‹ï¼Œä½†éšç€é¢†åŸŸçš„æˆç†Ÿï¼Œè®¾è®¡å¾ˆå¿«è¾¾åˆ°äº†å®ƒçš„æé™ã€‚

æˆ‘å¯¹æˆ‘ä»¬åœ¨è¿‡å»ä¸€å¹´ä¸­æ‰€åšçš„æ”¹å˜æ„Ÿåˆ°éå¸¸è‡ªè±ªï¼Œè¿™äº›æ”¹å˜ä½¿LangChainå’ŒLangGraphæ›´åŠ çµæ´»å’Œå¯å®šåˆ¶ã€‚å¦‚æœä½ åªé€šè¿‡é«˜çº§åŒ…è£…å™¨ä½¿ç”¨è¿‡LangChainï¼Œè¯·æŸ¥çœ‹ä½çº§éƒ¨åˆ†ã€‚å®ƒä»¬æ›´åŠ å¯å®šåˆ¶ï¼ŒçœŸçš„å¯ä»¥è®©ä½ æ§åˆ¶åº”ç”¨ç¨‹åºçš„è®¤çŸ¥æ¶æ„ã€‚

å¦‚æœä½ åœ¨æ„å»ºç®€å•çš„é“¾æ¡å’Œæ£€ç´¢æµç¨‹ï¼Œè¯·æŸ¥çœ‹Pythonå’ŒJavaScriptä¸­çš„LangChainã€‚å¯¹äºæ›´å¤æ‚çš„ä»£ç†å·¥ä½œæµï¼Œè¯·å°è¯•Pythonå’ŒJavaScriptä¸­çš„LangGraphã€‚


## [Why you should outsource your agentic infrastructure, but own your cognitive architecture](https://blog.langchain.dev/why-you-should-outsource-your-agentic-infrastructure-but-own-your-cognitive-architecture/)
ä¸ºä»€ä¹ˆä½ åº”è¯¥å¤–åŒ…ä½ çš„ä»£ç†åŸºç¡€è®¾æ–½ï¼Œä½†æ‹¥æœ‰ä½ çš„è®¤çŸ¥æ¶æ„

When OpenAI Assistants API came out, it was a bold step towards the future of agents. It moved OpenAI from a company producing LLM APIs to a company producing Agent APIs.
å½“ OpenAI åŠ©æ‰‹ API æ¨å‡ºæ—¶ï¼Œè¿™æ˜¯è¿ˆå‘ä»£ç†æœªæ¥çš„å¤§èƒ†ä¸€æ­¥ã€‚å®ƒå°† OpenAI ä»ä¸€å®¶ç”Ÿäº§ LLM API çš„å…¬å¸è½¬å˜ä¸ºä¸€å®¶ç”Ÿäº§ä»£ç† API çš„å…¬å¸ã€‚

There are several things that I think the OpenAI Assistants API got right - it introduced a lot of new and helpful infrastructure specifically aimed at running agentic applications. At the same time, it limits what â€œcognitive architecturesâ€ can be built on top of it for really complex (and valuable!) agents.
æˆ‘è®¤ä¸º OpenAI åŠ©æ‰‹ API åšå¯¹äº†å‡ ä»¶äº‹â€”â€”å®ƒå¼•å…¥äº†è®¸å¤šä¸“é—¨é’ˆå¯¹è¿è¡Œä»£ç†åº”ç”¨ç¨‹åºçš„æ–°ä¸”æœ‰ç”¨çš„åŸºç¡€è®¾æ–½ã€‚åŒæ—¶ï¼Œå®ƒé™åˆ¶äº†å¯ä»¥åœ¨å…¶ä¸Šæ„å»ºçš„â€œè®¤çŸ¥æ¶æ„â€ï¼Œä»¥ç”¨äºçœŸæ­£å¤æ‚ï¼ˆä¸”æœ‰ä»·å€¼ï¼ï¼‰çš„ä»£ç†ã€‚

This shows off the difference between â€œagentic infrastructureâ€ and â€œcognitive architecturesâ€. Jeff Bezos has the brilliant quote: â€œFocus on what makes your beer taste betterâ€. If we take this metaphor and apply it to companies building agents:
è¿™å±•ç¤ºäº†â€œä»£ç†åŸºç¡€è®¾æ–½â€å’Œâ€œè®¤çŸ¥æ¶æ„â€ä¹‹é—´çš„åŒºåˆ«ã€‚æ°å¤«Â·è´ç´¢æ–¯æœ‰ä¸€å¥ç²¾å½©çš„åè¨€ï¼šâ€œä¸“æ³¨äºè®©ä½ çš„å•¤é…’å‘³é“æ›´å¥½çš„ä¸œè¥¿â€ã€‚å¦‚æœæˆ‘ä»¬é‡‡ç”¨è¿™ä¸ªæ¯”å–»å¹¶å°†å…¶åº”ç”¨äºæ„å»ºä»£ç†çš„å…¬å¸ï¼š

ğŸ’¡
Agentic infrastructure does not make your beer taste better
ä»£ç†åŸºç¡€è®¾æ–½ä¸ä¼šè®©ä½ çš„å•¤é…’å‘³é“æ›´å¥½

ğŸ’¡
Cognitive architectures absolutely make your beer taste better
è®¤çŸ¥æ¶æ„ç»å¯¹ä¼šè®©ä½ çš„å•¤é…’å‘³é“æ›´å¥½

### The need for agentic infrastructure
ä»£ç†åŸºç¡€è®¾æ–½çš„éœ€æ±‚

OpenAI was pretty spot on in that developers want better infrastructure for running agentic applications. In particular:
OpenAI éå¸¸å‡†ç¡®åœ°æŒ‡å‡ºï¼Œå¼€å‘äººå‘˜éœ€è¦æ›´å¥½çš„åŸºç¡€è®¾æ–½æ¥è¿è¡Œä»£ç†åº”ç”¨ç¨‹åºã€‚ç‰¹åˆ«æ˜¯ï¼š

- The ability to â€œconfigureâ€ assistants with a prompt and tools made it easy to get started and create different agents
é€šè¿‡æç¤ºå’Œå·¥å…·â€œé…ç½®â€åŠ©æ‰‹çš„èƒ½åŠ›ä½¿å¾—å…¥é—¨å’Œåˆ›å»ºä¸åŒçš„ä»£ç†å˜å¾—å®¹æ˜“

- The ability to run assistants as background runs made it easier to manage longer running workflows
å°†åŠ©æ‰‹ä½œä¸ºåå°è¿è¡Œçš„èƒ½åŠ›ä½¿å¾—ç®¡ç†é•¿æ—¶é—´è¿è¡Œçš„å·¥ä½œæµç¨‹å˜å¾—æ›´å®¹æ˜“

- The built-in persistence of messages made it easy to manage state
å†…ç½®çš„æ¶ˆæ¯æŒä¹…æ€§ä½¿å¾—ç®¡ç†çŠ¶æ€å˜å¾—å®¹æ˜“

All of these things are things that developers shouldnâ€™t really have to think about. None of these things make your application differentiated - in Jeff Bezosâ€™s words, they donâ€™t make your beer taste better.
æ‰€æœ‰è¿™äº›äº‹æƒ…éƒ½æ˜¯å¼€å‘äººå‘˜ä¸åº”è¯¥çœŸæ­£è€ƒè™‘çš„äº‹æƒ…ã€‚è¿™äº›éƒ½ä¸ä¼šä½¿ä½ çš„åº”ç”¨ç¨‹åºä¸ä¼—ä¸åŒâ€”â€”ç”¨æ°å¤«Â·è´ç´¢æ–¯çš„è¯æ¥è¯´ï¼Œå®ƒä»¬ä¸ä¼šè®©ä½ çš„å•¤é…’å‘³é“æ›´å¥½ã€‚

There is still even more infrastructure that can be built to assist developers! In OpenAI Assistants AI, you currently canâ€™t run multiple runs on the same thread. You canâ€™t easily modify the state of a thread. Still - the Assistants API was a fantastic step in the right direction.
ä»ç„¶æœ‰æ›´å¤šçš„åŸºç¡€è®¾æ–½å¯ä»¥æ„å»ºæ¥å¸®åŠ©å¼€å‘äººå‘˜ï¼åœ¨ OpenAI åŠ©æ‰‹ AI ä¸­ï¼Œä½ ç›®å‰ä¸èƒ½åœ¨åŒä¸€ä¸ªçº¿ç¨‹ä¸Šè¿è¡Œå¤šä¸ªè¿è¡Œã€‚ä½ ä¸èƒ½è½»æ˜“ä¿®æ”¹çº¿ç¨‹çš„çŠ¶æ€ã€‚ä¸è¿‡â€”â€”åŠ©æ‰‹ API æ˜¯æœç€æ­£ç¡®æ–¹å‘è¿ˆå‡ºçš„ç»ä½³ä¸€æ­¥ã€‚

### The importance of an application-specific cognitive architecture
åº”ç”¨ç¨‹åºç‰¹å®šè®¤çŸ¥æ¶æ„çš„é‡è¦æ€§

The issue with the Assistants API is that it is too limiting in what you can easily build on top of it.
åŠ©æ‰‹ API çš„é—®é¢˜åœ¨äºå®ƒå¯¹ä½ å¯ä»¥è½»æ¾æ„å»ºçš„å†…å®¹é™åˆ¶å¤ªå¤šã€‚

If you are looking to build a chatbot - fantastic! The â€œstateâ€ of a thread is a list of messages, perfect for that.
å¦‚æœä½ æƒ³æ„å»ºä¸€ä¸ªèŠå¤©æœºå™¨äººâ€”â€”å¤ªæ£’äº†ï¼çº¿ç¨‹çš„â€œçŠ¶æ€â€æ˜¯æ¶ˆæ¯åˆ—è¡¨ï¼Œéå¸¸é€‚åˆã€‚

If you are looking to build a simple ReAct style agent - great! It also probably works well for that - basically just running an LLM in a while loop.
å¦‚æœä½ æƒ³æ„å»ºä¸€ä¸ªç®€å•çš„ ReAct é£æ ¼ä»£ç†â€”â€”å¾ˆå¥½ï¼å®ƒä¹Ÿå¯èƒ½å¯¹æ­¤éå¸¸æœ‰æ•ˆâ€”â€”åŸºæœ¬ä¸Šåªæ˜¯åœ¨ while å¾ªç¯ä¸­è¿è¡Œä¸€ä¸ª LLMã€‚

But agentic applications are more than just a single chat model invoking the same tools with the same prompt over and over again. They have more complex state that they track than just a list of messages. This control over the state and flow of an application is crucial for bringing any semblance of reliability to agents.
ä½†ä»£ç†åº”ç”¨ç¨‹åºä¸ä»…ä»…æ˜¯ä¸€ä¸ªå•ä¸€çš„èŠå¤©æ¨¡å‹ï¼Œä¸€éåˆä¸€éåœ°è°ƒç”¨ç›¸åŒçš„å·¥å…·å’Œæç¤ºã€‚å®ƒä»¬è·Ÿè¸ªçš„çŠ¶æ€æ¯”æ¶ˆæ¯åˆ—è¡¨æ›´å¤æ‚ã€‚å¯¹åº”ç”¨ç¨‹åºçŠ¶æ€å’Œæµç¨‹çš„æ§åˆ¶å¯¹äºä¸ºä»£ç†å¸¦æ¥ä»»ä½•å¯é æ€§è‡³å…³é‡è¦ã€‚

From working with thousands of builders, we see that the agents making their way to production all have slightly different cognitive architectures. The cognitive architecture of an application is how you get it to really work well - this is where teams are innovating. This is what they are building to make their application differentiated - to make their beer taste better.
é€šè¿‡ä¸æˆåƒä¸Šä¸‡çš„æ„å»ºè€…åˆä½œï¼Œæˆ‘ä»¬çœ‹åˆ°è¿›å…¥ç”Ÿäº§çš„ä»£ç†éƒ½æœ‰ç•¥å¾®ä¸åŒçš„è®¤çŸ¥æ¶æ„ã€‚åº”ç”¨ç¨‹åºçš„è®¤çŸ¥æ¶æ„æ˜¯è®©å®ƒçœŸæ­£è¿è¡Œè‰¯å¥½çš„æ–¹å¼â€”â€”è¿™æ˜¯å›¢é˜Ÿåˆ›æ–°çš„åœ°æ–¹ã€‚è¿™æ˜¯ä»–ä»¬æ„å»ºçš„ä½¿å…¶åº”ç”¨ç¨‹åºä¸ä¼—ä¸åŒçš„ä¸œè¥¿â€”â€”è®©ä»–ä»¬çš„å•¤é…’å‘³é“æ›´å¥½ã€‚

This isnâ€™t to say you canâ€™t do more complex things with the Assistants API. You probably can. But the API doesnâ€™t make it easy. It doesnâ€™t want you to. OpenAI made a bet on a generic cognitive architecture, which in turn makes it hard to build the application-specific cognitive architectures that are needed to make agents reliable.
è¿™å¹¶ä¸æ˜¯è¯´ä½ ä¸èƒ½ç”¨åŠ©æ‰‹ API åšæ›´å¤æ‚çš„äº‹æƒ…ã€‚ä½ å¯èƒ½å¯ä»¥ã€‚ä½† API å¹¶ä¸å®¹æ˜“ã€‚å®ƒä¸å¸Œæœ›ä½ è¿™æ ·åšã€‚OpenAI æŠ¼æ³¨äºé€šç”¨çš„è®¤çŸ¥æ¶æ„ï¼Œè¿™åè¿‡æ¥ä½¿å¾—æ„å»ºä½¿ä»£ç†å¯é çš„åº”ç”¨ç¨‹åºç‰¹å®šè®¤çŸ¥æ¶æ„å˜å¾—å›°éš¾ã€‚

### Why do we care?
æˆ‘ä»¬ä¸ºä»€ä¹ˆå…³å¿ƒï¼Ÿ

Why do I care so much? Why am I writing so many words on this? Itâ€™s because I actually think OpenAI got a lot of things right, and they took a stance early in the market that there is a need for agentic infrastructure. They made it easy for developers not to worry about where to store the state of their agents, how to manage a task queue, etc â€” which is fantastic.
æˆ‘ä¸ºä»€ä¹ˆè¿™ä¹ˆåœ¨æ„ï¼Ÿä¸ºä»€ä¹ˆæˆ‘è¦å†™è¿™ä¹ˆå¤šå­—ï¼Ÿå› ä¸ºæˆ‘å®é™…ä¸Šè®¤ä¸º OpenAI åšå¯¹äº†å¾ˆå¤šäº‹æƒ…ï¼Œä»–ä»¬åœ¨å¸‚åœºæ—©æœŸå°±é‡‡å–äº†ç«‹åœºï¼Œè®¤ä¸ºéœ€è¦ä»£ç†åŸºç¡€è®¾æ–½ã€‚ä»–ä»¬ä½¿å¼€å‘äººå‘˜ä¸å¿…æ‹…å¿ƒåœ¨å“ªé‡Œå­˜å‚¨ä»£ç†çš„çŠ¶æ€ã€å¦‚ä½•ç®¡ç†ä»»åŠ¡é˜Ÿåˆ—ç­‰â€”â€”è¿™éå¸¸æ£’ã€‚

Our goal at LangChain is to make it as easy as possible to build agentic applications. This type of infrastructure is absolutely part of what is needed.
LangChain çš„ç›®æ ‡æ˜¯ä½¿æ„å»ºä»£ç†åº”ç”¨ç¨‹åºå°½å¯èƒ½å®¹æ˜“ã€‚è¿™ç§ç±»å‹çš„åŸºç¡€è®¾æ–½ç»å¯¹æ˜¯æ‰€éœ€çš„ä¸€éƒ¨åˆ†ã€‚

We want to bring the benefits of that agentic infrastructure and marry it with the control that LangGraph gives you over your cognitive architecture. Thatâ€™s why we built LangGraph Cloud. Write your custom cognitive architecture with LangGraph, then deploy it with LangGraph Cloud and get all the benefits of this agentic infrastructure.
æˆ‘ä»¬å¸Œæœ›å°†è¿™ç§ä»£ç†åŸºç¡€è®¾æ–½çš„å¥½å¤„ä¸ LangGraph æä¾›çš„å¯¹è®¤çŸ¥æ¶æ„çš„æ§åˆ¶ç»“åˆèµ·æ¥ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ„å»º LangGraph Cloud çš„åŸå› ã€‚ä½¿ç”¨ LangGraph ç¼–å†™ä½ çš„è‡ªå®šä¹‰è®¤çŸ¥æ¶æ„ï¼Œç„¶åä½¿ç”¨ LangGraph Cloud éƒ¨ç½²å®ƒï¼Œå¹¶è·å¾—è¿™ç§ä»£ç†åŸºç¡€è®¾æ–½çš„æ‰€æœ‰å¥½å¤„ã€‚

LangGraph Cloud provides fault-tolerant scalability, optimized for real-world interactions. We designed it to have horizontally-scaling task queues and servers, a built-in persistence layer optimized for heavy loads, and configurable caching of nodes across runs. This lets you own the differentiating parts of your application and outsource the rest.
LangGraph Cloud æä¾›å®¹é”™æ‰©å±•æ€§ï¼Œé’ˆå¯¹ç°å®ä¸–ç•Œçš„äº¤äº’è¿›è¡Œäº†ä¼˜åŒ–ã€‚æˆ‘ä»¬è®¾è®¡äº†æ°´å¹³æ‰©å±•çš„ä»»åŠ¡é˜Ÿåˆ—å’ŒæœåŠ¡å™¨ï¼Œå†…ç½®çš„æŒä¹…å±‚é’ˆå¯¹é‡è´Ÿè½½è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¹¶ä¸”å¯ä»¥è·¨è¿è¡Œé…ç½®èŠ‚ç‚¹ç¼“å­˜ã€‚è¿™è®©ä½ æ‹¥æœ‰åº”ç”¨ç¨‹åºçš„å·®å¼‚åŒ–éƒ¨åˆ†ï¼Œå¹¶å°†å…¶ä½™éƒ¨åˆ†å¤–åŒ…ã€‚

In conclusion, focus on what makes your beer taste better: cognitive architectures, not infrastructure.
æ€»ä¹‹ï¼Œä¸“æ³¨äºè®©ä½ çš„å•¤é…’å‘³é“æ›´å¥½çš„ä¸œè¥¿ï¼šè®¤çŸ¥æ¶æ„ï¼Œè€Œä¸æ˜¯åŸºç¡€è®¾æ–½ã€‚


## [Planning for Agents](https://blog.langchain.dev/planning-for-agents/)
è§„åˆ’ä»£ç†

At Sequoiaâ€™s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. Check out that talk here. In this post, I will dive more into planning for agents.
åœ¨ä¸‰æœˆçš„çº¢æ‰AI Ascentä¼šè®®ä¸Šï¼Œæˆ‘è°ˆåˆ°äº†ä»£ç†çš„ä¸‰ä¸ªå±€é™æ€§ï¼šè§„åˆ’ã€ç”¨æˆ·ä½“éªŒå’Œè®°å¿†ã€‚åœ¨è¿™é‡ŒæŸ¥çœ‹è¯¥æ¼”è®²ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†æ·±å…¥æ¢è®¨ä»£ç†çš„è§„åˆ’ã€‚

If you ask any developer building agents with LLMs, he or she will probably cite the inability for agents to plan and reason well as a big pain point for agent reliability. What does planning mean for an agent, and how do we see people currently overcoming this shortcoming? What is (our best guess at what) the future of planning and reasoning for agents will look like? Weâ€™ll cover all of this below.
å¦‚æœä½ é—®ä»»ä½•ä½¿ç”¨LLMæ„å»ºä»£ç†çš„å¼€å‘äººå‘˜ï¼Œä»–æˆ–å¥¹å¯èƒ½ä¼šæåˆ°ä»£ç†æ— æ³•å¾ˆå¥½åœ°è§„åˆ’å’Œæ¨ç†æ˜¯ä»£ç†å¯é æ€§çš„ä¸€å¤§ç—›ç‚¹ã€‚è§„åˆ’å¯¹ä»£ç†æ„å‘³ç€ä»€ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•çœ‹åˆ°äººä»¬ç›®å‰å…‹æœè¿™ä¸€ç¼ºç‚¹ï¼Ÿæˆ‘ä»¬å¯¹ä»£ç†è§„åˆ’å’Œæ¨ç†çš„æœªæ¥ï¼ˆæˆ‘ä»¬æœ€å¥½çš„çŒœæµ‹ï¼‰ä¼šæ˜¯ä»€ä¹ˆæ ·å­ï¼Ÿæˆ‘ä»¬å°†åœ¨ä¸‹é¢è®¨è®ºæ‰€æœ‰è¿™äº›ã€‚

### What exactly is meant by planning and reasoning?
è§„åˆ’å’Œæ¨ç†åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

Planning and reasoning by an agent involves the LLMâ€™s ability to think about what actions to take. This involves both short-term and long term steps. The LLM evaluates all available information and then decides: what are the series of steps that I need to take, and which is the first one I should take right now?
ä»£ç†çš„è§„åˆ’å’Œæ¨ç†æ¶‰åŠLLMæ€è€ƒé‡‡å–å“ªäº›è¡ŒåŠ¨çš„èƒ½åŠ›ã€‚è¿™æ¶‰åŠçŸ­æœŸå’Œé•¿æœŸæ­¥éª¤ã€‚LLMè¯„ä¼°æ‰€æœ‰å¯ç”¨ä¿¡æ¯ï¼Œç„¶åå†³å®šï¼šæˆ‘éœ€è¦é‡‡å–å“ªäº›æ­¥éª¤ï¼Œå“ªä¸€ä¸ªæ˜¯æˆ‘ç°åœ¨åº”è¯¥é‡‡å–çš„ç¬¬ä¸€ä¸ªæ­¥éª¤ï¼Ÿ

Most of the time, developers use function calling (also known as tool calling) to let LLMs choose what actions to take. Function calling is a capability first added to LLM APIs by OpenAI in June of 2023 and then by others in late 2023/early 2024. With function calling, you can provide JSON schemas for different functions and have the LLM output object match one (or more) of those schemas. For more information on how to do this, see our guide here.
å¤§å¤šæ•°æ—¶å€™ï¼Œå¼€å‘äººå‘˜ä½¿ç”¨å‡½æ•°è°ƒç”¨ï¼ˆä¹Ÿç§°ä¸ºå·¥å…·è°ƒç”¨ï¼‰è®©LLMé€‰æ‹©é‡‡å–å“ªäº›è¡ŒåŠ¨ã€‚å‡½æ•°è°ƒç”¨æ˜¯OpenAIåœ¨2023å¹´6æœˆé¦–æ¬¡æ·»åŠ åˆ°LLM APIä¸­çš„åŠŸèƒ½ï¼Œç„¶ååœ¨2023å¹´åº•/2024å¹´åˆç”±å…¶ä»–äººæ·»åŠ ã€‚é€šè¿‡å‡½æ•°è°ƒç”¨ï¼Œæ‚¨å¯ä»¥ä¸ºä¸åŒçš„å‡½æ•°æä¾›JSONæ¨¡å¼ï¼Œå¹¶è®©LLMè¾“å‡ºå¯¹è±¡åŒ¹é…å…¶ä¸­ä¸€ä¸ªï¼ˆæˆ–å¤šä¸ªï¼‰æ¨¡å¼ã€‚æœ‰å…³å¦‚ä½•æ‰§è¡Œæ­¤æ“ä½œçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æˆ‘ä»¬çš„æŒ‡å—ã€‚

Function calling is used to let the agent choose what to do as an immediate action. Often times though, to successfully accomplish a complex task you need to take a number of actions in sequence. This long-term planning and reasoning is a tougher task for LLMs for a few reasons. First, the LLM must think about a longer time-horizon goal, but then jump back into a short-term action to take. Second, as the agent takes more and more actions, the results of those actions are fed back to the LLM; this lead to the context window growing, which can cause the LLM to get â€œdistractedâ€ and perform poorly.
å‡½æ•°è°ƒç”¨ç”¨äºè®©ä»£ç†é€‰æ‹©ä½œä¸ºå³æ—¶è¡ŒåŠ¨çš„æ“ä½œã€‚ç„¶è€Œï¼ŒæˆåŠŸå®Œæˆå¤æ‚ä»»åŠ¡é€šå¸¸éœ€è¦æŒ‰é¡ºåºé‡‡å–å¤šä¸ªè¡ŒåŠ¨ã€‚é•¿æœŸè§„åˆ’å’Œæ¨ç†å¯¹LLMæ¥è¯´æ˜¯ä¸€ä¸ªæ›´è‰°éš¾çš„ä»»åŠ¡ï¼ŒåŸå› æœ‰å‡ ä¸ªã€‚é¦–å…ˆï¼ŒLLMå¿…é¡»è€ƒè™‘ä¸€ä¸ªæ›´é•¿æ—¶é—´èŒƒå›´çš„ç›®æ ‡ï¼Œç„¶åå†å›åˆ°è¦é‡‡å–çš„çŸ­æœŸè¡ŒåŠ¨ã€‚å…¶æ¬¡ï¼Œéšç€ä»£ç†é‡‡å–è¶Šæ¥è¶Šå¤šçš„è¡ŒåŠ¨ï¼Œè¿™äº›è¡ŒåŠ¨çš„ç»“æœä¼šåé¦ˆç»™LLMï¼›è¿™å¯¼è‡´ä¸Šä¸‹æ–‡çª—å£å¢é•¿ï¼Œå¯èƒ½ä¼šå¯¼è‡´LLMâ€œåˆ†å¿ƒâ€å¹¶è¡¨ç°ä¸ä½³ã€‚

Like most things in the LLM world, it is tough to measure exactly how well current models do at planning and reasoning. There are reasonable benchmarks like the Berkeley Function Calling Leaderboard for evaluating function calling. Weâ€™ve done a little research on evaluating multi-step applications. But the best way to get a sense for this is build up an evaluation set for your specific problem and attempt to evaluate on that yourself.
åƒLLMä¸–ç•Œä¸­çš„å¤§å¤šæ•°äº‹æƒ…ä¸€æ ·ï¼Œå¾ˆéš¾å‡†ç¡®è¡¡é‡å½“å‰æ¨¡å‹åœ¨è§„åˆ’å’Œæ¨ç†æ–¹é¢çš„è¡¨ç°ã€‚è¯„ä¼°å‡½æ•°è°ƒç”¨æœ‰åˆç†çš„åŸºå‡†ï¼Œå¦‚ä¼¯å…‹åˆ©å‡½æ•°è°ƒç”¨æ’è¡Œæ¦œã€‚æˆ‘ä»¬å·²ç»å¯¹è¯„ä¼°å¤šæ­¥éª¤åº”ç”¨ç¨‹åºè¿›è¡Œäº†ä¸€äº›ç ”ç©¶ã€‚ä½†æœ€å¥½çš„æ–¹æ³•æ˜¯ä¸ºæ‚¨çš„ç‰¹å®šé—®é¢˜å»ºç«‹ä¸€ä¸ªè¯„ä¼°é›†ï¼Œå¹¶å°è¯•è‡ªè¡Œè¯„ä¼°ã€‚

ğŸ’¡
Anecdotally, it's a common conclusion that planning and reasoning is still not at the level itâ€™s needed to be for real-world tasks.
æ®ä¼ ï¼Œæ™®éçš„ç»“è®ºæ˜¯ï¼Œè§„åˆ’å’Œæ¨ç†ä»æœªè¾¾åˆ°å®é™…ä»»åŠ¡æ‰€éœ€çš„æ°´å¹³ã€‚

### What are current fixes to improve planning by agents?
ç›®å‰æœ‰å“ªäº›æ”¹è¿›ä»£ç†è§„åˆ’çš„æ–¹æ³•ï¼Ÿ

The lowest hanging fix for improving planning is to ensuring the LLM has all the information required to reason/plan appropriately. As basic as this sounds, oftentimes the prompt being passed into the LLM simply does not contain enough information for the LLM to make a reasonable decision. Adding a retrieval step, or clarifying the prompt instructions, can be an easy improvement. Thatâ€™s why its crucial to actually look at the data and see what the LLM is actually seeing.
æ”¹è¿›è§„åˆ’çš„æœ€ä½æ‚¬æŒ‚ä¿®å¤æ˜¯ç¡®ä¿LLMæ‹¥æœ‰è¿›è¡Œé€‚å½“æ¨ç†/è§„åˆ’æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚å°½ç®¡è¿™å¬èµ·æ¥å¾ˆåŸºæœ¬ï¼Œä½†é€šå¸¸ä¼ é€’ç»™LLMçš„æç¤ºæ ¹æœ¬ä¸åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ï¼ŒLLMæ— æ³•åšå‡ºåˆç†çš„å†³å®šã€‚æ·»åŠ æ£€ç´¢æ­¥éª¤æˆ–æ¾„æ¸…æç¤ºè¯´æ˜å¯ä»¥æ˜¯ä¸€ä¸ªç®€å•çš„æ”¹è¿›ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®é™…æŸ¥çœ‹æ•°æ®å¹¶æŸ¥çœ‹LLMå®é™…çœ‹åˆ°çš„å†…å®¹è‡³å…³é‡è¦ã€‚

After that, Iâ€™d recommend you try changing the cognitive architecture of your application. By â€œcognitive architectureâ€, I mean the data engineering logic your application uses to reason. There are two categories of cognitive architectures you can look towards to improve reasoning: general purpose cognitive architectures and domain specific cognitive architectures.
åœ¨é‚£ä¹‹åï¼Œæˆ‘å»ºè®®æ‚¨å°è¯•æ›´æ”¹åº”ç”¨ç¨‹åºçš„è®¤çŸ¥æ¶æ„ã€‚é€šè¿‡â€œè®¤çŸ¥æ¶æ„â€ï¼Œæˆ‘çš„æ„æ€æ˜¯æ‚¨çš„åº”ç”¨ç¨‹åºç”¨äºæ¨ç†çš„æ•°æ®å·¥ç¨‹é€»è¾‘ã€‚æ‚¨å¯ä»¥è€ƒè™‘æ”¹è¿›æ¨ç†çš„ä¸¤ç±»è®¤çŸ¥æ¶æ„ï¼šé€šç”¨è®¤çŸ¥æ¶æ„å’Œç‰¹å®šé¢†åŸŸè®¤çŸ¥æ¶æ„ã€‚

### General purpose cognitive architectures vs domain specific cognitive architectures
é€šç”¨è®¤çŸ¥æ¶æ„ä¸ç‰¹å®šé¢†åŸŸè®¤çŸ¥æ¶æ„

General purpose cognitive architectures attempt to achieve better reasoning generically. They can be applied to any task. One good example of this is the â€œplan and solveâ€ architecture. This paper explores an architecture where first you come up with a plan, and then execute on each step in that plan. Another general purpose architecture is the Reflexion architecture. This paper explores putting an explicit â€œreflectionâ€ step after the agent does a task to reflect on whether it did it correctly or not.
é€šç”¨è®¤çŸ¥æ¶æ„è¯•å›¾åœ¨ä¸€èˆ¬æƒ…å†µä¸‹å®ç°æ›´å¥½çš„æ¨ç†ã€‚å®ƒä»¬å¯ä»¥åº”ç”¨äºä»»ä½•ä»»åŠ¡ã€‚ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯â€œè®¡åˆ’å’Œè§£å†³â€æ¶æ„ã€‚è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ¶æ„ï¼Œé¦–å…ˆä½ æå‡ºä¸€ä¸ªè®¡åˆ’ï¼Œç„¶åæ‰§è¡Œè¯¥è®¡åˆ’ä¸­çš„æ¯ä¸€æ­¥ã€‚å¦ä¸€ä¸ªé€šç”¨æ¶æ„æ˜¯Reflexionæ¶æ„ã€‚è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨ä»£ç†æ‰§è¡Œä»»åŠ¡åæ”¾ç½®ä¸€ä¸ªæ˜ç¡®çš„â€œåæ€â€æ­¥éª¤ï¼Œä»¥åæ€å®ƒæ˜¯å¦æ­£ç¡®å®Œæˆäº†ä»»åŠ¡ã€‚

Though these ideas show improvement, they are often too general for practical use by agents in production. Rather, we see agents being built with domain-specific cognitive architectures. This often manifests in domain-specific classification/routing steps, domain specific verification steps. Some of the general ideas of planning and reflection can be applied here, but they are often applied in a domain specific way.
å°½ç®¡è¿™äº›æƒ³æ³•æ˜¾ç¤ºå‡ºæ”¹è¿›ï¼Œä½†å®ƒä»¬é€šå¸¸å¯¹äºç”Ÿäº§ä¸­çš„ä»£ç†å®é™…ä½¿ç”¨æ¥è¯´è¿‡äºç¬¼ç»Ÿã€‚ç›¸åï¼Œæˆ‘ä»¬çœ‹åˆ°ä»£ç†æ˜¯ç”¨ç‰¹å®šé¢†åŸŸçš„è®¤çŸ¥æ¶æ„æ„å»ºçš„ã€‚è¿™é€šå¸¸è¡¨ç°ä¸ºç‰¹å®šé¢†åŸŸçš„åˆ†ç±»/è·¯ç”±æ­¥éª¤ï¼Œç‰¹å®šé¢†åŸŸçš„éªŒè¯æ­¥éª¤ã€‚è§„åˆ’å’Œåæ€çš„ä¸€äº›ä¸€èˆ¬æƒ³æ³•å¯ä»¥åœ¨è¿™é‡Œåº”ç”¨ï¼Œä½†å®ƒä»¬é€šå¸¸ä»¥ç‰¹å®šé¢†åŸŸçš„æ–¹å¼åº”ç”¨ã€‚

As a concrete example, letâ€™s look at the AlphaCodium paper. This achieved state-of-the-art performance by using what they called â€œflow engineeringâ€ (another way to talk about cognitive architectures). See a diagram of the flow they use below.
ä½œä¸ºä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹AlphaCodiumè®ºæ–‡ã€‚é€šè¿‡ä½¿ç”¨ä»–ä»¬æ‰€è°“çš„â€œæµå·¥ç¨‹â€ï¼ˆå¦ä¸€ç§è°ˆè®ºè®¤çŸ¥æ¶æ„çš„æ–¹å¼ï¼‰ï¼Œå®ƒå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯·å‚è§ä¸‹é¢ä»–ä»¬ä½¿ç”¨çš„æµç¨‹å›¾ã€‚

![](/images/2024/LangChain-Blog-In-the-Loop/AlphaCodium-flow-engineering.png)

The flow is VERY specific to the problem they are trying to solve. They are telling the agent what to do in steps - come up with tests, then come up with a solution, then iterate on more tests, etc. This cognitive architecture is highly domain specific - it wouldnâ€™t help you write essays, for example.
è¯¥æµç¨‹éå¸¸å…·ä½“åœ°é’ˆå¯¹ä»–ä»¬è¯•å›¾è§£å†³çš„é—®é¢˜ã€‚ä»–ä»¬å‘Šè¯‰ä»£ç†åˆ†æ­¥éª¤åšä»€ä¹ˆâ€”â€”æå‡ºæµ‹è¯•ï¼Œç„¶åæå‡ºè§£å†³æ–¹æ¡ˆï¼Œç„¶ååœ¨æ›´å¤šæµ‹è¯•ä¸Šè¿­ä»£ï¼Œç­‰ç­‰ã€‚è¿™ç§è®¤çŸ¥æ¶æ„æ˜¯é«˜åº¦ç‰¹å®šé¢†åŸŸçš„â€”â€”ä¾‹å¦‚ï¼Œå®ƒä¸ä¼šå¸®åŠ©ä½ å†™æ–‡ç« ã€‚

### Why are domain specific cognitive architectures so helpful?
ä¸ºä»€ä¹ˆç‰¹å®šé¢†åŸŸçš„è®¤çŸ¥æ¶æ„å¦‚æ­¤æœ‰ç”¨ï¼Ÿ

There are two ways I like to think about this.
æˆ‘å–œæ¬¢ä»ä¸¤ä¸ªæ–¹é¢è€ƒè™‘è¿™ä¸ªé—®é¢˜ã€‚

First: you can view this as just another method of communicating to the agent what it should do. You can communicate instructions in prompt instructions, or you can hardcode specific transitions in code. Either one is valid! Code is fantastic way of communicating what you want to have happen.
é¦–å…ˆï¼šä½ å¯ä»¥å°†å…¶è§†ä¸ºå‘ä»£ç†ä¼ è¾¾å…¶åº”åšä»€ä¹ˆçš„å¦ä¸€ç§æ–¹æ³•ã€‚æ‚¨å¯ä»¥åœ¨æç¤ºè¯´æ˜ä¸­ä¼ è¾¾æŒ‡ä»¤ï¼Œæˆ–è€…å¯ä»¥åœ¨ä»£ç ä¸­ç¡¬ç¼–ç ç‰¹å®šçš„è½¬æ¢ã€‚ä¸¤è€…éƒ½æ˜¯æœ‰æ•ˆçš„ï¼ä»£ç æ˜¯ä¼ è¾¾æ‚¨æƒ³è¦å‘ç”Ÿçš„äº‹æƒ…çš„ç»ä½³æ–¹å¼ã€‚

Second: this is essentially removing the planning responsibilities from the LLM to us as engineers. We are are basically saying: â€œdonâ€™t worry about planning, LLM, Iâ€™ll do it for you!â€ Of course, weâ€™re not removing ALL planning from the LLM, as we still ask it do some planning in some instances.
å…¶æ¬¡ï¼šè¿™æœ¬è´¨ä¸Šæ˜¯å°†è§„åˆ’è´£ä»»ä»LLMè½¬ç§»åˆ°æˆ‘ä»¬å·¥ç¨‹å¸ˆèº«ä¸Šã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯åœ¨è¯´ï¼šâ€œä¸è¦æ‹…å¿ƒè§„åˆ’ï¼ŒLLMï¼Œæˆ‘ä¼šä¸ºä½ åšï¼â€å½“ç„¶ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰ä»LLMä¸­åˆ é™¤æ‰€æœ‰è§„åˆ’ï¼Œå› ä¸ºæˆ‘ä»¬ä»ç„¶åœ¨æŸäº›æƒ…å†µä¸‹è¦æ±‚å®ƒè¿›è¡Œä¸€äº›è§„åˆ’ã€‚

For example, letâ€™s look back at the AlphaCodium example above. The steps in the flow are basically us doing planning for the LLM! Weâ€™re telling it to first write tests, then code, then run the tests, etc. This is presumably what the authors thought a good plan for writing software was. If they were planning how to do a problem, this is how they would do it. And rather than tell the LLM to do in the prompt - where it may ignore it, not understand it, not have all the details - they told the system to do it by constructing a domain specific cognitive architecture.
ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸Šé¢çš„AlphaCodiumç¤ºä¾‹ã€‚æµç¨‹ä¸­çš„æ­¥éª¤åŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬ä¸ºLLMè¿›è¡Œè§„åˆ’ï¼æˆ‘ä»¬å‘Šè¯‰å®ƒå…ˆç¼–å†™æµ‹è¯•ï¼Œç„¶åç¼–å†™ä»£ç ï¼Œç„¶åè¿è¡Œæµ‹è¯•ï¼Œç­‰ç­‰ã€‚è¿™å¤§æ¦‚æ˜¯ä½œè€…è®¤ä¸ºç¼–å†™è½¯ä»¶çš„å¥½è®¡åˆ’ã€‚å¦‚æœä»–ä»¬åœ¨è§„åˆ’å¦‚ä½•è§£å†³é—®é¢˜ï¼Œè¿™å°±æ˜¯ä»–ä»¬ä¼šè¿™æ ·åšçš„æ–¹å¼ã€‚ä¸å…¶åœ¨æç¤ºä¸­å‘Šè¯‰LLMå»åšâ€”â€”å®ƒå¯èƒ½ä¼šå¿½ç•¥å®ƒï¼Œä¸ç†è§£å®ƒï¼Œæ²¡æœ‰æ‰€æœ‰ç»†èŠ‚â€”â€”ä»–ä»¬é€šè¿‡æ„å»ºç‰¹å®šé¢†åŸŸçš„è®¤çŸ¥æ¶æ„å‘Šè¯‰ç³»ç»Ÿå»åšã€‚

ğŸ’¡
Nearly all the advanced â€œagentsâ€ we see in production actually have a very domain specific and custom cognitive architecture.
æˆ‘ä»¬åœ¨ç”Ÿäº§ä¸­çœ‹åˆ°çš„å‡ ä¹æ‰€æœ‰é«˜çº§â€œä»£ç†â€å®é™…ä¸Šéƒ½æœ‰ä¸€ä¸ªéå¸¸ç‰¹å®šé¢†åŸŸå’Œå®šåˆ¶çš„è®¤çŸ¥æ¶æ„ã€‚

Weâ€™re making building these custom cognitive architectures easier with LangGraph. One of the big focus points of LangGraph is on controllability. Weâ€™ve designed LangGraph to very low level and highly controllable - this is because weâ€™ve seen that level of controllability is 100% needed to create a reliable custom cognitive architecture.
æˆ‘ä»¬æ­£åœ¨é€šè¿‡LangGraphä½¿æ„å»ºè¿™äº›å®šåˆ¶è®¤çŸ¥æ¶æ„å˜å¾—æ›´å®¹æ˜“ã€‚LangGraphçš„ä¸€ä¸ªé‡è¦å…³æ³¨ç‚¹æ˜¯å¯æ§æ€§ã€‚æˆ‘ä»¬è®¾è®¡LangGraphçš„ç›®çš„æ˜¯éå¸¸ä½çº§å’Œé«˜åº¦å¯æ§çš„â€”â€”è¿™æ˜¯å› ä¸ºæˆ‘ä»¬å·²ç»çœ‹åˆ°è¿™ç§å¯æ§æ€§æ°´å¹³æ˜¯åˆ›å»ºå¯é çš„å®šåˆ¶è®¤çŸ¥æ¶æ„æ‰€å¿…éœ€çš„ã€‚

### What does the future of planning and reasoning look like?
è§„åˆ’å’Œæ¨ç†çš„æœªæ¥æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ

The LLM space has been changing and evolving rapidly, and we should keep that in mind when building applications, and especially when building tools.
LLMé¢†åŸŸä¸€ç›´åœ¨å¿«é€Ÿå˜åŒ–å’Œå‘å±•ï¼Œæˆ‘ä»¬åœ¨æ„å»ºåº”ç”¨ç¨‹åºæ—¶ï¼Œå°¤å…¶æ˜¯åœ¨æ„å»ºå·¥å…·æ—¶ï¼Œåº”è¯¥ç‰¢è®°è¿™ä¸€ç‚¹ã€‚

My current take is that general purpose reasoning will get more and more absorbed into the model layer. The models will get more and more intelligent, whether through scale or research breakthroughs - it seems foolish to bet against that. Models will get faster and cheaper as well, so it will become more and more feasible to pass a large amount of context to them.
æˆ‘ç›®å‰çš„çœ‹æ³•æ˜¯ï¼Œé€šç”¨æ¨ç†å°†è¶Šæ¥è¶Šå¤šåœ°è¢«å¸æ”¶åˆ°æ¨¡å‹å±‚ä¸­ã€‚æ— è®ºæ˜¯é€šè¿‡è§„æ¨¡è¿˜æ˜¯ç ”ç©¶çªç ´ï¼Œæ¨¡å‹å°†å˜å¾—è¶Šæ¥è¶Šæ™ºèƒ½â€”â€”åå¯¹è¿™ä¸€ç‚¹ä¼¼ä¹æ˜¯æ„šè ¢çš„ã€‚æ¨¡å‹ä¹Ÿä¼šå˜å¾—æ›´å¿«å’Œæ›´ä¾¿å®œï¼Œå› æ­¤å°†å¤§é‡ä¸Šä¸‹æ–‡ä¼ é€’ç»™å®ƒä»¬å°†å˜å¾—è¶Šæ¥è¶Šå¯è¡Œã€‚

However, I believe that no matter how powerful the model becomes, you will always need to communicate to the agent, in some form, what it should do. As a result, I believe prompting and custom architectures are here to stay. For simple tasks, prompting may suffice. For more complex tasks, you may want to put the logic of how it should behave in code. Code-first approaches may be faster, more reliable, more debuggable, and oftentimes easier/more logical to express.
ç„¶è€Œï¼Œæˆ‘ç›¸ä¿¡æ— è®ºæ¨¡å‹å˜å¾—å¤šä¹ˆå¼ºå¤§ï¼Œæ‚¨æ€»æ˜¯éœ€è¦ä»¥æŸç§å½¢å¼å‘ä»£ç†ä¼ è¾¾å®ƒåº”è¯¥åšä»€ä¹ˆã€‚å› æ­¤ï¼Œæˆ‘ç›¸ä¿¡æç¤ºå’Œå®šåˆ¶æ¶æ„å°†ç»§ç»­å­˜åœ¨ã€‚å¯¹äºç®€å•çš„ä»»åŠ¡ï¼Œæç¤ºå¯èƒ½å°±è¶³å¤Ÿäº†ã€‚å¯¹äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼Œæ‚¨å¯èƒ½å¸Œæœ›å°†å…¶è¡Œä¸ºé€»è¾‘æ”¾åœ¨ä»£ç ä¸­ã€‚ä»£ç ä¼˜å…ˆçš„æ–¹æ³•å¯èƒ½æ›´å¿«ã€æ›´å¯é ã€æ›´æ˜“è°ƒè¯•ï¼Œå¹¶ä¸”é€šå¸¸æ›´å®¹æ˜“/æ›´åˆä¹é€»è¾‘åœ°è¡¨è¾¾ã€‚

I went on a podcast recently with Sonya and Pat from Sequoia and talked about this topic. They drew a fantastic diagram showing how the role / importance of prompting, cognitive architectures, and the model may evolve over time.
æœ€è¿‘ï¼Œæˆ‘ä¸çº¢æ‰çš„Sonyaå’ŒPatä¸€èµ·å‚åŠ äº†ä¸€æœŸæ’­å®¢ï¼Œè®¨è®ºäº†è¿™ä¸ªè¯é¢˜ã€‚ä»–ä»¬ç»˜åˆ¶äº†ä¸€å¼ ç²¾å½©çš„å›¾è¡¨ï¼Œå±•ç¤ºäº†æç¤ºã€è®¤çŸ¥æ¶æ„å’Œæ¨¡å‹çš„è§’è‰²/é‡è¦æ€§å¦‚ä½•éšæ—¶é—´æ¼”å˜ã€‚

![](/images/2024/LangChain-Blog-In-the-Loop/role-prompt.png)

So while the planning and reasoning of LLMs will certainly get better, we also strongly believe that if you are building a task-specific agent then you will need to build a custom cognitive architecture. Thatâ€™s why weâ€™re so bullish on the future of LangGraph.
å› æ­¤ï¼Œè™½ç„¶LLMçš„è§„åˆ’å’Œæ¨ç†è‚¯å®šä¼šå˜å¾—æ›´å¥½ï¼Œä½†æˆ‘ä»¬ä¹Ÿåšä¿¡ï¼Œå¦‚æœæ‚¨æ­£åœ¨æ„å»ºç‰¹å®šä»»åŠ¡çš„ä»£ç†ï¼Œé‚£ä¹ˆæ‚¨å°†éœ€è¦æ„å»ºä¸€ä¸ªå®šåˆ¶çš„è®¤çŸ¥æ¶æ„ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯¹LangGraphçš„æœªæ¥å¦‚æ­¤çœ‹å¥½ã€‚


## [UX for Agents, Part 1: Chat](https://blog.langchain.dev/ux-for-agents-part-1-chat-2/)
ä»£ç†çš„ç”¨æˆ·ä½“éªŒï¼Œç¬¬1éƒ¨åˆ†ï¼šèŠå¤©

At Sequoiaâ€™s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. Check out that talk [here](https://www.youtube.com/watch?v=pBBe1pk8hf4&ref=blog.langchain.dev). In this post I will dive deeper into UX for agents. Thanks to Nuno Campos, LangChain founding engineer for many of the original thoughts and analogies here.
åœ¨ä¸‰æœˆçš„çº¢æ‰AI Ascentä¼šè®®ä¸Šï¼Œæˆ‘è°ˆåˆ°äº†ä»£ç†çš„ä¸‰ä¸ªé™åˆ¶ï¼šè§„åˆ’ã€ç”¨æˆ·ä½“éªŒå’Œè®°å¿†ã€‚è¯·åœ¨è¿™é‡ŒæŸ¥çœ‹è¯¥æ¼”è®²ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†æ·±å…¥æ¢è®¨ä»£ç†çš„ç”¨æˆ·ä½“éªŒã€‚æ„Ÿè°¢LangChainåˆ›å§‹å·¥ç¨‹å¸ˆNuno Camposæä¾›çš„è®¸å¤šåŸå§‹æƒ³æ³•å’Œç±»æ¯”ã€‚

Because there are so many different aspects of UX for agents, this topic will be split into three separate blogs. This is first in the series.
ç”±äºä»£ç†çš„ç”¨æˆ·ä½“éªŒæœ‰å¾ˆå¤šä¸åŒçš„æ–¹é¢ï¼Œè¿™ä¸ªè¯é¢˜å°†åˆ†æˆä¸‰ç¯‡ç‹¬ç«‹çš„åšå®¢ã€‚è¿™æ˜¯ç³»åˆ—ä¸­çš„ç¬¬ä¸€ç¯‡ã€‚

Human-Computer Interaction has been a well-studied area for years. I believe that in the coming years, Human-Agent Interaction will also become a key area of research.
äººæœºäº¤äº’å¤šå¹´æ¥ä¸€ç›´æ˜¯ä¸€ä¸ªç ”ç©¶å¾—å¾ˆé€å½»çš„é¢†åŸŸã€‚æˆ‘ç›¸ä¿¡åœ¨æœªæ¥å‡ å¹´ï¼Œäººæœºä»£ç†äº¤äº’ä¹Ÿå°†æˆä¸ºä¸€ä¸ªå…³é”®çš„ç ”ç©¶é¢†åŸŸã€‚

Agentic systems differ from traditional computer systems of the past due to new challenges stemming from latency, unreliability, and natural language interfaces. As such, I strongly believe that new UI/UX paradigms for interacting with these agentic applications will emerge.
ä»£ç†ç³»ç»Ÿä¸è¿‡å»çš„ä¼ ç»Ÿè®¡ç®—æœºç³»ç»Ÿä¸åŒï¼Œå› ä¸ºå®ƒä»¬é¢ä¸´æ¥è‡ªå»¶è¿Ÿã€ä¸å¯é æ€§å’Œè‡ªç„¶è¯­è¨€æ¥å£çš„æ–°æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘åšä¿¡ä¸è¿™äº›ä»£ç†åº”ç”¨ç¨‹åºäº¤äº’çš„æ–°UI/UXèŒƒå¼å°†ä¼šå‡ºç°ã€‚

While itâ€™s still early days for agentic systems, I think there are multiple emerging UX paradigms. In this blog we will discuss perhaps the most dominant UX so far: chat.
è™½ç„¶ä»£ç†ç³»ç»Ÿè¿˜å¤„äºæ—©æœŸé˜¶æ®µï¼Œä½†æˆ‘è®¤ä¸ºæœ‰å¤šç§æ–°å…´çš„ç”¨æˆ·ä½“éªŒèŒƒå¼ã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºè¿„ä»Šä¸ºæ­¢å¯èƒ½æœ€ä¸»è¦çš„ç”¨æˆ·ä½“éªŒï¼šèŠå¤©ã€‚

### Streaming Chat
æµå¼èŠå¤©

The â€œstreaming chatâ€ UX is the most dominant UX so far. This quite simply is an agentic system that streams back its thoughts and actions in a chat format â€” ChatGPT is the most popular example. This interaction pattern seems basic, but is actually quite good for a few reasons.
â€œæµå¼èŠå¤©â€ç”¨æˆ·ä½“éªŒæ˜¯è¿„ä»Šä¸ºæ­¢æœ€ä¸»è¦çš„ç”¨æˆ·ä½“éªŒã€‚è¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªä»£ç†ç³»ç»Ÿï¼Œå®ƒä»¥èŠå¤©æ ¼å¼æµå›å…¶æƒ³æ³•å’Œè¡ŒåŠ¨â€”â€”ChatGPTæ˜¯æœ€æµè¡Œçš„ä¾‹å­ã€‚è¿™ç§äº¤äº’æ¨¡å¼çœ‹èµ·æ¥å¾ˆåŸºæœ¬ï¼Œä½†å®é™…ä¸Šæœ‰å‡ ä¸ªåŸå› ä½¿å…¶éå¸¸å¥½ã€‚

The main way to â€œprogramâ€ an LLM is with natural language. In chat, you interact directly with the LLM through natural language. This means there is little to no barrier between you and the LLM.
â€œç¼–ç¨‹â€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸»è¦æ–¹å¼æ˜¯ä½¿ç”¨è‡ªç„¶è¯­è¨€ã€‚åœ¨èŠå¤©ä¸­ï¼Œä½ é€šè¿‡è‡ªç„¶è¯­è¨€ç›´æ¥ä¸å¤§å‹è¯­è¨€æ¨¡å‹äº’åŠ¨ã€‚è¿™æ„å‘³ç€ä½ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´å‡ ä¹æ²¡æœ‰éšœç¢ã€‚

ğŸ’¡
In some ways, streaming chat is the â€œterminalâ€ of early computers.
åœ¨æŸäº›æ–¹é¢ï¼Œæµå¼èŠå¤©æ˜¯æ—©æœŸè®¡ç®—æœºçš„â€œç»ˆç«¯â€ã€‚

A terminal (especially in early computers) provides lower-level and more direct access to the underlying OS. But over time, computers have shifted to more UI-based interactions. Streaming chat may be similar - itâ€™s the first way we built to interact with LLMs, and it provides pretty direct access to the underlying LLM. Over time, other UXs may emerge (just as computers became more UI-based) â€“ but low-level access has significant benefits, especially at the start!
ç»ˆç«¯ï¼ˆå°¤å…¶æ˜¯åœ¨æ—©æœŸè®¡ç®—æœºä¸­ï¼‰æä¾›äº†å¯¹åº•å±‚æ“ä½œç³»ç»Ÿçš„æ›´ä½çº§å’Œæ›´ç›´æ¥çš„è®¿é—®ã€‚ä½†éšç€æ—¶é—´çš„æ¨ç§»ï¼Œè®¡ç®—æœºå·²ç»è½¬å‘æ›´å¤šåŸºäºUIçš„äº¤äº’ã€‚æµå¼èŠå¤©å¯èƒ½ç±»ä¼¼â€”â€”è¿™æ˜¯æˆ‘ä»¬ä¸å¤§å‹è¯­è¨€æ¨¡å‹äº’åŠ¨çš„ç¬¬ä¸€ç§æ–¹å¼ï¼Œå®ƒæä¾›äº†å¯¹åº•å±‚å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›¸å½“ç›´æ¥çš„è®¿é—®ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œå…¶ä»–ç”¨æˆ·ä½“éªŒå¯èƒ½ä¼šå‡ºç°ï¼ˆå°±åƒè®¡ç®—æœºå˜å¾—æ›´å¤šåŸºäºUIä¸€æ ·ï¼‰â€”â€”ä½†ä½çº§è®¿é—®åœ¨å¼€å§‹æ—¶å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼

One of the reasons that streaming chat is great is that LLMs can take a while to work. Streaming enables the user to understand exactly what is happening under the hood. You can stream back both intermediate actions the LLM takes (both what actions they take, and what the results are) as well as the tokens as the LLM â€œthinksâ€.
æµå¼èŠå¤©å¾ˆæ£’çš„åŸå› ä¹‹ä¸€æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´æ‰èƒ½å·¥ä½œã€‚æµå¼èŠå¤©ä½¿ç”¨æˆ·èƒ½å¤Ÿç¡®åˆ‡äº†è§£åº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆã€‚ä½ å¯ä»¥æµå›å¤§å‹è¯­è¨€æ¨¡å‹é‡‡å–çš„ä¸­é—´åŠ¨ä½œï¼ˆåŒ…æ‹¬å®ƒä»¬é‡‡å–çš„åŠ¨ä½œå’Œç»“æœï¼‰ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹â€œæ€è€ƒâ€æ—¶çš„ tokensã€‚

Another benefit of streaming chat is that LLMs can mess up often. Chat provides a great interface to naturally correct and guide it! Weâ€™re very used to having follow-up conversations and discussing things iteratively over chat already.
æµå¼èŠå¤©çš„å¦ä¸€ä¸ªå¥½å¤„æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ç»å¸¸ä¼šå‡ºé”™ã€‚èŠå¤©æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„ç•Œé¢æ¥è‡ªç„¶åœ°çº æ­£å’Œå¼•å¯¼å®ƒï¼æˆ‘ä»¬å·²ç»éå¸¸ä¹ æƒ¯äºåœ¨èŠå¤©ä¸­è¿›è¡Œåç»­å¯¹è¯å’Œè¿­ä»£è®¨è®ºã€‚

Still, streaming chat has its drawbacks. First - streaming chat is a relatively new UX, so our existing chat platforms (iMessage, Facebook messenger, Slack, etc) donâ€™t have this mode built in. Second, itâ€™s a bit awkward for longer-running tasks â€” am I just going to sit there and watch the agent work? Third, streaming chat generally needs to be triggered by a human, which means the human is still very much in the loop.
å°½ç®¡å¦‚æ­¤ï¼Œæµå¼èŠå¤©ä¹Ÿæœ‰å…¶ç¼ºç‚¹ã€‚é¦–å…ˆâ€”â€”æµå¼èŠå¤©æ˜¯ä¸€ç§ç›¸å¯¹è¾ƒæ–°çš„ç”¨æˆ·ä½“éªŒï¼Œå› æ­¤æˆ‘ä»¬ç°æœ‰çš„èŠå¤©å¹³å°ï¼ˆiMessageã€Facebook Messengerã€Slackç­‰ï¼‰æ²¡æœ‰å†…ç½®è¿™ç§æ¨¡å¼ã€‚å…¶æ¬¡ï¼Œå¯¹äºé•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡æ¥è¯´ï¼Œè¿™æœ‰ç‚¹å°´å°¬â€”â€”æˆ‘åªæ˜¯ååœ¨é‚£é‡Œçœ‹ç€ä»£ç†å·¥ä½œå—ï¼Ÿç¬¬ä¸‰ï¼Œæµå¼èŠå¤©é€šå¸¸éœ€è¦ç”±äººæ¥è§¦å‘ï¼Œè¿™æ„å‘³ç€äººä»ç„¶åœ¨å¾ªç¯ä¸­ã€‚

### Non-streaming Chat
éæµå¼èŠå¤©

It feels odd to call it â€œnon-streamingâ€ chat, since we would have just called this â€œchatâ€ up until two years ago â€” but here we are. Non-streaming chat has many of the same properties of streaming chat - it exposes the LLM pretty directly to the user, and it allows for very natural corrections.
ç§°å…¶ä¸ºâ€œéæµå¼â€èŠå¤©æ„Ÿè§‰å¾ˆå¥‡æ€ªï¼Œå› ä¸ºç›´åˆ°ä¸¤å¹´å‰æˆ‘ä»¬è¿˜åªæ˜¯ç§°å…¶ä¸ºâ€œèŠå¤©â€â€”â€”ä½†ç°åœ¨æˆ‘ä»¬åœ¨è¿™é‡Œã€‚éæµå¼èŠå¤©å…·æœ‰æµå¼èŠå¤©çš„è®¸å¤šç›¸åŒå±æ€§â€”â€”å®ƒéå¸¸ç›´æ¥åœ°å‘ç”¨æˆ·å±•ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶å…è®¸éå¸¸è‡ªç„¶çš„çº æ­£ã€‚

The big difference for non-streaming chat is that responses come back in completed batches, which has its pros and cons. The main con is that you canâ€™t see whatâ€™s going on under the hood, leaving you in the dark.
éæµå¼èŠå¤©çš„æœ€å¤§åŒºåˆ«åœ¨äºå“åº”ä»¥å®Œæˆçš„æ‰¹æ¬¡è¿”å›ï¼Œè¿™æœ‰å…¶ä¼˜ç¼ºç‚¹ã€‚ä¸»è¦çš„ç¼ºç‚¹æ˜¯ä½ çœ‹ä¸åˆ°åº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆï¼Œè®©ä½ ä¸€æ— æ‰€çŸ¥ã€‚

Butâ€¦ is that actually okay?
ä½†æ˜¯â€¦â€¦è¿™çœŸçš„å¯ä»¥å—ï¼Ÿ

Linus Lee had some great thoughts on â€œdelegationâ€ recently that I really liked. A snippet just to illustrate:
Linus Leeæœ€è¿‘å¯¹â€œå§”æ´¾â€æœ‰ä¸€äº›å¾ˆå¥½çš„æƒ³æ³•ï¼Œæˆ‘éå¸¸å–œæ¬¢ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç‰‡æ®µæ¥è¯´æ˜ï¼š

> I intentionally built the interface to be as opaque as possible.
æˆ‘æ•…æ„å°†ç•Œé¢è®¾è®¡å¾—å°½å¯èƒ½ä¸é€æ˜ã€‚

He argues that an opaque interface requires a certain amount of trust, but once established, allows you to just delegate tasks to the agent without micro-managing. This async nature also lends itself to longer-running tasks - which means agents doing more work for you.
ä»–è®¤ä¸ºï¼Œä¸é€æ˜çš„ç•Œé¢éœ€è¦ä¸€å®šç¨‹åº¦çš„ä¿¡ä»»ï¼Œä½†ä¸€æ—¦å»ºç«‹èµ·æ¥ï¼Œå°±å…è®¸ä½ å°†ä»»åŠ¡å§”æ´¾ç»™ä»£ç†è€Œæ— éœ€å¾®è§‚ç®¡ç†ã€‚è¿™ç§å¼‚æ­¥æ€§è´¨ä¹Ÿé€‚ç”¨äºé•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡â€”â€”è¿™æ„å‘³ç€ä»£ç†ä¸ºä½ åšæ›´å¤šçš„å·¥ä½œã€‚

Assuming trust is established, this seems good. But it also opens up other issues. For example, how do you handle â€œdouble-textingâ€ â€” where the user messages once, the agent starts doing something, and then the user messages again with a different (and sometimes unrelated) thought before the agent finishes its task. With streaming chat, you generally donâ€™t have this issue because the streaming of the agent blocks the user from typing new input.
å‡è®¾ä¿¡ä»»å·²ç»å»ºç«‹ï¼Œè¿™çœ‹èµ·æ¥ä¸é”™ã€‚ä½†å®ƒä¹Ÿå¸¦æ¥äº†å…¶ä»–é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œä½ å¦‚ä½•å¤„ç†â€œåŒé‡æ–‡æœ¬â€â€”â€”ç”¨æˆ·å‘é€ä¸€æ¬¡æ¶ˆæ¯ï¼Œä»£ç†å¼€å§‹åšæŸäº‹ï¼Œç„¶ååœ¨ä»£ç†å®Œæˆä»»åŠ¡ä¹‹å‰ï¼Œç”¨æˆ·å†æ¬¡å‘é€ä¸åŒï¼ˆæœ‰æ—¶ä¸ç›¸å…³ï¼‰çš„æƒ³æ³•ã€‚åœ¨æµå¼èŠå¤©ä¸­ï¼Œä½ é€šå¸¸ä¸ä¼šæœ‰è¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºä»£ç†çš„æµå¼ä¼ è¾“ä¼šé˜»æ­¢ç”¨æˆ·è¾“å…¥æ–°å†…å®¹ã€‚

One of the benefits of the non-streaming chat UX is also much more native to us, which means that it may be easier to integrate into existing workflows. People are used to texting humans - why shouldnâ€™t they easily adapt to texting with an AI?
éæµå¼èŠå¤©ç”¨æˆ·ä½“éªŒçš„ä¸€ä¸ªå¥½å¤„æ˜¯å®ƒå¯¹æˆ‘ä»¬æ¥è¯´æ›´åŠ æœ¬åœŸåŒ–ï¼Œè¿™æ„å‘³ç€å®ƒå¯èƒ½æ›´å®¹æ˜“é›†æˆåˆ°ç°æœ‰çš„å·¥ä½œæµç¨‹ä¸­ã€‚äººä»¬ä¹ æƒ¯äºä¸äººç±»å‘çŸ­ä¿¡â€”â€”ä¸ºä»€ä¹ˆä»–ä»¬ä¸èƒ½è½»æ¾é€‚åº”ä¸AIå‘çŸ­ä¿¡å‘¢ï¼Ÿ

ğŸ’¡
Another large benefit of non-streaming chat is that itâ€™s often acceptable for the AI to take longer to respond.
éæµå¼èŠå¤©çš„å¦ä¸€ä¸ªå¤§å¥½å¤„æ˜¯AIé€šå¸¸å¯ä»¥æ¥å—æ›´é•¿æ—¶é—´çš„å“åº”ã€‚

This is often due to non-streaming chat being integrated more natively into our existing workflows. We donâ€™t expect our friends to text us back instantaneously - why should we expect an AI to? This makes it easier to interact with more complex agentic systems - these systems often take a while, and if there is the expectation of an instantaneous response that could be frustrating. Non-streaming chat often removes that expectation, making it easier to do more complex things.
è¿™é€šå¸¸æ˜¯å› ä¸ºéæµå¼èŠå¤©æ›´æœ¬åœ°åŒ–åœ°é›†æˆåˆ°æˆ‘ä»¬ç°æœ‰çš„å·¥ä½œæµç¨‹ä¸­ã€‚æˆ‘ä»¬ä¸æœŸæœ›æœ‹å‹ç«‹å³å›å¤æˆ‘ä»¬çš„çŸ­ä¿¡â€”â€”ä¸ºä»€ä¹ˆæˆ‘ä»¬æœŸæœ›AIè¿™æ ·åšå‘¢ï¼Ÿè¿™ä½¿å¾—ä¸æ›´å¤æ‚çš„ä»£ç†ç³»ç»Ÿäº¤äº’å˜å¾—æ›´å®¹æ˜“â€”â€”è¿™äº›ç³»ç»Ÿé€šå¸¸éœ€è¦ä¸€æ®µæ—¶é—´ï¼Œå¦‚æœæœŸæœ›å³æ—¶å“åº”å¯èƒ½ä¼šä»¤äººæ²®ä¸§ã€‚éæµå¼èŠå¤©é€šå¸¸æ¶ˆé™¤äº†è¿™ç§æœŸæœ›ï¼Œä½¿å¾—åšæ›´å¤æ‚çš„äº‹æƒ…å˜å¾—æ›´å®¹æ˜“ã€‚

It may initially seem that streaming is newer, flashier, and more futuristic than standard chatâ€¦ but as we trust our agentic systems more, will this reverse?
æœ€åˆå¯èƒ½çœ‹èµ·æ¥æµå¼èŠå¤©æ¯”æ ‡å‡†èŠå¤©æ›´æ–°ã€æ›´ç‚«ã€æ›´å…·æœªæ¥æ„Ÿâ€¦â€¦ä½†éšç€æˆ‘ä»¬å¯¹ä»£ç†ç³»ç»Ÿçš„ä¿¡ä»»å¢åŠ ï¼Œè¿™ä¼šé€†è½¬å—ï¼Ÿ

### Is there more than just chat?
é™¤äº†èŠå¤©è¿˜æœ‰æ›´å¤šå—ï¼Ÿ

As this is just part one of a three-part series, we believe there are more UXs to consider beyond chat. Still - it is worth reminding that chat is a very good UX, and that hereâ€™s a reason itâ€™s so widely used.
ç”±äºè¿™åªæ˜¯ä¸‰éƒ¨åˆ†ç³»åˆ—ä¸­çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ç›¸ä¿¡é™¤äº†èŠå¤©è¿˜æœ‰æ›´å¤šçš„ç”¨æˆ·ä½“éªŒéœ€è¦è€ƒè™‘ã€‚å°½ç®¡å¦‚æ­¤â€”â€”å€¼å¾—æé†’çš„æ˜¯ï¼ŒèŠå¤©æ˜¯ä¸€ç§éå¸¸å¥½çš„ç”¨æˆ·ä½“éªŒï¼Œè¿™å°±æ˜¯å®ƒè¢«å¹¿æ³›ä½¿ç”¨çš„åŸå› ã€‚

Benefits of chat:
èŠå¤©çš„å¥½å¤„ï¼š
- Allows user to interact directly with the model
å…è®¸ç”¨æˆ·ç›´æ¥ä¸æ¨¡å‹äº’åŠ¨
- Allows for easy follow up questions and/or corrections
å…è®¸è½»æ¾è¿›è¡Œåç»­é—®é¢˜å’Œ/æˆ–çº æ­£

Pros/Cons of streaming vs. non-streaming chat
æµå¼èŠå¤©ä¸éæµå¼èŠå¤©çš„ä¼˜ç¼ºç‚¹

|     | Pros (ä¼˜ç‚¹) | Cons (ç¼ºç‚¹) |
| --- | --- | --- |
| Streaming Chat | - Shows the user that some work is being done<br>- å‘ç”¨æˆ·å±•ç¤ºæ­£åœ¨è¿›è¡Œä¸€äº›å·¥ä½œ<br><br>- If you stream intermediate steps, allows the user to build trust by seeing what is going on under the hood<br>- å¦‚æœæ‚¨æµå¼ä¼ è¾“ä¸­é—´æ­¥éª¤ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡çœ‹åˆ°åº•å±‚å‘ç”Ÿçš„äº‹æƒ…æ¥å»ºç«‹ä¿¡ä»» | - Most apps today aren't designed for streaming<br>- å¤§å¤šæ•°åº”ç”¨ç¨‹åºä»Šå¤©å¹¶ä¸æ˜¯ä¸ºæµå¼è®¾è®¡çš„ |
| <nobr>Non-Streaming Chat</nobr> | - Usually has less of an expectation of an instantaneous response, allowing the agent to take on more work<br>- é€šå¸¸ä¸ä¼šæœŸæœ›å³æ—¶å“åº”ï¼Œä»è€Œä½¿ä»£ç†å¯ä»¥æ‰¿æ‹…æ›´å¤šå·¥ä½œ<br><br>- More apps natively support non-streaming already<br>- æ›´å¤šåº”ç”¨ç¨‹åºå·²ç»æœ¬åœ°æ”¯æŒéæµå¼ | - Hard to know what's happening under the hood<br>- å¾ˆéš¾çŸ¥é“åº•å±‚å‘ç”Ÿäº†ä»€ä¹ˆ<br><br>- UX needs to handle double-texting, where the user interacts again with the system before the first response is completed<br>- ç”¨æˆ·åœ¨ç¬¬ä¸€ä¸ªå“åº”å®Œæˆä¹‹å‰å†æ¬¡ä¸ç³»ç»Ÿäº’åŠ¨ï¼ŒUXéœ€è¦å¤„ç†åŒé‡æ–‡æœ¬ |


## [UX for Agents, Part 2: Ambient](https://blog.langchain.dev/ux-for-agents-part-2-ambient/)
ä»£ç†çš„ç”¨æˆ·ä½“éªŒï¼Œç¬¬2éƒ¨åˆ†ï¼šç¯å¢ƒ

In our previous blog post on chat-based UX for agents, we discussed how chat requires users to proactively think about messaging an AI. But what if the AI could just work in the background for you?
åœ¨æˆ‘ä»¬ä¹‹å‰å…³äºåŸºäºèŠå¤©çš„ä»£ç†ç”¨æˆ·ä½“éªŒçš„åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†èŠå¤©å¦‚ä½•è¦æ±‚ç”¨æˆ·ä¸»åŠ¨è€ƒè™‘å‘AIå‘é€æ¶ˆæ¯ã€‚ä½†å¦‚æœAIå¯ä»¥åœ¨åå°ä¸ºä½ å·¥ä½œå‘¢ï¼Ÿ

Iâ€™d argue that in order for agentic systems to really reach their potential, this shift towards allowing AI to work in the background needs to happen.
æˆ‘è®¤ä¸ºï¼Œä¸ºäº†ä½¿ä»£ç†ç³»ç»ŸçœŸæ­£å‘æŒ¥å…¶æ½œåŠ›ï¼Œè¿™ç§å…è®¸AIåœ¨åå°å·¥ä½œçš„è½¬å˜æ˜¯å¿…è¦çš„ã€‚

When tasks are handled in the background, users are generally more tolerant of longer completion times (as they relax expectations for low latency).
å½“ä»»åŠ¡åœ¨åå°å¤„ç†æ—¶ï¼Œç”¨æˆ·é€šå¸¸å¯¹è¾ƒé•¿çš„å®Œæˆæ—¶é—´æ›´ä¸ºå®½å®¹ï¼ˆå› ä¸ºä»–ä»¬æ”¾æ¾äº†å¯¹ä½å»¶è¿Ÿçš„æœŸæœ›ï¼‰ã€‚

This frees up the agent to do more work, often more carefully / diligently than in a chat UX.
è¿™ä½¿å¾—ä»£ç†å¯ä»¥åšæ›´å¤šçš„å·¥ä½œï¼Œé€šå¸¸æ¯”åœ¨èŠå¤©ç”¨æˆ·ä½“éªŒä¸­æ›´ä»”ç»†/å‹¤å¥‹ã€‚

Additionally, running agents in the background enables us humans to scale our capabilities more effectively.
æ­¤å¤–ï¼Œåœ¨åå°è¿è¡Œä»£ç†ä½¿æˆ‘ä»¬äººç±»èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ‰©å±•æˆ‘ä»¬çš„èƒ½åŠ›ã€‚

Chat interfaces typically limit us to one task at a time. But if agents are running ambiently in the background, there can be many agents handling multiple tasks simultaneously.
èŠå¤©ç•Œé¢é€šå¸¸é™åˆ¶æˆ‘ä»¬ä¸€æ¬¡åªèƒ½å¤„ç†ä¸€ä¸ªä»»åŠ¡ã€‚ä½†å¦‚æœä»£ç†åœ¨åå°ç¯å¢ƒä¸­è¿è¡Œï¼Œå¯ä»¥æœ‰è®¸å¤šä»£ç†åŒæ—¶å¤„ç†å¤šä¸ªä»»åŠ¡ã€‚

So, what would this background agent UX look like?
é‚£ä¹ˆï¼Œè¿™ç§åå°ä»£ç†ç”¨æˆ·ä½“éªŒä¼šæ˜¯ä»€ä¹ˆæ ·å­å‘¢ï¼Ÿ

### Building trust with background agents: Moving from â€œHuman-in-the-loopâ€ to â€œHuman-on-the-loopâ€
ä¸åå°ä»£ç†å»ºç«‹ä¿¡ä»»ï¼šä»â€œäººç±»åœ¨å¾ªç¯å†…â€åˆ°â€œäººç±»åœ¨å¾ªç¯ä¸Šâ€

It requires a certain level of trust to let an agent run in the background. How do you build this trust?
è®©ä»£ç†åœ¨åå°è¿è¡Œéœ€è¦ä¸€å®šç¨‹åº¦çš„ä¿¡ä»»ã€‚ä½ å¦‚ä½•å»ºç«‹è¿™ç§ä¿¡ä»»ï¼Ÿ

One straightforward idea is to just show users exactly what the agent is doing.
ä¸€ä¸ªç®€å•çš„æƒ³æ³•æ˜¯å‘ç”¨æˆ·å±•ç¤ºä»£ç†æ­£åœ¨åšä»€ä¹ˆã€‚

Display all the steps it is taking, and let users observe what is happening.
æ˜¾ç¤ºå®ƒæ­£åœ¨é‡‡å–çš„æ‰€æœ‰æ­¥éª¤ï¼Œå¹¶è®©ç”¨æˆ·è§‚å¯Ÿæ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚

While this information may not be immediately visible (as it would be when streaming a response back), it should be available for users to click into and observe.
è™½ç„¶è¿™äº›ä¿¡æ¯å¯èƒ½ä¸ä¼šç«‹å³å¯è§ï¼ˆå°±åƒæµå¼ä¼ è¾“å“åº”æ—¶é‚£æ ·ï¼‰ï¼Œä½†ç”¨æˆ·åº”è¯¥å¯ä»¥ç‚¹å‡»å¹¶è§‚å¯Ÿã€‚

The next step is to not only let users see what is happening, but also let them correct the agent.
ä¸‹ä¸€æ­¥ä¸ä»…æ˜¯è®©ç”¨æˆ·çœ‹åˆ°æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œè¿˜è¦è®©ä»–ä»¬çº æ­£ä»£ç†ã€‚

If they see that the agent made an incorrect choice on step 4 of 10, they should be able to go back to step 4 and correct the agent in some way.
å¦‚æœä»–ä»¬çœ‹åˆ°ä»£ç†åœ¨ç¬¬10æ­¥ä¸­çš„ç¬¬4æ­¥åšå‡ºäº†é”™è¯¯çš„é€‰æ‹©ï¼Œä»–ä»¬åº”è¯¥èƒ½å¤Ÿå›åˆ°ç¬¬4æ­¥å¹¶ä»¥æŸç§æ–¹å¼çº æ­£ä»£ç†ã€‚

What does this correction look like? There are a few forms this can take.
è¿™ç§çº æ­£æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼Ÿè¿™å¯ä»¥é‡‡å–å‡ ç§å½¢å¼ã€‚

Letâ€™s take a concrete example of correcting an agent that called a tool incorrectly:
è®©æˆ‘ä»¬ä¸¾ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ¥çº æ­£ä¸€ä¸ªé”™è¯¯è°ƒç”¨å·¥å…·çš„ä»£ç†ï¼š

1. You could manually type out the correct tool invocation and make it as if the agent had outputted that, then resume from there.
ä½ å¯ä»¥æ‰‹åŠ¨è¾“å…¥æ­£ç¡®çš„å·¥å…·è°ƒç”¨ï¼Œå¹¶ä½¿å…¶çœ‹èµ·æ¥åƒæ˜¯ä»£ç†è¾“å‡ºçš„é‚£æ ·ï¼Œç„¶åä»é‚£é‡Œç»§ç»­ã€‚

2. You give the agent explicit instructions on how to call the tool better - e.g., â€œcall it with argument X, not argument Yâ€ - and ask the agent to update its prediction.
ä½ ç»™ä»£ç†æ˜ç¡®çš„æŒ‡ç¤ºï¼Œå‘Šè¯‰å®ƒå¦‚ä½•æ›´å¥½åœ°è°ƒç”¨å·¥å…·â€”â€”ä¾‹å¦‚ï¼Œâ€œç”¨å‚æ•°Xè°ƒç”¨å®ƒï¼Œè€Œä¸æ˜¯å‚æ•°Yâ€â€”â€”å¹¶è¦æ±‚ä»£ç†æ›´æ–°å…¶é¢„æµ‹ã€‚

3. You could update the instructions or state of the agent at the point in time, and then rerun from that step onwards.
ä½ å¯ä»¥åœ¨é‚£ä¸ªæ—¶é—´ç‚¹æ›´æ–°ä»£ç†çš„æŒ‡ä»¤æˆ–çŠ¶æ€ï¼Œç„¶åä»é‚£ä¸€æ­¥é‡æ–°è¿è¡Œã€‚

The difference between options 2 and 3 lies in whether the agent is aware of its previous mistakes.
é€‰é¡¹2å’Œ3ä¹‹é—´çš„åŒºåˆ«åœ¨äºä»£ç†æ˜¯å¦æ„è¯†åˆ°å…¶å…ˆå‰çš„é”™è¯¯ã€‚

In option 2, the agent is presented with its previous poor generation and asked to correct it, while in option 3, it does not know of its bad prediction (and simply follows updated instructions).
åœ¨é€‰é¡¹2ä¸­ï¼Œä»£ç†è¢«å‘ŠçŸ¥å…¶å…ˆå‰çš„é”™è¯¯ç”Ÿæˆå¹¶è¢«è¦æ±‚çº æ­£ï¼Œè€Œåœ¨é€‰é¡¹3ä¸­ï¼Œå®ƒä¸çŸ¥é“å…¶é”™è¯¯é¢„æµ‹ï¼ˆå¹¶ä¸”åªæ˜¯éµå¾ªæ›´æ–°çš„æŒ‡ä»¤ï¼‰ã€‚

This approach moves the human from being â€œin-the-loopâ€ to â€œon-the-loopâ€.
è¿™ç§æ–¹æ³•å°†äººç±»ä»â€œåœ¨å¾ªç¯å†…â€è½¬ç§»åˆ°â€œåœ¨å¾ªç¯ä¸Šâ€ã€‚

â€œOn-the-loopâ€ requires the ability to show the user all intermediate steps the agent took, allowing the user to pause a workflow halfway through, provide feedback, and then let the agent continue.
â€œåœ¨å¾ªç¯ä¸Šâ€éœ€è¦èƒ½å¤Ÿå‘ç”¨æˆ·å±•ç¤ºä»£ç†é‡‡å–çš„æ‰€æœ‰ä¸­é—´æ­¥éª¤ï¼Œå…è®¸ç”¨æˆ·åœ¨ä¸­é€”æš‚åœå·¥ä½œæµç¨‹ï¼Œæä¾›åé¦ˆï¼Œç„¶åè®©ä»£ç†ç»§ç»­ã€‚

One application that has implemented a UX similar to this is Devin, the AI software engineer.
ä¸€ä¸ªå®ç°äº†ç±»ä¼¼ç”¨æˆ·ä½“éªŒçš„åº”ç”¨ç¨‹åºæ˜¯Devinï¼ŒAIè½¯ä»¶å·¥ç¨‹å¸ˆã€‚

Devin runs for an extended period of time, but you can see all the steps taken, rewind to the state of development at a specific point in time, and issue corrections from there.
Devinè¿è¡Œäº†å¾ˆé•¿æ—¶é—´ï¼Œä½†ä½ å¯ä»¥çœ‹åˆ°æ‰€æœ‰é‡‡å–çš„æ­¥éª¤ï¼Œå€’å›åˆ°ç‰¹å®šæ—¶é—´ç‚¹çš„å¼€å‘çŠ¶æ€ï¼Œå¹¶ä»é‚£é‡Œå‘å‡ºçº æ­£ã€‚

### Integrating human input: How agents can ask for help when needed
æ•´åˆäººç±»è¾“å…¥ï¼šä»£ç†å¦‚ä½•åœ¨éœ€è¦æ—¶å¯»æ±‚å¸®åŠ©

Although the agent may be running in the background, that does not mean that it needs to perform a task completely autonomously.
è™½ç„¶ä»£ç†å¯èƒ½åœ¨åå°è¿è¡Œï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€å®ƒéœ€è¦å®Œå…¨è‡ªä¸»åœ°æ‰§è¡Œä»»åŠ¡ã€‚

There will be moments when the agent does not know what to do or how to answer.
ä¼šæœ‰ä¸€äº›æ—¶å€™ä»£ç†ä¸çŸ¥é“è¯¥åšä»€ä¹ˆæˆ–å¦‚ä½•å›ç­”ã€‚

At this point, it needs to get the attention of a human and ask for help.
åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œå®ƒéœ€è¦å¼•èµ·äººç±»çš„æ³¨æ„å¹¶å¯»æ±‚å¸®åŠ©ã€‚

A concrete example of this is with an email assistant agent I am building.
ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ˜¯æˆ‘æ­£åœ¨æ„å»ºçš„ç”µå­é‚®ä»¶åŠ©æ‰‹ä»£ç†ã€‚

Although the email assistant can answer basic emails, it often needs my input on certain tasks I do not want to automate away.
è™½ç„¶ç”µå­é‚®ä»¶åŠ©æ‰‹å¯ä»¥å›ç­”åŸºæœ¬çš„ç”µå­é‚®ä»¶ï¼Œä½†å®ƒç»å¸¸éœ€è¦æˆ‘å¯¹æŸäº›æˆ‘ä¸æƒ³è‡ªåŠ¨åŒ–çš„ä»»åŠ¡è¿›è¡Œè¾“å…¥ã€‚

These tasks include reviewing complicated LangChain bug reports, decisions on whether I want to go to conferences, etc.
è¿™äº›ä»»åŠ¡åŒ…æ‹¬å®¡æŸ¥å¤æ‚çš„LangChainé”™è¯¯æŠ¥å‘Šï¼Œå†³å®šæˆ‘æ˜¯å¦æƒ³å‚åŠ ä¼šè®®ç­‰ã€‚

In this case, the email assistant needs a way of communicating to me that it needs information to respond.
åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”µå­é‚®ä»¶åŠ©æ‰‹éœ€è¦ä¸€ç§ä¸æˆ‘æ²Ÿé€šçš„æ–¹å¼ï¼Œå‘Šè¯‰æˆ‘å®ƒéœ€è¦ä¿¡æ¯æ¥å›åº”ã€‚

Note that itâ€™s not asking me to respond directly; instead, itâ€™s seeks my opinion on certain tasks, which it can then use to craft and send a nice email or schedule a calendar invite.
è¯·æ³¨æ„ï¼Œå®ƒä¸æ˜¯è¦æ±‚æˆ‘ç›´æ¥å›åº”ï¼›ç›¸åï¼Œå®ƒæ˜¯å¯»æ±‚æˆ‘å¯¹æŸäº›ä»»åŠ¡çš„æ„è§ï¼Œç„¶åå®ƒå¯ä»¥ç”¨è¿™äº›æ„è§æ¥åˆ¶ä½œå¹¶å‘é€ä¸€å°æ¼‚äº®çš„ç”µå­é‚®ä»¶æˆ–å®‰æ’ä¸€ä¸ªæ—¥å†é‚€è¯·ã€‚

Currently, I have this assistant set up in Slack.
ç›®å‰ï¼Œæˆ‘åœ¨Slackä¸­è®¾ç½®äº†è¿™ä¸ªåŠ©æ‰‹ã€‚

It pings me a question and I respond to it in a thread, natively integrating with my workflow.
å®ƒå‘æˆ‘å‘é€ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘åœ¨ä¸€ä¸ªçº¿ç¨‹ä¸­å›åº”å®ƒï¼Œä¸æˆ‘çš„å·¥ä½œæµç¨‹æœ¬åœ°é›†æˆã€‚

If I were to think about this type of UX at a larger scale than just an email assistant for myself, I would envision a UX similar to a customer support dashboard.
å¦‚æœæˆ‘è€ƒè™‘è¿™ç§ç±»å‹çš„ç”¨æˆ·ä½“éªŒåœ¨æ¯”ä»…ä»…æ˜¯ä¸ºæˆ‘è‡ªå·±æä¾›çš„ç”µå­é‚®ä»¶åŠ©æ‰‹æ›´å¤§çš„è§„æ¨¡ä¸Šï¼Œæˆ‘ä¼šè®¾æƒ³ä¸€ä¸ªç±»ä¼¼äºå®¢æˆ·æ”¯æŒä»ªè¡¨æ¿çš„ç”¨æˆ·ä½“éªŒã€‚

This interface would show all the areas where the assistant needs human help, the priority of requests, and any additional metadata.
è¿™ä¸ªç•Œé¢å°†æ˜¾ç¤ºåŠ©æ‰‹éœ€è¦äººç±»å¸®åŠ©çš„æ‰€æœ‰åŒºåŸŸï¼Œè¯·æ±‚çš„ä¼˜å…ˆçº§å’Œä»»ä½•é™„åŠ çš„å…ƒæ•°æ®ã€‚

![](/images/2024/LangChain-Blog-In-the-Loop/team-inbox.jpg)

I initially used the term â€œAgent Inboxâ€ when describing this email assistant - but more accurately, itâ€™s an inbox for humans to assist agents on certain tasksâ€¦ a bit of a chilling thought.
æˆ‘æœ€åˆåœ¨æè¿°è¿™ä¸ªç”µå­é‚®ä»¶åŠ©æ‰‹æ—¶ä½¿ç”¨äº†â€œä»£ç†æ”¶ä»¶ç®±â€è¿™ä¸ªæœ¯è¯­â€”â€”ä½†æ›´å‡†ç¡®åœ°è¯´ï¼Œå®ƒæ˜¯ä¸€ä¸ªäººç±»å¸®åŠ©ä»£ç†å¤„ç†æŸäº›ä»»åŠ¡çš„æ”¶ä»¶ç®±â€¦â€¦æœ‰ç‚¹ä»¤äººæ¯›éª¨æ‚šç„¶çš„æƒ³æ³•ã€‚

Conclusion
ç»“è®º

I am incredibly bullish on ambient agents, as I think they are key to allowing us to scale our own capabilities as humans.
æˆ‘å¯¹ç¯å¢ƒä»£ç†éå¸¸çœ‹å¥½ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºå®ƒä»¬æ˜¯å…è®¸æˆ‘ä»¬æ‰©å±•è‡ªå·±ä½œä¸ºäººç±»çš„èƒ½åŠ›çš„å…³é”®ã€‚

As our team continues building LangGraph, we are building with these types of UXs in mind.
éšç€æˆ‘ä»¬çš„å›¢é˜Ÿç»§ç»­æ„å»ºLangGraphï¼Œæˆ‘ä»¬æ­£åœ¨è€ƒè™‘è¿™äº›ç±»å‹çš„ç”¨æˆ·ä½“éªŒã€‚

We checkpoint all states, easily allowing for human-on-the-loop observability, rewinding, and editing.
æˆ‘ä»¬æ£€æŸ¥æ‰€æœ‰çŠ¶æ€ï¼Œè½»æ¾å®ç°äººç±»åœ¨ç¯ä¸Šçš„å¯è§‚å¯Ÿæ€§ã€å€’å›å’Œç¼–è¾‘ã€‚

This also enables agents to reach out to a human and wait for their response before continuing.
è¿™ä¹Ÿä½¿å¾—ä»£ç†èƒ½å¤Ÿè”ç³»åˆ°äººç±»å¹¶ç­‰å¾…ä»–ä»¬çš„å›åº”ï¼Œç„¶åå†ç»§ç»­ã€‚

If youâ€™re building an application with ambient agents, please reach out. Weâ€™d love to hear about your experience!
å¦‚æœä½ æ­£åœ¨æ„å»ºä¸€ä¸ªå¸¦æœ‰ç¯å¢ƒä»£ç†çš„åº”ç”¨ç¨‹åºï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚æˆ‘ä»¬å¾ˆæƒ³å¬å¬ä½ çš„ç»éªŒï¼


## [UX for Agents, Part 3: Spreadsheet, Generative, and Collaborative UI/UX](https://blog.langchain.dev/ux-for-agents-part-3/)
ä»£ç†çš„ç”¨æˆ·ä½“éªŒï¼Œç¬¬3éƒ¨åˆ†ï¼šç”µå­è¡¨æ ¼ã€ç”Ÿæˆå’Œåä½œUI/UX

The UI/UX space for agents is one of the spaces I am most excited about and will be watching closely for innovation in the coming months.
ä»£ç†çš„UI/UXé¢†åŸŸæ˜¯æˆ‘æœ€æ„Ÿå…´è¶£çš„é¢†åŸŸä¹‹ä¸€ï¼Œæˆ‘å°†åœ¨æœªæ¥å‡ ä¸ªæœˆå¯†åˆ‡å…³æ³¨è¿™ä¸€é¢†åŸŸçš„åˆ›æ–°ã€‚

In an attempt to wrap up the discussion on agent UI/UX, Iâ€™ll highlight three lesser-known UXs that have recently become more popular.
ä¸ºäº†æ€»ç»“ä»£ç†UI/UXçš„è®¨è®ºï¼Œæˆ‘å°†é‡ç‚¹ä»‹ç»æœ€è¿‘å˜å¾—æ›´å—æ¬¢è¿çš„ä¸‰ç§ä¸å¤ªä¸ºäººæ‰€çŸ¥çš„ç”¨æˆ·ä½“éªŒã€‚

Each of these could easily deserve its own blog post (which may happen down the line!).
æ¯ä¸€ç§éƒ½å¯ä»¥è½»æ¾åœ°æˆä¸ºä¸€ç¯‡ç‹¬ç«‹çš„åšå®¢æ–‡ç« ï¼ˆè¿™å¯èƒ½ä¼šåœ¨ä»¥åå‘ç”Ÿï¼ï¼‰ã€‚

### Spreadsheet UX
ç”µå­è¡¨æ ¼ç”¨æˆ·ä½“éªŒ

One UX paradigm Iâ€™ve seen a lot in the past ~2 months is a spreadsheet UX. I first saw this when Matrices, an AI-native spreadsheet, was launched earlier this year.
åœ¨è¿‡å»çš„ä¸¤ä¸ªæœˆé‡Œï¼Œæˆ‘çœ‹åˆ°çš„ä¸€ä¸ªç”¨æˆ·ä½“éªŒèŒƒå¼æ˜¯ç”µå­è¡¨æ ¼ç”¨æˆ·ä½“éªŒã€‚æˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°è¿™ä¸ªæ˜¯ä»Šå¹´æ—©äº›æ—¶å€™æ¨å‡ºçš„[AIåŸç”Ÿç”µå­è¡¨æ ¼Matrices](https://x.com/dina_yrl/status/1753206294784418024)ã€‚

I loved seeing this. First and foremost, the spreadsheet UX a super intuitive and user friendly way to support batch workloads.
æˆ‘å¾ˆå–œæ¬¢çœ‹åˆ°è¿™ä¸ªã€‚é¦–å…ˆï¼Œç”µå­è¡¨æ ¼ç”¨æˆ·ä½“éªŒæ˜¯ä¸€ç§è¶…çº§ç›´è§‚å’Œç”¨æˆ·å‹å¥½çš„æ–¹å¼æ¥æ”¯æŒæ‰¹é‡å·¥ä½œè´Ÿè½½ã€‚

Each cell becomes it own agent, going to off to research a particular thing. This batching allows users to scale up and interact with multiple agents simultaneously.
æ¯ä¸ªå•å…ƒæ ¼éƒ½æˆä¸ºè‡ªå·±çš„ä»£ç†ï¼Œå»ç ”ç©¶ç‰¹å®šçš„äº‹æƒ…ã€‚è¿™ç§æ‰¹å¤„ç†å…è®¸ç”¨æˆ·æ‰©å±•å¹¶åŒæ—¶ä¸å¤šä¸ªä»£ç†äº’åŠ¨ã€‚

There are other benefits of this UX as well. The spreadsheet format is a very common UX familiar to most users, so it fits in well with existing workflows.
è¿™ç§ç”¨æˆ·ä½“éªŒè¿˜æœ‰å…¶ä»–å¥½å¤„ã€‚ç”µå­è¡¨æ ¼æ ¼å¼æ˜¯ä¸€ç§éå¸¸å¸¸è§çš„ç”¨æˆ·ä½“éªŒï¼Œå¤§å¤šæ•°ç”¨æˆ·éƒ½å¾ˆç†Ÿæ‚‰ï¼Œå› æ­¤å®ƒå¾ˆå¥½åœ°é€‚åº”äº†ç°æœ‰çš„å·¥ä½œæµç¨‹ã€‚

This type of UX is perfect for data enrichment, a common LLM use case where each column can represent a different attribute to enrich.
è¿™ç§ç±»å‹çš„ç”¨æˆ·ä½“éªŒéå¸¸é€‚åˆæ•°æ®ä¸°å¯Œï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¸è§çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”¨ä¾‹ï¼Œå…¶ä¸­æ¯åˆ—å¯ä»¥ä»£è¡¨ä¸åŒçš„å±æ€§è¿›è¡Œä¸°å¯Œã€‚

Since then, Iâ€™ve seen this UX pop up in a few places (Clay and Otto are two great examples of this).
ä»é‚£æ—¶èµ·ï¼Œæˆ‘åœ¨ä¸€äº›åœ°æ–¹çœ‹åˆ°äº†è¿™ç§ç”¨æˆ·ä½“éªŒï¼ˆ[Clay](https://www.clay.com/)å’Œ[Otto](https://x.com/SullyOmarr/status/1803779798658859067)æ˜¯ä¸¤ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼‰ã€‚

### Generative UI
ç”Ÿæˆå¼ç”¨æˆ·ç•Œé¢

The concept of â€œgenerative UIâ€ can mean a few different things.
â€œç”Ÿæˆå¼ç”¨æˆ·ç•Œé¢â€çš„æ¦‚å¿µå¯ä»¥æ„å‘³ç€å‡ ç§ä¸åŒçš„äº‹æƒ…ã€‚

One interpretation is a truly generative UI where the model generates the raw components to display.
ä¸€ç§è§£é‡Šæ˜¯ä¸€ä¸ªçœŸæ­£çš„ç”Ÿæˆå¼ç”¨æˆ·ç•Œé¢ï¼Œå…¶ä¸­æ¨¡å‹ç”Ÿæˆè¦æ˜¾ç¤ºçš„åŸå§‹ç»„ä»¶ã€‚

This is similar to applications like WebSim. Under the hood, the agent is largely writing raw HTML, allowing it to have FULL control over what is displayed.
è¿™ç±»ä¼¼äºWebSimè¿™æ ·çš„åº”ç”¨ç¨‹åºã€‚åœ¨åº•å±‚ï¼Œä»£ç†ä¸»è¦ç¼–å†™åŸå§‹HTMLï¼Œä½¿å…¶èƒ½å¤Ÿå®Œå…¨æ§åˆ¶æ˜¾ç¤ºçš„å†…å®¹ã€‚

However, this approach allows for a lot of variability in the quality of the generated HTML, so the end result may look a bit wild or unpolished.
ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å…è®¸ç”Ÿæˆçš„HTMLè´¨é‡æœ‰å¾ˆå¤§çš„å˜åŒ–ï¼Œå› æ­¤æœ€ç»ˆç»“æœå¯èƒ½çœ‹èµ·æ¥æœ‰ç‚¹ç²—ç³™æˆ–ä¸å®Œå–„ã€‚

Another more constrained approach to generative UI involves programmatically mapping the LLM response to different pre-defined UI components.
å¦ä¸€ç§æ›´å—é™åˆ¶çš„ç”Ÿæˆå¼ç”¨æˆ·ç•Œé¢æ–¹æ³•æ¶‰åŠä»¥ç¼–ç¨‹æ–¹å¼å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„å“åº”æ˜ å°„åˆ°ä¸åŒçš„é¢„å®šä¹‰ç”¨æˆ·ç•Œé¢ç»„ä»¶ã€‚

This is often done with tool calls. For instance, if an LLM calls a weather API, it then triggers the rendering of a weather map UI component.
è¿™é€šå¸¸é€šè¿‡å·¥å…·è°ƒç”¨æ¥å®Œæˆã€‚ä¾‹å¦‚ï¼Œå¦‚æœå¤§å‹è¯­è¨€æ¨¡å‹è°ƒç”¨å¤©æ°”APIï¼Œå®ƒä¼šè§¦å‘å¤©æ°”å›¾ç”¨æˆ·ç•Œé¢ç»„ä»¶çš„æ¸²æŸ“ã€‚

Since the components rendered are not truly generated (but more chosen), the resulting UI will be more polished, though less flexible in what it can generate.
ç”±äºæ¸²æŸ“çš„ç»„ä»¶ä¸æ˜¯å®Œå…¨ç”Ÿæˆçš„ï¼ˆè€Œæ˜¯æ›´å¤šé€‰æ‹©çš„ï¼‰ï¼Œå› æ­¤ç”Ÿæˆçš„ç”¨æˆ·ç•Œé¢å°†æ›´åŠ å®Œå–„ï¼Œå°½ç®¡åœ¨ç”Ÿæˆå†…å®¹æ–¹é¢ä¸é‚£ä¹ˆçµæ´»ã€‚

You can learn more about generative UI in our video series here.
æ‚¨å¯ä»¥åœ¨æˆ‘ä»¬çš„è§†é¢‘ç³»åˆ—ä¸­äº†è§£æ›´å¤šå…³äºç”Ÿæˆå¼ç”¨æˆ·ç•Œé¢çš„ä¿¡æ¯ã€‚

### Collaborative UX
åä½œç”¨æˆ·ä½“éªŒ

A lesser explored UX: what happens when agents and humans are working together?
ä¸€ä¸ªè¾ƒå°‘æ¢ç´¢çš„ç”¨æˆ·ä½“éªŒï¼šå½“ä»£ç†å’Œäººç±»ä¸€èµ·å·¥ä½œæ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

Think Google Docs, where you can collaborate with teammates on writing or editing documents - but instead, one of your collaborators is an agent.
æƒ³æƒ³Google Docsï¼Œæ‚¨å¯ä»¥ä¸å›¢é˜Ÿæˆå‘˜åˆä½œç¼–å†™æˆ–ç¼–è¾‘æ–‡æ¡£â€”â€”ä½†å…¶ä¸­ä¸€ä¸ªåˆä½œè€…æ˜¯ä»£ç†ã€‚

The leading thinkers in the space in my mind are Geoffrey Litt and Ink & Switch, with their Patchwork project being a great example of human-agent collaboration.
åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™ä¸ªé¢†åŸŸçš„é¢†å…ˆæ€æƒ³å®¶æ˜¯Geoffrey Littå’ŒInk & Switchï¼Œä»–ä»¬çš„Patchworké¡¹ç›®æ˜¯äººç±»ä¸ä»£ç†åˆä½œçš„ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ã€‚

How does collaborative UX compare to the previously discussed ambient UX?
åä½œç”¨æˆ·ä½“éªŒä¸ä¹‹å‰è®¨è®ºçš„ç¯å¢ƒç”¨æˆ·ä½“éªŒæœ‰ä½•ä¸åŒï¼Ÿ

Our founding engineer Nuno highlights the key differences between the two:
æˆ‘ä»¬çš„åˆ›å§‹å·¥ç¨‹å¸ˆNunoå¼ºè°ƒäº†ä¸¤è€…ä¹‹é—´çš„ä¸»è¦åŒºåˆ«ï¼š

The main difference between ambient and collaborative is concurrency:
ç¯å¢ƒç”¨æˆ·ä½“éªŒå’Œåä½œç”¨æˆ·ä½“éªŒä¹‹é—´çš„ä¸»è¦åŒºåˆ«æ˜¯å¹¶å‘æ€§ï¼š

- In a collaborative UX, you and the LLM often do work simultaneously, "feeding" off of each others work
åœ¨åä½œç”¨æˆ·ä½“éªŒä¸­ï¼Œæ‚¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸åŒæ—¶å·¥ä½œï¼Œç›¸äº’â€œå€Ÿé‰´â€å½¼æ­¤çš„å·¥ä½œ

- In an ambient UX, the LLM is continuously doing work in the background while you, the user, focus on something else entirely
åœ¨ç¯å¢ƒç”¨æˆ·ä½“éªŒä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åå°æŒç»­å·¥ä½œï¼Œè€Œæ‚¨ï¼Œç”¨æˆ·ï¼Œå®Œå…¨ä¸“æ³¨äºå…¶ä»–äº‹æƒ…

These differences also translate into distinct requirements when building these applications:
è¿™äº›å·®å¼‚åœ¨æ„å»ºè¿™äº›åº”ç”¨ç¨‹åºæ—¶ä¹Ÿè½¬åŒ–ä¸ºä¸åŒçš„éœ€æ±‚ï¼š

- For collaborative UX, you may need to display granular pieces of work being done by the LLM.(This falls somewhere on the spectrum between individual tokens and larger, application-specific pieces of work like paragraphs in a text editor).A common requirement might be having an automated way to merge concurrent changes, similar to how Google Doc manages real-time collaboration.
å¯¹äºåä½œç”¨æˆ·ä½“éªŒï¼Œæ‚¨å¯èƒ½éœ€è¦æ˜¾ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹æ­£åœ¨å®Œæˆçš„ç»†ç²’åº¦å·¥ä½œã€‚ï¼ˆè¿™ä»‹äºå•ä¸ªæ ‡è®°å’Œæ›´å¤§çš„ã€ç‰¹å®šäºåº”ç”¨ç¨‹åºçš„å·¥ä½œä¹‹é—´ï¼Œä¾‹å¦‚æ–‡æœ¬ç¼–è¾‘å™¨ä¸­çš„æ®µè½ï¼‰ã€‚ä¸€ä¸ªå¸¸è§çš„éœ€æ±‚å¯èƒ½æ˜¯æœ‰ä¸€ç§è‡ªåŠ¨åˆå¹¶å¹¶å‘æ›´æ”¹çš„æ–¹æ³•ï¼Œç±»ä¼¼äºGoogle Docç®¡ç†å®æ—¶åä½œçš„æ–¹å¼ã€‚

- For ambient UX, you may need to summarize the work done by the LLM or highlight any changes.A common requirement might be to trigger a run from an event that happened in some other system, e.g. via a webhook.
å¯¹äºç¯å¢ƒç”¨æˆ·ä½“éªŒï¼Œæ‚¨å¯èƒ½éœ€è¦æ€»ç»“å¤§å‹è¯­è¨€æ¨¡å‹å®Œæˆçš„å·¥ä½œæˆ–çªå‡ºä»»ä½•æ›´æ”¹ã€‚ä¸€ä¸ªå¸¸è§çš„éœ€æ±‚å¯èƒ½æ˜¯ä»å…¶ä»–ç³»ç»Ÿä¸­å‘ç”Ÿçš„äº‹ä»¶è§¦å‘è¿è¡Œï¼Œä¾‹å¦‚é€šè¿‡webhookã€‚

### Why are we thinking about this?
ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦è€ƒè™‘è¿™ä¸ªï¼Ÿ

LangChain is not known for being a UI/UX focused company. But we spend a lot of time thinking about this. Why?
LangChainå¹¶ä¸æ˜¯ä¸€å®¶ä»¥ç”¨æˆ·ç•Œé¢/ç”¨æˆ·ä½“éªŒä¸ºé‡ç‚¹çš„å…¬å¸ã€‚ä½†æˆ‘ä»¬èŠ±äº†å¾ˆå¤šæ—¶é—´æ€è€ƒè¿™ä¸ªã€‚ä¸ºä»€ä¹ˆï¼Ÿ

Our goal is to make it as easy as possible to build agentic applications.
æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°½å¯èƒ½ç®€åŒ–æ„å»ºä»£ç†åº”ç”¨ç¨‹åºçš„è¿‡ç¨‹ã€‚

How humans interact with these applications greatly affects the type of infrastructure that we need to build.
äººç±»å¦‚ä½•ä¸è¿™äº›åº”ç”¨ç¨‹åºäº’åŠ¨æå¤§åœ°å½±å“äº†æˆ‘ä»¬éœ€è¦æ„å»ºçš„åŸºç¡€è®¾æ–½ç±»å‹ã€‚

For example - we recently launched LangGraph Cloud, our infrastructure for deploying agentic applications at scale.
ä¾‹å¦‚â€”â€”æˆ‘ä»¬æœ€è¿‘æ¨å‡ºäº†LangGraph Cloudï¼Œè¿™æ˜¯æˆ‘ä»¬ç”¨äºå¤§è§„æ¨¡éƒ¨ç½²ä»£ç†åº”ç”¨ç¨‹åºçš„åŸºç¡€è®¾æ–½ã€‚

It features multiple streaming modes, support for â€œdouble-textingâ€ use cases, and async background runs.
å®ƒå…·æœ‰å¤šç§æµæ¨¡å¼ï¼Œæ”¯æŒâ€œåŒæ–‡æœ¬â€ç”¨ä¾‹å’Œå¼‚æ­¥åå°è¿è¡Œã€‚

All of these were directly influenced by UI/UX trends that we saw emerging.
æ‰€æœ‰è¿™äº›éƒ½ç›´æ¥å—åˆ°æˆ‘ä»¬çœ‹åˆ°çš„æ–°å…´ç”¨æˆ·ç•Œé¢/ç”¨æˆ·ä½“éªŒè¶‹åŠ¿çš„å½±å“ã€‚

If you are building an application with a novel or interesting UI/UX (e.g. non-streaming chat) we would love to hear from you at hello@langchain.dev!
å¦‚æœæ‚¨æ­£åœ¨æ„å»ºä¸€ä¸ªå…·æœ‰æ–°é¢–æˆ–æœ‰è¶£çš„ç”¨æˆ·ç•Œé¢/ç”¨æˆ·ä½“éªŒçš„åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚éæµå¼èŠå¤©ï¼‰ï¼Œæˆ‘ä»¬å¾ˆæƒ³å¬åˆ°æ‚¨çš„æ¶ˆæ¯ï¼Œè¯·å‘é€é‚®ä»¶è‡³hello@langchain.devï¼


## [Memory for agents](https://blog.langchain.dev/memory-for-agents/)
ä»£ç†çš„è®°å¿†

If agents are the biggest buzzword of LLM application development in 2024, memory might be the second biggest. But what even is memory?
å¦‚æœä»£ç†æ˜¯2024å¹´LLMåº”ç”¨å¼€å‘ä¸­æœ€å¤§çš„æµè¡Œè¯ï¼Œé‚£ä¹ˆè®°å¿†å¯èƒ½æ˜¯ç¬¬äºŒå¤§çš„ã€‚ä½†è®°å¿†ç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Ÿ

At a high level, memory is just a system that remembers something about previous interactions. This can be crucial for building a good agent experience.
ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œè®°å¿†åªæ˜¯ä¸€ä¸ªè®°ä½å…ˆå‰äº¤äº’å†…å®¹çš„ç³»ç»Ÿã€‚è¿™å¯¹äºæ„å»ºè‰¯å¥½çš„ä»£ç†ä½“éªŒè‡³å…³é‡è¦ã€‚

Imagine if you had a coworker who never remembered what you told them, forcing you to keep repeating that information - that would be insanely frustrating!
æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªåŒäº‹ä»ä¸è®°å¾—ä½ å‘Šè¯‰ä»–ä»¬çš„äº‹æƒ…ï¼Œè¿«ä½¿ä½ ä¸æ–­é‡å¤è¿™äº›ä¿¡æ¯â€”â€”é‚£å°†æ˜¯éå¸¸ä»¤äººæ²®ä¸§çš„ï¼

People often expect LLM systems to innately have memory, maybe because LLMs feel so human-like already. However, LLMs themselves do NOT inherently remember things â€” so you need to intentionally add memory in.
äººä»¬å¸¸å¸¸æœŸæœ›LLMç³»ç»Ÿå¤©ç”Ÿå°±æœ‰è®°å¿†ï¼Œå¯èƒ½æ˜¯å› ä¸ºLLMå·²ç»æ„Ÿè§‰éå¸¸åƒäººç±»ã€‚ç„¶è€Œï¼ŒLLMæœ¬èº«å¹¶ä¸ä¼šå¤©ç”Ÿè®°ä½äº‹æƒ…â€”â€”æ‰€ä»¥ä½ éœ€è¦æœ‰æ„åœ°æ·»åŠ è®°å¿†ã€‚

But how exactly should you think about doing that?
ä½†ä½ ç©¶ç«Ÿåº”è¯¥å¦‚ä½•è€ƒè™‘è¿™æ ·åšå‘¢ï¼Ÿ

### Memory is application-specific
è®°å¿†æ˜¯ç‰¹å®šäºåº”ç”¨çš„

Weâ€™ve been thinking about memory for a while, and we believe that memory is application-specific.
æˆ‘ä»¬å·²ç»æ€è€ƒè®°å¿†æœ‰ä¸€æ®µæ—¶é—´äº†ï¼Œæˆ‘ä»¬ç›¸ä¿¡è®°å¿†æ˜¯ç‰¹å®šäºåº”ç”¨çš„ã€‚

What [Replitâ€™s coding agent](https://blog.langchain.dev/customers-replit/) may choose to remember about a given user is very different than what [Unifyâ€™s research agent](https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/) might remember.
Replitçš„ç¼–ç ä»£ç†å¯èƒ½é€‰æ‹©è®°ä½ç»™å®šç”¨æˆ·çš„å†…å®¹ä¸Unifyçš„ç ”ç©¶ä»£ç†å¯èƒ½è®°ä½çš„å†…å®¹éå¸¸ä¸åŒã€‚

Replit may choose to remember Python libraries that the user likes; Unify may remember the industries of the companies a user is researching.
Replitå¯èƒ½ä¼šé€‰æ‹©è®°ä½ç”¨æˆ·å–œæ¬¢çš„Pythonåº“ï¼›Unifyå¯èƒ½ä¼šè®°ä½ç”¨æˆ·æ­£åœ¨ç ”ç©¶çš„å…¬å¸çš„è¡Œä¸šã€‚

Not only does what an agent remember vary by application, but how the agent remembers may differ too.
ä¸ä»…ä»£ç†è®°ä½çš„å†…å®¹å› åº”ç”¨è€Œå¼‚ï¼Œä»£ç†è®°ä½çš„æ–¹å¼ä¹Ÿå¯èƒ½ä¸åŒã€‚

As discussed in a previous post, a key aspect of agents is the UX around them. Different UXs offer distinct ways to gather and update feedback accordingly.
å¦‚ä¹‹å‰çš„æ–‡ç« æ‰€è®¨è®ºçš„ï¼Œä»£ç†çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯å›´ç»•å®ƒä»¬çš„ç”¨æˆ·ä½“éªŒã€‚ä¸åŒçš„ç”¨æˆ·ä½“éªŒæä¾›äº†ä¸åŒçš„æ–¹å¼æ¥ç›¸åº”åœ°æ”¶é›†å’Œæ›´æ–°åé¦ˆã€‚

So, how are we approaching memory at LangChain?
é‚£ä¹ˆï¼Œæˆ‘ä»¬åœ¨LangChainæ˜¯å¦‚ä½•å¤„ç†è®°å¿†çš„å‘¢ï¼Ÿ

ğŸ’¡
Much like our approach to agents: we aim to give users low-level control over memory and the ability to customize it as they see fit.
å°±åƒæˆ‘ä»¬å¯¹ä»£ç†çš„å¤„ç†æ–¹å¼ä¸€æ ·ï¼šæˆ‘ä»¬æ—¨åœ¨è®©ç”¨æˆ·å¯¹è®°å¿†æœ‰ä½å±‚æ¬¡çš„æ§åˆ¶æƒï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®éœ€è¦è¿›è¡Œå®šåˆ¶ã€‚

This philosophy guided much of our development of the Memory Store, which we added into LangGraph last week.
è¿™ç§ç†å¿µæŒ‡å¯¼äº†æˆ‘ä»¬å¯¹Memory Storeçš„å¤§éƒ¨åˆ†å¼€å‘ï¼Œæˆ‘ä»¬ä¸Šå‘¨å°†å…¶æ·»åŠ åˆ°LangGraphä¸­ã€‚

### Types of memory
è®°å¿†çš„ç±»å‹

While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. These types of memory are nothing new - they mimic human memory types.
è™½ç„¶ä½ çš„ä»£ç†æ‹¥æœ‰çš„è®°å¿†çš„ç¡®åˆ‡å½¢æ€å¯èƒ½å› åº”ç”¨è€Œå¼‚ï¼Œä½†æˆ‘ä»¬ç¡®å®çœ‹åˆ°äº†ä¸åŒçš„é«˜çº§è®°å¿†ç±»å‹ã€‚è¿™äº›è®°å¿†ç±»å‹å¹¶ä¸æ–°é²œâ€”â€”å®ƒä»¬æ¨¡ä»¿äººç±»çš„è®°å¿†ç±»å‹ã€‚

Thereâ€™s been some great work to map these human memory types to agent memory. My favorite is the [CoALA paper](https://arxiv.org/pdf/2309.02427).Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type.
å·²ç»æœ‰ä¸€äº›å¾ˆå¥½çš„å·¥ä½œå°†è¿™äº›äººç±»è®°å¿†ç±»å‹æ˜ å°„åˆ°ä»£ç†è®°å¿†ä¸­ã€‚æˆ‘æœ€å–œæ¬¢çš„æ˜¯CoALAè®ºæ–‡ã€‚ä»¥ä¸‹æ˜¯æˆ‘å¯¹æ¯ç§ç±»å‹çš„ç²—ç•¥ã€ELI5è§£é‡Šï¼Œä»¥åŠå½“ä»Šä»£ç†å¯èƒ½å¦‚ä½•ä½¿ç”¨å’Œæ›´æ–°è¿™ç§è®°å¿†ç±»å‹çš„å®é™…æ–¹æ³•ã€‚

![](/images/2024/LangChain-Blog-In-the-Loop/Decision-procedure-diagram-from-CoALA-paper.jpeg)

Decision procedure diagram from CoALA paper (Sumers, Yao, Narasimhan, Griffiths 2024)
CoALAè®ºæ–‡ä¸­çš„å†³ç­–ç¨‹åºå›¾ï¼ˆSumers, Yao, Narasimhan, Griffiths 2024ï¼‰

- Procedural Memory: ç¨‹åºæ€§è®°å¿†
- Semantic Memory: è¯­ä¹‰è®°å¿†
- Episodic Memory: æƒ…æ™¯è®°å¿†
- Working Memory: å·¥ä½œè®°å¿†

### Procedural Memory
ç¨‹åºæ€§è®°å¿†

This term refers to long-term memory for how to perform tasks, similar to a brainâ€™s core instruction set.
è¿™ä¸ªæœ¯è¯­æŒ‡çš„æ˜¯å¦‚ä½•æ‰§è¡Œä»»åŠ¡çš„é•¿æœŸè®°å¿†ï¼Œç±»ä¼¼äºå¤§è„‘çš„æ ¸å¿ƒæŒ‡ä»¤é›†ã€‚

Procedural memory in humans: remembering how to ride a bike.
äººç±»çš„ç¨‹åºæ€§è®°å¿†ï¼šè®°ä½å¦‚ä½•éª‘è‡ªè¡Œè½¦ã€‚

Procedural memory in Agents: the CoALA paper describes procedural memory as the combination of LLM weights and agent code, which fundamentally determine how the agent works.
ä»£ç†çš„ç¨‹åºæ€§è®°å¿†ï¼šCoALAè®ºæ–‡å°†ç¨‹åºæ€§è®°å¿†æè¿°ä¸ºLLMæƒé‡å’Œä»£ç†ä»£ç çš„ç»„åˆï¼Œè¿™äº›åŸºæœ¬ä¸Šå†³å®šäº†ä»£ç†çš„å·¥ä½œæ–¹å¼ã€‚

In practice, we donâ€™t see many (any?) agentic systems that update the weights of their LLM automatically or rewrite their code.
åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰çœ‹åˆ°å¾ˆå¤šï¼ˆä»»ä½•ï¼Ÿï¼‰ä»£ç†ç³»ç»Ÿè‡ªåŠ¨æ›´æ–°å…¶LLMçš„æƒé‡æˆ–é‡å†™å…¶ä»£ç ã€‚

We do, however, see some examples of an agent updating its own system prompt. While this is the closest practical example, it remains relatively uncommon.
ç„¶è€Œï¼Œæˆ‘ä»¬ç¡®å®çœ‹åˆ°äº†ä¸€äº›ä»£ç†æ›´æ–°å…¶è‡ªèº«ç³»ç»Ÿæç¤ºçš„ä¾‹å­ã€‚è™½ç„¶è¿™æ˜¯æœ€æ¥è¿‘çš„å®é™…ä¾‹å­ï¼Œä½†å®ƒä»ç„¶ç›¸å¯¹ç½•è§ã€‚

### Semantic Memory
è¯­ä¹‰è®°å¿†

This is someoneâ€™s long-term store of knowledge.
è¿™æ˜¯æŸäººçš„é•¿æœŸçŸ¥è¯†å‚¨å­˜ã€‚

Semantic memory in humans: itâ€™s composed of pieces of information such as facts learned in school, what concepts mean and how they are related.
äººç±»çš„è¯­ä¹‰è®°å¿†ï¼šå®ƒç”±åœ¨å­¦æ ¡å­¦åˆ°çš„äº‹å®ã€æ¦‚å¿µçš„å«ä¹‰åŠå…¶å…³ç³»ç­‰ä¿¡æ¯ç»„æˆã€‚

Semantic memory in agents: the CoALA paper describes semantic memory as a repository of facts about the world.
ä»£ç†çš„è¯­ä¹‰è®°å¿†ï¼šCoALAè®ºæ–‡å°†è¯­ä¹‰è®°å¿†æè¿°ä¸ºå…³äºä¸–ç•Œäº‹å®çš„å­˜å‚¨åº“ã€‚

Today, this is most often used by agents to personalize an application.
ä»Šå¤©ï¼Œè¿™é€šå¸¸è¢«ä»£ç†ç”¨æ¥ä¸ªæ€§åŒ–åº”ç”¨ç¨‹åºã€‚

Practically, we see this being done by using an LLM to extract information from the conversation or interactions the agent had.
å®é™…ä¸Šï¼Œæˆ‘ä»¬çœ‹åˆ°è¿™æ˜¯é€šè¿‡ä½¿ç”¨LLMä»ä»£ç†çš„å¯¹è¯æˆ–äº¤äº’ä¸­æå–ä¿¡æ¯æ¥å®Œæˆçš„ã€‚

The exact shape of this information is usually application-specific. This information is then retrieved in future conversations and inserted into the system prompt to influence the agentâ€™s responses.
è¿™äº›ä¿¡æ¯çš„ç¡®åˆ‡å½¢æ€é€šå¸¸æ˜¯ç‰¹å®šäºåº”ç”¨çš„ã€‚è¿™äº›ä¿¡æ¯éšååœ¨æœªæ¥çš„å¯¹è¯ä¸­è¢«æ£€ç´¢å¹¶æ’å…¥ç³»ç»Ÿæç¤ºä¸­ï¼Œä»¥å½±å“ä»£ç†çš„å“åº”ã€‚

### Episodic Memory
æƒ…æ™¯è®°å¿†

This refers to recalling specific past events.
è¿™æŒ‡çš„æ˜¯å›å¿†ç‰¹å®šçš„è¿‡å»äº‹ä»¶ã€‚

Episodic memory in humans: when a person recalls a particular event (or â€œepisodeâ€) experienced in the past.
äººç±»çš„æƒ…æ™¯è®°å¿†ï¼šå½“ä¸€ä¸ªäººå›å¿†è¿‡å»ç»å†çš„ç‰¹å®šäº‹ä»¶ï¼ˆæˆ–â€œæƒ…èŠ‚â€ï¼‰æ—¶ã€‚

Episodic memory in agents: the CoALA paper defines episodic memory as storing sequences of the agentâ€™s past actions.
ä»£ç†çš„æƒ…æ™¯è®°å¿†ï¼šCoALAè®ºæ–‡å°†æƒ…æ™¯è®°å¿†å®šä¹‰ä¸ºå­˜å‚¨ä»£ç†è¿‡å»è¡ŒåŠ¨çš„åºåˆ—ã€‚

This is used primarily to get an agent to perform as intended.
è¿™ä¸»è¦ç”¨äºè®©ä»£ç†æŒ‰é¢„æœŸæ‰§è¡Œã€‚

In practice, episodic memory is implemented as few-shot example prompting.
åœ¨å®è·µä¸­ï¼Œæƒ…æ™¯è®°å¿†è¢«å®ç°ä¸ºå°‘é‡ç¤ºä¾‹æç¤ºã€‚

If you collect enough of these sequences, then this can be done via dynamic few-shot prompting.
å¦‚æœä½ æ”¶é›†äº†è¶³å¤Ÿå¤šçš„è¿™äº›åºåˆ—ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡åŠ¨æ€å°‘é‡ç¤ºä¾‹æç¤ºæ¥å®Œæˆã€‚

This is usually great for guiding the agent if there is a correct way to perform specific actions that have been done before.
å¦‚æœæœ‰ä¸€ç§æ­£ç¡®çš„æ–¹æ³•æ¥æ‰§è¡Œä¹‹å‰å®Œæˆçš„ç‰¹å®šæ“ä½œï¼Œè¿™é€šå¸¸éå¸¸é€‚åˆæŒ‡å¯¼ä»£ç†ã€‚

In contrast, semantic memory is more relevant if there isnâ€™t necessarily a correct way to do things, or if the agent is constantly doing new things so the previous examples donâ€™t help much.
ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¦‚æœæ²¡æœ‰å¿…è¦çš„æ­£ç¡®æ–¹æ³•æ¥åšäº‹æƒ…ï¼Œæˆ–è€…ä»£ç†ä¸æ–­åœ¨åšæ–°äº‹æƒ…ï¼Œé‚£ä¹ˆè¯­ä¹‰è®°å¿†æ›´ä¸ºç›¸å…³ï¼Œå› ä¸ºä¹‹å‰çš„ç¤ºä¾‹å¸®åŠ©ä¸å¤§ã€‚

### How to update memory
å¦‚ä½•æ›´æ–°è®°å¿†

Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory.
é™¤äº†è€ƒè™‘åœ¨ä»£ç†ä¸­æ›´æ–°çš„è®°å¿†ç±»å‹å¤–ï¼Œæˆ‘ä»¬è¿˜çœ‹åˆ°å¼€å‘äººå‘˜åœ¨è€ƒè™‘å¦‚ä½•æ›´æ–°ä»£ç†è®°å¿†ã€‚

One way to update agent memory is â€œin the hot pathâ€. This is where the agent system explicitly decides to remember facts (usually via tool calling) before responding. This is the approach taken by ChatGPT.
æ›´æ–°ä»£ç†è®°å¿†çš„ä¸€ç§æ–¹æ³•æ˜¯â€œåœ¨çƒ­è·¯å¾„ä¸­â€ã€‚è¿™æ˜¯ä»£ç†ç³»ç»Ÿåœ¨å“åº”ä¹‹å‰æ˜ç¡®å†³å®šè®°ä½äº‹å®ï¼ˆé€šå¸¸é€šè¿‡å·¥å…·è°ƒç”¨ï¼‰çš„åœ°æ–¹ã€‚è¿™æ˜¯ChatGPTé‡‡ç”¨çš„æ–¹æ³•ã€‚

Another way to update memory is â€œin the backgroundâ€. In this case, a background process runs either during or after the conversation to update memory.
å¦ä¸€ç§æ›´æ–°è®°å¿†çš„æ–¹æ³•æ˜¯åœ¨â€œåå°â€ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåå°è¿›ç¨‹åœ¨å¯¹è¯æœŸé—´æˆ–ä¹‹åè¿è¡Œä»¥æ›´æ–°è®°å¿†ã€‚

Comparing these two approaches, the â€œin the hot pathâ€ approach has the downside of introducing some extra latency before any response is delivered. It also requires combining the memory logic with the agent logic.
æ¯”è¾ƒè¿™ä¸¤ç§æ–¹æ³•ï¼Œâ€œåœ¨çƒ­è·¯å¾„ä¸­â€çš„æ–¹æ³•çš„ç¼ºç‚¹æ˜¯ï¼Œåœ¨ä»»ä½•å“åº”äº¤ä»˜ä¹‹å‰å¼•å…¥äº†ä¸€äº›é¢å¤–çš„å»¶è¿Ÿã€‚å®ƒè¿˜éœ€è¦å°†è®°å¿†é€»è¾‘ä¸ä»£ç†é€»è¾‘ç»“åˆèµ·æ¥ã€‚

However, running in the background avoids those issues - thereâ€™s no added latency, and memory logic remains separate.
ç„¶è€Œï¼Œåœ¨åå°è¿è¡Œé¿å…äº†è¿™äº›é—®é¢˜â€”â€”æ²¡æœ‰å¢åŠ å»¶è¿Ÿï¼Œè®°å¿†é€»è¾‘ä¿æŒç‹¬ç«‹ã€‚

But running â€œin the backgroundâ€ also has its own drawbacks: the memory is not updated immediately, and extra logic is needed determine when to kick off the background process.
ä½†åœ¨â€œåå°â€è¿è¡Œä¹Ÿæœ‰å…¶è‡ªèº«çš„ç¼ºç‚¹ï¼šè®°å¿†ä¸ä¼šç«‹å³æ›´æ–°ï¼Œå¹¶ä¸”éœ€è¦é¢å¤–çš„é€»è¾‘æ¥ç¡®å®šä½•æ—¶å¯åŠ¨åå°è¿›ç¨‹ã€‚

Another way to updating memory involves user feedback, which is particularly relevant to episodic memory.
å¦ä¸€ç§æ›´æ–°è®°å¿†çš„æ–¹æ³•æ¶‰åŠç”¨æˆ·åé¦ˆï¼Œè¿™ä¸æƒ…æ™¯è®°å¿†ç‰¹åˆ«ç›¸å…³ã€‚

For example, If the user marks an interaction as a positive one, you can save that feedback to recall in the future.
ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ·å°†æŸæ¬¡äº¤äº’æ ‡è®°ä¸ºæ­£é¢äº¤äº’ï¼Œä½ å¯ä»¥ä¿å­˜è¯¥åé¦ˆä»¥ä¾›å°†æ¥å›å¿†ã€‚

### Why do we care about memory for agents?
ä¸ºä»€ä¹ˆæˆ‘ä»¬å…³å¿ƒä»£ç†çš„è®°å¿†ï¼Ÿ

How does this impact what weâ€™re building at LangChain? Well, memory greatly affects the usefulness of an agentic system, so weâ€™re extremely interested in making it as easy as possible to leverage memory for applications.
è¿™å¯¹æˆ‘ä»¬åœ¨LangChainæ„å»ºçš„å†…å®¹æœ‰ä½•å½±å“ï¼Ÿå—¯ï¼Œè®°å¿†æå¤§åœ°å½±å“äº†ä»£ç†ç³»ç»Ÿçš„æœ‰ç”¨æ€§ï¼Œæ‰€ä»¥æˆ‘ä»¬éå¸¸æ„Ÿå…´è¶£çš„æ˜¯å°½å¯èƒ½ç®€åŒ–åˆ©ç”¨è®°å¿†æ¥åº”ç”¨ã€‚

To this end, weâ€™ve built a lot of functionality for this into our products. This includes:
ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨äº§å“ä¸­æ„å»ºäº†è®¸å¤šåŠŸèƒ½ã€‚è¿™åŒ…æ‹¬ï¼š

- Low-level abstractions for a memory store in LangGraph to give you full control over your agentâ€™s memory
åœ¨LangGraphä¸­ä¸ºè®°å¿†å­˜å‚¨æä¾›çš„ä½å±‚æ¬¡æŠ½è±¡ï¼Œä»¥ä¾¿ä½ å®Œå…¨æ§åˆ¶ä»£ç†çš„è®°å¿†

- Template for running memory both â€œin the hot pathâ€ and â€œin the backgroundâ€ in LangGraph
åœ¨LangGraphä¸­è¿è¡Œè®°å¿†çš„â€œçƒ­è·¯å¾„â€å’Œâ€œåå°â€æ¨¡æ¿

- Dynamic few shot example selection in LangSmith for rapid iteration
åœ¨LangSmithä¸­è¿›è¡Œå¿«é€Ÿè¿­ä»£çš„åŠ¨æ€å°‘é‡ç¤ºä¾‹é€‰æ‹©

Weâ€™ve even built a few applications of our own that leverage memory! Itâ€™s still early though, so weâ€™ll keep on learning about agent memory and the areas it can be used effectively ğŸ™‚
æˆ‘ä»¬ç”šè‡³æ„å»ºäº†ä¸€äº›åˆ©ç”¨è®°å¿†çš„åº”ç”¨ç¨‹åºï¼ä¸è¿‡è¿˜å¤„äºæ—©æœŸé˜¶æ®µï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ç»§ç»­å­¦ä¹ ä»£ç†è®°å¿†åŠå…¶å¯ä»¥æœ‰æ•ˆä½¿ç”¨çš„é¢†åŸŸğŸ™‚


## [Communication is all you need](https://blog.langchain.dev/communication-is-all-you-need/)
æ²Ÿé€šå°±æ˜¯ä½ éœ€è¦çš„ä¸€åˆ‡

â€œWhat weâ€™ve got here is failure to communicateâ€ - Cool Hand Luke (1967)
â€œæˆ‘ä»¬è¿™é‡Œçš„é—®é¢˜æ˜¯æ²Ÿé€šå¤±è´¥â€ - ã€Šå†·æ‰‹å¢å…‹ã€‹ï¼ˆ1967ï¼‰

Communication is the hardest part of life. Itâ€™s also the hardest part of building LLM applications.
æ²Ÿé€šæ˜¯ç”Ÿæ´»ä¸­æœ€éš¾çš„éƒ¨åˆ†ã€‚å®ƒä¹Ÿæ˜¯æ„å»ºLLMåº”ç”¨ç¨‹åºä¸­æœ€éš¾çš„éƒ¨åˆ†ã€‚

New hires always requires a lot of communication when first joining a company, no matter how smart they may be. This might include getting a guidebook of key procedures and best practices, having a manager step in to help the new hire get up to speed, and gaining access to specific software to do the job properly. While ramping up, giving and receiving continuous feedback ensures that the new hire is successful in their role.
æ–°å‘˜å·¥åœ¨åˆšåŠ å…¥å…¬å¸æ—¶æ€»æ˜¯éœ€è¦å¤§é‡çš„æ²Ÿé€šï¼Œæ— è®ºä»–ä»¬å¤šä¹ˆèªæ˜ã€‚è¿™å¯èƒ½åŒ…æ‹¬è·å–å…³é”®ç¨‹åºå’Œæœ€ä½³å®è·µçš„æŒ‡å—æ‰‹å†Œï¼Œç»ç†ä»‹å…¥å¸®åŠ©æ–°å‘˜å·¥è·Ÿä¸Šè¿›åº¦ï¼Œä»¥åŠè·å¾—æ­£ç¡®å®Œæˆå·¥ä½œçš„ç‰¹å®šè½¯ä»¶ã€‚åœ¨é€‚åº”è¿‡ç¨‹ä¸­ï¼ŒæŒç»­çš„åé¦ˆç¡®ä¿æ–°å‘˜å·¥åœ¨å…¶è§’è‰²ä¸­å–å¾—æˆåŠŸã€‚

Just as onboarding a new hire requires thoughtful communication, building an agent also requires high standards for good communication. As smart as the underlying LLMs may become, they will still need the proper context to function reliably, and that context needs to be communicated properly.
æ­£å¦‚å…¥èŒæ–°å‘˜å·¥éœ€è¦å‘¨åˆ°çš„æ²Ÿé€šï¼Œæ„å»ºä»£ç†ä¹Ÿéœ€è¦é«˜æ ‡å‡†çš„è‰¯å¥½æ²Ÿé€šã€‚å°½ç®¡åº•å±‚LLMå¯èƒ½å˜å¾—éå¸¸æ™ºèƒ½ï¼Œå®ƒä»¬ä»ç„¶éœ€è¦é€‚å½“çš„ä¸Šä¸‹æ–‡æ‰èƒ½å¯é åœ°è¿è¡Œï¼Œè€Œè¿™äº›ä¸Šä¸‹æ–‡éœ€è¦æ­£ç¡®åœ°ä¼ è¾¾ã€‚

ğŸ’¡
Most of the time when an agent is not performing reliably the underlying cause is not that the model is not intelligent enough, but rather that context and instructions have not been communicated properly to the model.
å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå½“ä»£ç†è¡¨ç°ä¸å¯é æ—¶ï¼Œæ ¹æœ¬åŸå› ä¸æ˜¯æ¨¡å‹ä¸å¤Ÿæ™ºèƒ½ï¼Œè€Œæ˜¯ä¸Šä¸‹æ–‡å’ŒæŒ‡ä»¤æ²¡æœ‰æ­£ç¡®ä¼ è¾¾ç»™æ¨¡å‹ã€‚

Donâ€™t get me wrong - the models do mess up and have room to improve. But more often than not, it comes down to basic communication issues.
ä¸è¦è¯¯ä¼šâ€”â€”æ¨¡å‹ç¡®å®ä¼šå‡ºé”™å¹¶ä¸”æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚ä½†æ›´å¤šæ—¶å€™ï¼Œè¿™å½’ç»“ä¸ºåŸºæœ¬çš„æ²Ÿé€šé—®é¢˜ã€‚

If we believe that communication is a key part of building LLM applications, then from that axiom, we can derive some other â€œhot takesâ€ about agents that should hold. Iâ€™ve listed a few below in brief detail. All of these could (and maybe will) be individual blogs.
å¦‚æœæˆ‘ä»¬è®¤ä¸ºæ²Ÿé€šæ˜¯æ„å»ºLLMåº”ç”¨ç¨‹åºçš„å…³é”®éƒ¨åˆ†ï¼Œé‚£ä¹ˆä»è¿™ä¸ªå…¬ç†å‡ºå‘ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºä¸€äº›å…³äºä»£ç†çš„å…¶ä»–â€œçƒ­é—¨è§‚ç‚¹â€ã€‚æˆ‘åœ¨ä¸‹é¢ç®€è¦åˆ—å‡ºäº†ä¸€äº›ã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥ï¼ˆä¹Ÿè®¸ä¼šï¼‰æˆä¸ºå•ç‹¬çš„åšå®¢ã€‚

### Why prompt engineering isnâ€™t going away
ä¸ºä»€ä¹ˆæç¤ºå·¥ç¨‹ä¸ä¼šæ¶ˆå¤±

As models improve, prompt engineering tricks like bribing an LLM with a tip or worrying about JSON vs XML formatting will become near obsolete. However, it will still be critical for you to effectively and clearly communicate to the model and give it clear instructions on how to handle various scenarios.
éšç€æ¨¡å‹çš„æ”¹è¿›ï¼Œåƒç”¨å°è´¹è´¿èµ‚LLMæˆ–æ‹…å¿ƒJSONä¸XMLæ ¼å¼åŒ–ä¹‹ç±»çš„æç¤ºå·¥ç¨‹æŠ€å·§å°†å˜å¾—å‡ ä¹è¿‡æ—¶ã€‚ç„¶è€Œï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¸”æ¸…æ™°åœ°ä¸æ¨¡å‹æ²Ÿé€šå¹¶ç»™å‡ºæ˜ç¡®çš„å¤„ç†å„ç§åœºæ™¯çš„æŒ‡ä»¤ä»ç„¶è‡³å…³é‡è¦ã€‚

ğŸ’¡
The model is not a mind reader - if you want it to behave a certain way or process specific information, you must provide that context.
æ¨¡å‹ä¸æ˜¯è¯»å¿ƒæœ¯è€…â€”â€”å¦‚æœä½ å¸Œæœ›å®ƒä»¥æŸç§æ–¹å¼è¡Œä¸ºæˆ–å¤„ç†ç‰¹å®šä¿¡æ¯ï¼Œä½ å¿…é¡»æä¾›é‚£ä¸ªä¸Šä¸‹æ–‡ã€‚

The best tip for diagnosing why your agent isnâ€™t working is to simply look at the actual calls to the LLM and the exact inputs to the promptsâ€” then make sure that if you gave these inputs to the smartest human you know, they would be able to respond as you expect. If they couldnâ€™t do that, then you need to clarify your request, typically by adjusting the prompt. This process, known as prompt engineering, is unlikely to disappear anytime soon.
è¯Šæ–­ä»£ç†ä¸ºä½•ä¸èµ·ä½œç”¨çš„æœ€ä½³å»ºè®®æ˜¯ç®€å•åœ°æŸ¥çœ‹å¯¹LLMçš„å®é™…è°ƒç”¨å’Œæç¤ºçš„ç¡®åˆ‡è¾“å…¥â€”â€”ç„¶åç¡®ä¿å¦‚æœä½ å°†è¿™äº›è¾“å…¥ç»™ä½ è®¤è¯†çš„æœ€èªæ˜çš„äººï¼Œä»–ä»¬èƒ½å¤ŸæŒ‰ä½ é¢„æœŸçš„æ–¹å¼å›åº”ã€‚å¦‚æœä»–ä»¬ä¸èƒ½åšåˆ°è¿™ä¸€ç‚¹ï¼Œé‚£ä¹ˆä½ éœ€è¦æ¾„æ¸…ä½ çš„è¯·æ±‚ï¼Œé€šå¸¸é€šè¿‡è°ƒæ•´æç¤ºã€‚è¿™ä¸€è¿‡ç¨‹è¢«ç§°ä¸ºæç¤ºå·¥ç¨‹ï¼ŒçŸ­æœŸå†…ä¸å¤ªå¯èƒ½æ¶ˆå¤±ã€‚

### Why code will make up a large part of the "cognitive architecture" of an agent
ä¸ºä»€ä¹ˆä»£ç å°†æ„æˆä»£ç†â€œè®¤çŸ¥æ¶æ„â€çš„å¤§éƒ¨åˆ†

Prompts are one way to communicate to an LLM how they should behave as part of an agentic system, but code is just as important. Code is a fantastic way to communicate how a system should behave. Compared to natural language, code lets you communicate much more precisely the steps you expect a system to take.
æç¤ºæ˜¯ä¸LLMæ²Ÿé€šå®ƒä»¬åœ¨ä»£ç†ç³»ç»Ÿä¸­åº”å¦‚ä½•è¡Œä¸ºçš„ä¸€ç§æ–¹å¼ï¼Œä½†ä»£ç åŒæ ·é‡è¦ã€‚ä»£ç æ˜¯ä¸€ç§ä¼ è¾¾ç³»ç»Ÿåº”å¦‚ä½•è¡Œä¸ºçš„ç»ä½³æ–¹å¼ã€‚ä¸è‡ªç„¶è¯­è¨€ç›¸æ¯”ï¼Œä»£ç è®©ä½ æ›´ç²¾ç¡®åœ°ä¼ è¾¾ä½ æœŸæœ›ç³»ç»Ÿé‡‡å–çš„æ­¥éª¤ã€‚

ğŸ’¡
The "cognitive architecture" of your agent will consist of both code and prompts.
ä½ çš„ä»£ç†çš„â€œè®¤çŸ¥æ¶æ„â€å°†ç”±ä»£ç å’Œæç¤ºç»„æˆã€‚

Some instructions an agent can only be communicated in natural language. Others could be either code or language. Code can be more precise and more efficient, and so there are many spots we see code being more useful than prompts when building the "cognitive architecture" of your agent.
æœ‰äº›æŒ‡ä»¤åªèƒ½ç”¨è‡ªç„¶è¯­è¨€ä¼ è¾¾ç»™ä»£ç†ã€‚å…¶ä»–çš„å¯ä»¥æ˜¯ä»£ç æˆ–è¯­è¨€ã€‚ä»£ç å¯ä»¥æ›´ç²¾ç¡®å’Œæ›´é«˜æ•ˆï¼Œå› æ­¤åœ¨æ„å»ºä»£ç†çš„â€œè®¤çŸ¥æ¶æ„â€æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°è®¸å¤šåœ°æ–¹ä»£ç æ¯”æç¤ºæ›´æœ‰ç”¨ã€‚

### Why you need an agent framework
ä¸ºä»€ä¹ˆä½ éœ€è¦ä¸€ä¸ªä»£ç†æ¡†æ¶

Some parts of coding are necessary for you, as an agent developer, to write, in order to best communicate to the agent what it should be doing. This makes up the cognitive architecture of your application and is part of your competitive advantage and moat.
ä½œä¸ºä»£ç†å¼€å‘è€…ï¼Œæœ‰äº›ä»£ç éƒ¨åˆ†æ˜¯ä½ å¿…é¡»ç¼–å†™çš„ï¼Œä»¥ä¾¿æœ€å¥½åœ°ä¼ è¾¾ç»™ä»£ç†å®ƒåº”è¯¥åšä»€ä¹ˆã€‚è¿™æ„æˆäº†ä½ çš„åº”ç”¨ç¨‹åºçš„è®¤çŸ¥æ¶æ„ï¼Œæ˜¯ä½ ç«äº‰ä¼˜åŠ¿å’ŒæŠ¤åŸæ²³çš„ä¸€éƒ¨åˆ†ã€‚

ğŸ’¡
There are other pieces of code that you will have to write that are generic infrastructure and tooling that you need to build, but don't provide any differentiation. This is where an agent framework can assist.
è¿˜æœ‰ä¸€äº›ä»£ç éƒ¨åˆ†æ˜¯ä½ å¿…é¡»ç¼–å†™çš„ï¼Œå®ƒä»¬æ˜¯ä½ éœ€è¦æ„å»ºçš„é€šç”¨åŸºç¡€è®¾æ–½å’Œå·¥å…·ï¼Œä½†ä¸æä¾›ä»»ä½•å·®å¼‚åŒ–ã€‚è¿™å°±æ˜¯ä»£ç†æ¡†æ¶å¯ä»¥å¸®åŠ©çš„åœ°æ–¹ã€‚

An agent framework facilitates this by letting you focus on the parts of code that matter - what the agent should be doing â€” while taking care of common concerns unrelated to your applicationâ€™s cognitive architecture, such as:
ä»£ç†æ¡†æ¶é€šè¿‡è®©ä½ ä¸“æ³¨äºé‡è¦çš„ä»£ç éƒ¨åˆ†â€”â€”ä»£ç†åº”è¯¥åšä»€ä¹ˆâ€”â€”åŒæ—¶å¤„ç†ä¸ä½ çš„åº”ç”¨ç¨‹åºè®¤çŸ¥æ¶æ„æ— å…³çš„å¸¸è§é—®é¢˜æ¥ä¿ƒè¿›è¿™ä¸€ç‚¹ï¼Œä¾‹å¦‚ï¼š

- Clear streaming of what the agent is doing
ä»£ç†æ­£åœ¨åšä»€ä¹ˆçš„æ¸…æ™°æµå¼ä¼ è¾“
- Persistence to enable multi-tenant memory
æŒä¹…æ€§ä»¥å®ç°å¤šç§Ÿæˆ·å†…å­˜
- Infrastructure to power human-in-the-loop interaction patterns
æ”¯æŒäººç±»åœ¨å¾ªç¯å†…äº¤äº’æ¨¡å¼çš„åŸºç¡€è®¾æ–½
- Running agents in a fault tolerant, horizontally scalable way
ä»¥å®¹é”™ã€æ°´å¹³å¯æ‰©å±•çš„æ–¹å¼è¿è¡Œä»£ç†

### Why it matters that LangGraph is the most controllable agent framework out there
ä¸ºä»€ä¹ˆLangGraphæ˜¯æœ€å¯æ§çš„ä»£ç†æ¡†æ¶å¾ˆé‡è¦

You want an agent framework that takes care of some of the issues that are listed above, but that still lets you communicate as clearly as possible (through prompts and code) what the agent should be doing. Any agent framework that obstructs that is just going to get in the way - even if it makes it easier to get started. Transparently, thatâ€™s what we saw with langchain - it made it easy to get started but suffered from built-in prompts, a hard-coded while loop, and wasnâ€™t easy to extend.
ä½ éœ€è¦ä¸€ä¸ªä»£ç†æ¡†æ¶æ¥å¤„ç†ä¸Šè¿°ä¸€äº›é—®é¢˜ï¼Œä½†ä»ç„¶è®©ä½ å°½å¯èƒ½æ¸…æ™°åœ°ï¼ˆé€šè¿‡æç¤ºå’Œä»£ç ï¼‰ä¼ è¾¾ä»£ç†åº”è¯¥åšä»€ä¹ˆã€‚ä»»ä½•é˜»ç¢è¿™ä¸€ç‚¹çš„ä»£ç†æ¡†æ¶éƒ½ä¼šå¦¨ç¢â€”â€”å³ä½¿å®ƒè®©å…¥é—¨å˜å¾—æ›´å®¹æ˜“ã€‚æ˜¾ç„¶ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨langchainä¸­çœ‹åˆ°çš„â€”â€”å®ƒè®©å…¥é—¨å˜å¾—å®¹æ˜“ï¼Œä½†å—é™äºå†…ç½®æç¤ºã€ç¡¬ç¼–ç çš„whileå¾ªç¯ï¼Œå¹¶ä¸”ä¸æ˜“æ‰©å±•ã€‚

We made sure to fix that with LangGraph.
æˆ‘ä»¬ç¡®ä¿ç”¨LangGraphè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚

ğŸ’¡
LangGraph stands apart from all other agent frameworks for its focus on being low-level, highly controllable, and highly customizable.
LangGraphä¸æ‰€æœ‰å…¶ä»–ä»£ç†æ¡†æ¶ä¸åŒï¼Œå› ä¸ºå®ƒä¸“æ³¨äºä½çº§ã€é«˜åº¦å¯æ§å’Œé«˜åº¦å¯å®šåˆ¶ã€‚

There is nothing built in that restricts the cognitive architectures you can build. The nodes and edges are nothing more than Python functions - you can put whatever you want inside them!
æ²¡æœ‰ä»»ä½•å†…ç½®çš„ä¸œè¥¿é™åˆ¶ä½ å¯ä»¥æ„å»ºçš„è®¤çŸ¥æ¶æ„ã€‚èŠ‚ç‚¹å’Œè¾¹ä¸è¿‡æ˜¯Pythonå‡½æ•°â€”â€”ä½ å¯ä»¥åœ¨å…¶ä¸­æ”¾ç½®ä»»ä½•ä½ æƒ³è¦çš„ä¸œè¥¿ï¼

Agents are going to heavily feature code as part of their cognitive architecture. Agent frameworks can help remove some of the common infrastructure needs. But they CANNOT restrict the cognitive architecture of your agent. That will impede your ability to communicate what exactly you want to happen and the agent wonâ€™t be reliable.
ä»£ç†å°†åœ¨å…¶è®¤çŸ¥æ¶æ„ä¸­å¤§é‡ä½¿ç”¨ä»£ç ã€‚ä»£ç†æ¡†æ¶å¯ä»¥å¸®åŠ©æ¶ˆé™¤ä¸€äº›å¸¸è§çš„åŸºç¡€è®¾æ–½éœ€æ±‚ã€‚ä½†å®ƒä»¬ä¸èƒ½é™åˆ¶ä½ çš„ä»£ç†çš„è®¤çŸ¥æ¶æ„ã€‚è¿™å°†å¦¨ç¢ä½ ä¼ è¾¾ä½ ç¡®åˆ‡æƒ³è¦å‘ç”Ÿçš„äº‹æƒ…çš„èƒ½åŠ›ï¼Œå¹¶ä¸”ä»£ç†å°†ä¸å¯é ã€‚

### Why agent frameworks like LangGraph are here to stay
ä¸ºä»€ä¹ˆåƒLangGraphè¿™æ ·çš„ä»£ç†æ¡†æ¶ä¼šä¸€ç›´å­˜åœ¨
A somewhat common question I get asked is: â€œas the models get better, will that remove the need for frameworks like LangGraph?â€. The underlying assumption is that the models will get so good that they will remove the need for any code around the LLM.
æˆ‘ç»å¸¸è¢«é—®åˆ°çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼šâ€œéšç€æ¨¡å‹å˜å¾—æ›´å¥½ï¼Œè¿™ä¼šæ¶ˆé™¤å¯¹åƒLangGraphè¿™æ ·çš„æ¡†æ¶çš„éœ€æ±‚å—ï¼Ÿâ€ã€‚æ½œåœ¨çš„å‡è®¾æ˜¯æ¨¡å‹ä¼šå˜å¾—å¦‚æ­¤å¥½ï¼Œä»¥è‡³äºä¸å†éœ€è¦å›´ç»•LLMçš„ä»»ä½•ä»£ç ã€‚

No.
ä¸ä¼šã€‚

If youâ€™re using LangGraph to elicit better general purpose reasoning from models, then sure, maybe.
å¦‚æœä½ ä½¿ç”¨LangGraphä»æ¨¡å‹ä¸­å¼•å‡ºæ›´å¥½çš„é€šç”¨æ¨ç†ï¼Œé‚£ä¹ˆå½“ç„¶ï¼Œå¯èƒ½ã€‚

But thatâ€™s not how most people are using it.
ä½†è¿™ä¸æ˜¯å¤§å¤šæ•°äººä½¿ç”¨å®ƒçš„æ–¹å¼ã€‚

ğŸ’¡
Most people are using LangGraph to build vertical-specific, highly customized agentic applications.
å¤§å¤šæ•°äººä½¿ç”¨LangGraphæ¥æ„å»ºç‰¹å®šå‚ç›´é¢†åŸŸçš„é«˜åº¦å®šåˆ¶çš„ä»£ç†åº”ç”¨ç¨‹åºã€‚

Communication is a key part of that, and code is a key part of communication. Communication isnâ€™t going away, and so neither is code â€” and so neither is LangGraph.
æ²Ÿé€šæ˜¯å…¶ä¸­çš„å…³é”®éƒ¨åˆ†ï¼Œä»£ç æ˜¯æ²Ÿé€šçš„å…³é”®éƒ¨åˆ†ã€‚æ²Ÿé€šä¸ä¼šæ¶ˆå¤±ï¼Œå› æ­¤ä»£ç ä¹Ÿä¸ä¼šæ¶ˆå¤±â€”â€”LangGraphä¹Ÿä¸ä¼šæ¶ˆå¤±ã€‚

### Why building agents is a multidisciplinary endeavor
ä¸ºä»€ä¹ˆæ„å»ºä»£ç†æ˜¯ä¸€ä¸ªå¤šå­¦ç§‘çš„åŠªåŠ›

One thing that we noticed quickly is that teams building agents arenâ€™t just made up of engineers.
æˆ‘ä»¬å¾ˆå¿«æ³¨æ„åˆ°ï¼Œæ„å»ºä»£ç†çš„å›¢é˜Ÿä¸ä»…ä»…ç”±å·¥ç¨‹å¸ˆç»„æˆã€‚

ğŸ’¡
Non-technical subject matter experts also often play a crucial role in the building process.
éæŠ€æœ¯ä¸»é¢˜ä¸“å®¶åœ¨æ„å»ºè¿‡ç¨‹ä¸­ä¹Ÿç»å¸¸æ‰®æ¼”å…³é”®è§’è‰²ã€‚

One key area is prompt engineering, where domain experts often write the best natural language instructions for prompts, since they know how the LLM should behave (more so than the engineers).
ä¸€ä¸ªå…³é”®é¢†åŸŸæ˜¯æç¤ºå·¥ç¨‹ï¼Œé¢†åŸŸä¸“å®¶é€šå¸¸ä¸ºæç¤ºç¼–å†™æœ€ä½³çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå› ä¸ºä»–ä»¬æ¯”å·¥ç¨‹å¸ˆæ›´äº†è§£LLMåº”è¯¥å¦‚ä½•è¡Œä¸ºã€‚

Yet, the value of domain experts goes beyond prompting. They can review the entire â€œcognitive architectureâ€ of the agent, to make sure all logic (whether expressed in language or in code) is correct. Tools like [LangGraph Studio](https://blog.langchain.dev/langgraph-studio-the-first-agent-ide/), which visualize the flow of your agent, make this process easier.
ç„¶è€Œï¼Œé¢†åŸŸä¸“å®¶çš„ä»·å€¼ä¸ä»…é™äºæç¤ºã€‚ä»–ä»¬å¯ä»¥å®¡æŸ¥ä»£ç†çš„æ•´ä¸ªâ€œè®¤çŸ¥æ¶æ„â€ï¼Œä»¥ç¡®ä¿æ‰€æœ‰é€»è¾‘ï¼ˆæ— è®ºæ˜¯ç”¨è¯­è¨€è¿˜æ˜¯ä»£ç è¡¨è¾¾ï¼‰éƒ½æ˜¯æ­£ç¡®çš„ã€‚åƒLangGraph Studioè¿™æ ·çš„å·¥å…·ï¼Œå¯ä»¥å¯è§†åŒ–ä½ çš„ä»£ç†çš„æµç¨‹ï¼Œä½¿è¿™ä¸ªè¿‡ç¨‹æ›´å®¹æ˜“ã€‚

Domain experts can also help debug why an agent is messing up, as agents often mess up because of a failure to communicate - a gap that domain experts are well-equipped to spot.
é¢†åŸŸä¸“å®¶è¿˜å¯ä»¥å¸®åŠ©è°ƒè¯•ä»£ç†ä¸ºä½•å‡ºé”™ï¼Œå› ä¸ºä»£ç†ç»å¸¸å› ä¸ºæ²Ÿé€šå¤±è´¥è€Œå‡ºé”™â€”â€”è¿™æ˜¯é¢†åŸŸä¸“å®¶èƒ½å¤Ÿå¾ˆå¥½åœ°å‘ç°çš„å·®è·ã€‚

### Why we made LangSmith the most user friendly â€œLLM Opsâ€ tool
ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†LangSmithæ‰“é€ æˆæœ€ç”¨æˆ·å‹å¥½çš„â€œLLM Opsâ€å·¥å…·

Since AI engineering requires multiple teams to collaborate to figure out how to best build with LLMs, an â€œLLM Opsâ€ tool like LangSmith also focuses on facilitating that type of collaboration. What most of the triaging flow amounts to is â€“ â€œLook at your data!â€, and we want to make looking at large, mostly text responses very easy in LangSmith.
ç”±äºAIå·¥ç¨‹éœ€è¦å¤šä¸ªå›¢é˜Ÿåˆä½œä»¥æ‰¾å‡ºå¦‚ä½•æœ€å¥½åœ°ä½¿ç”¨LLMæ„å»ºï¼ŒåƒLangSmithè¿™æ ·çš„â€œLLM Opsâ€å·¥å…·ä¹Ÿä¸“æ³¨äºä¿ƒè¿›è¿™ç§ç±»å‹çš„åˆä½œã€‚å¤§å¤šæ•°åˆ†ç±»æµç¨‹çš„å®è´¨æ˜¯â€”â€”â€œæŸ¥çœ‹ä½ çš„æ•°æ®ï¼â€ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨LangSmithä¸­è½»æ¾æŸ¥çœ‹å¤§é‡ä¸»è¦æ˜¯æ–‡æœ¬çš„å“åº”ã€‚

One thing weâ€™ve invested in really heavily from the beginning is a beautiful UI for visualizing agent traces. This beauty serves a purpose - it makes it easier for domain experts of all levels of technical ability to understand what is going on. It communicates so much more clearly what is happening that JSON logs ever would.
ä»ä¸€å¼€å§‹æˆ‘ä»¬å°±æŠ•å…¥å¤§é‡èµ„é‡‘åœ¨ä¸€ä¸ªç¾ä¸½çš„UIä¸Šï¼Œç”¨äºå¯è§†åŒ–ä»£ç†çš„è¸ªè¿¹ã€‚è¿™ç§ç¾ä¸½æ˜¯æœ‰ç›®çš„çš„â€”â€”å®ƒä½¿å„ä¸ªæŠ€æœ¯èƒ½åŠ›æ°´å¹³çš„é¢†åŸŸä¸“å®¶æ›´å®¹æ˜“ç†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚å®ƒæ¯”JSONæ—¥å¿—æ›´æ¸…æ™°åœ°ä¼ è¾¾äº†æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚

ğŸ’¡
The visualization of traces within LangSmith allows everyone - regardless of technical ability - to understand what is happening inside the agent, and contribute to diagnosing any issues.
LangSmithä¸­çš„è¸ªè¿¹å¯è§†åŒ–å…è®¸æ¯ä¸ªäººâ€”â€”æ— è®ºæŠ€æœ¯èƒ½åŠ›å¦‚ä½•â€”â€”éƒ½èƒ½ç†è§£ä»£ç†å†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œå¹¶æœ‰åŠ©äºè¯Šæ–­ä»»ä½•é—®é¢˜ã€‚

LangSmith also facilitates this cross team collaboration in other areas - most notably, the prompt playground- but I like to use tracing as an example because it is so subtle yet so important.
LangSmithè¿˜åœ¨å…¶ä»–é¢†åŸŸä¿ƒè¿›äº†è¿™ç§è·¨å›¢é˜Ÿåˆä½œâ€”â€”æœ€æ˜¾è‘—çš„æ˜¯æç¤ºæ“åœºâ€”â€”ä½†æˆ‘å–œæ¬¢ç”¨è¸ªè¿¹ä½œä¸ºä¾‹å­ï¼Œå› ä¸ºå®ƒå¦‚æ­¤å¾®å¦™å´å¦‚æ­¤é‡è¦ã€‚

### Why people have asked us to expose LangSmith traces to their end users
ä¸ºä»€ä¹ˆäººä»¬è¦æ±‚æˆ‘ä»¬å‘ä»–ä»¬çš„æœ€ç»ˆç”¨æˆ·å…¬å¼€LangSmithè¸ªè¿¹

For the same reasons listed above, we have had multiple companies ask to expose LangSmith traces to their end users. Understanding what the agent is doing isnâ€™t just important for developers - it's also important for end users!
å‡ºäºä¸Šè¿°ç›¸åŒçš„åŸå› ï¼Œæˆ‘ä»¬æœ‰å¤šå®¶å…¬å¸è¦æ±‚å‘ä»–ä»¬çš„æœ€ç»ˆç”¨æˆ·å…¬å¼€LangSmithè¸ªè¿¹ã€‚äº†è§£ä»£ç†åœ¨åšä»€ä¹ˆä¸ä»…å¯¹å¼€å‘äººå‘˜é‡è¦â€”â€”å¯¹æœ€ç»ˆç”¨æˆ·ä¹Ÿå¾ˆé‡è¦ï¼

There are other (more user-friendly ways) to do this than our traces, of course. But it is still flattering to hear this request.
å½“ç„¶ï¼Œè¿˜æœ‰å…¶ä»–ï¼ˆæ›´ç”¨æˆ·å‹å¥½çš„æ–¹å¼ï¼‰æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œè€Œä¸æ˜¯æˆ‘ä»¬çš„è¸ªè¿¹ã€‚ä½†å¬åˆ°è¿™ä¸ªè¯·æ±‚ä»ç„¶ä»¤äººæ„Ÿåˆ°å—å® è‹¥æƒŠã€‚

### Why UI/UX is the most important place to be innovating with AI
ä¸ºä»€ä¹ˆUI/UXæ˜¯AIåˆ›æ–°æœ€é‡è¦çš„åœ°æ–¹

Most of this post has focused on the importance of communication with AI agents when building them, but this also extends to end users. Allowing users to interact with an agent in a transparent, efficient, and reliable way can be crucial for adoption.
è¿™ç¯‡æ–‡ç« çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½é›†ä¸­åœ¨æ„å»ºAIä»£ç†æ—¶ä¸å…¶æ²Ÿé€šçš„é‡è¦æ€§ï¼Œä½†è¿™ä¹Ÿå»¶ä¼¸åˆ°æœ€ç»ˆç”¨æˆ·ã€‚å…è®¸ç”¨æˆ·ä»¥é€æ˜ã€é«˜æ•ˆå’Œå¯é çš„æ–¹å¼ä¸ä»£ç†äº’åŠ¨å¯¹äºé‡‡ç”¨è‡³å…³é‡è¦ã€‚

ğŸ’¡
The power an AI application comes down to how well it facilitates human-AI collaboration, and for that reason we think UI/UX is one of the most important places to be innovating.
AIåº”ç”¨ç¨‹åºçš„åŠ›é‡å½’ç»“äºå®ƒå¦‚ä½•ä¿ƒè¿›äººç±»ä¸AIçš„åˆä½œï¼Œå› æ­¤æˆ‘ä»¬è®¤ä¸ºUI/UXæ˜¯åˆ›æ–°æœ€é‡è¦çš„åœ°æ–¹ä¹‹ä¸€ã€‚

Weâ€™ve talked about different agentic UXs we see emerging (here, here, and here), but itâ€™s still super early on in this space.
æˆ‘ä»¬å·²ç»è®¨è®ºäº†æˆ‘ä»¬çœ‹åˆ°çš„ä¸åŒçš„ä»£ç†UXï¼Œä½†åœ¨è¿™ä¸ªé¢†åŸŸä»ç„¶éå¸¸æ—©æœŸã€‚

Communication is all you need, and so UI/UXs that best facilitate this human-agent interaction patterns will lead to better products.
æ²Ÿé€šæ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡ï¼Œå› æ­¤æœ€èƒ½ä¿ƒè¿›è¿™ç§äººç±»ä¸ä»£ç†äº’åŠ¨æ¨¡å¼çš„UI/UXå°†å¸¦æ¥æ›´å¥½çš„äº§å“ã€‚

### Communication is all you need
æ²Ÿé€šæ˜¯ä½ æ‰€éœ€è¦çš„ä¸€åˆ‡

Communication can mean a lot of things. Itâ€™s an integral part of the human experience. As agents attempt to accomplish more and more humanlike tasks, I strongly believe that good communication skills will make you a better agent developer â€” whether itâ€™s through prompts, code, or UX design.
æ²Ÿé€šå¯ä»¥æ„å‘³ç€å¾ˆå¤šäº‹æƒ…ã€‚å®ƒæ˜¯äººç±»ä½“éªŒçš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚éšç€ä»£ç†å°è¯•å®Œæˆè¶Šæ¥è¶Šå¤šçš„äººç±»ä»»åŠ¡ï¼Œæˆ‘åšä¿¡è‰¯å¥½çš„æ²Ÿé€šæŠ€å·§ä¼šè®©ä½ æˆä¸ºæ›´å¥½çš„ä»£ç†å¼€å‘è€…â€”â€”æ— è®ºæ˜¯é€šè¿‡æç¤ºã€ä»£ç è¿˜æ˜¯UXè®¾è®¡ã€‚

Communication is not just expression in natural language, but it can also involve code to communicate more precisely. The best people to communicate something are the ones who understand it best, and so building these agents will become cross-functional.
æ²Ÿé€šä¸ä»…ä»…æ˜¯è‡ªç„¶è¯­è¨€çš„è¡¨è¾¾ï¼Œå®ƒè¿˜å¯ä»¥æ¶‰åŠä»£ç ä»¥æ›´ç²¾ç¡®åœ°ä¼ è¾¾ã€‚æœ€èƒ½ä¼ è¾¾æŸäº‹çš„äººæ˜¯æœ€äº†è§£å®ƒçš„äººï¼Œå› æ­¤æ„å»ºè¿™äº›ä»£ç†å°†å˜å¾—è·¨èŒèƒ½ã€‚

And Iâ€™ll close with a tip from George Bernard Shaw â€œThe single biggest problem in communication is the illusion that it has taken place.â€ If we want a future in which LLM applications are solving real problems, we need to figure out how to communicate with them better.
æœ€åï¼Œæˆ‘å¼•ç”¨ä¹”æ²»Â·ä¼¯çº³å¾·Â·è‚–çš„ä¸€å¥è¯ç»“æŸï¼šâ€œæ²Ÿé€šä¸­æœ€å¤§çš„ä¸€ä¸ªé—®é¢˜æ˜¯å®ƒå·²ç»å‘ç”Ÿçš„é”™è§‰ã€‚â€å¦‚æœæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªLLMåº”ç”¨ç¨‹åºè§£å†³å®é™…é—®é¢˜çš„æœªæ¥ï¼Œæˆ‘ä»¬éœ€è¦å¼„æ¸…æ¥šå¦‚ä½•æ›´å¥½åœ°ä¸å®ƒä»¬æ²Ÿé€šã€‚
