---
layout: post
title:  具身智能大模型简介
date:   2025-05-11 10:00:00 +0800
categories: 具身智能 VLA
tags: [具身智能, VLA, 机器人操作, 数据集收集, RT, RDT, GR, PI0]
---

- [具身智能大模型简介](https://www.bilibili.com/video/BV1QxB9YuERU/)

## An introduction to robot manipulation

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/04.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/05.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/06.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/07.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/08.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/09.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/10.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/11.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/12.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/13.jpg)

![](/images/2025/EmbodiedAIFoundationModel/robot-manipulation/14.jpg)


## Leveraging vision-language-action models for robot manipulation

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/Outline2.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-01.png)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/Large-scaleDataCollection-01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/Large-scaleDataCollection-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/Large-scaleDataCollection-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/Large-scaleDataCollection-04.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/Large-scaleDataCollection-05.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/Large-scaleDataCollection-06.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-Architecture.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-1-01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-1-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-1-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-1-04.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-2-01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-2-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-2-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-2-04.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-2-05.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-X-01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-X-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-X-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-H-01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-H-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RT-H-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-OpenVLA-01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-OpenVLA-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-OpenVLA-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-OpenVLA-04.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-OpenVLA-05.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-OpenVLA-06.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/ImprovementsActionOutput-01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/ImprovementsActionOutput-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/ImprovementsActionOutput-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/ImprovementsActionOutput-04.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/ImprovementsActionOutput-05.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/ImprovementsActionOutput-06.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/ImprovementsActionOutput-07.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-PI_0-01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-PI_0-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-PI_0-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-PI_0-04.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-PI_0-05.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-PI_0-06.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-PI_0-07.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RDT-01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RDT-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RDT-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RDT-04.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-RDT-05.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-GR-2-01.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-GR-2-02.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-GR-2-03.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-GR-2-04.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-GR-2-05.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-GR-2-06.jpg)

![](/images/2025/EmbodiedAIFoundationModel/VisionLanguageActionModels/VLA-GR-2-07.jpg)


## Takeaways, limitations, and future work

![](/images/2025/EmbodiedAIFoundationModel/takeaways-limitations-future_work/Outline3.jpg)

![](/images/2025/EmbodiedAIFoundationModel/takeaways-limitations-future_work/Takeaways.jpg)

![](/images/2025/EmbodiedAIFoundationModel/takeaways-limitations-future_work/Limitations.jpg)

![](/images/2025/EmbodiedAIFoundationModel/takeaways-limitations-future_work/FutureWork.jpg)

![](/images/2025/EmbodiedAIFoundationModel/takeaways-limitations-future_work/References.jpg)
