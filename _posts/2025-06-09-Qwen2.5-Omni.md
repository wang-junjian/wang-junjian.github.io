---
layout: single
title:  "Qwen2.5-Omniï¼šç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§æ¨¡å‹"
date:   2025-06-09 10:00:00 +0800
categories: Qwen2.5-Omni å¤šæ¨¡æ€
tags: [Qwen2.5-Omni, å¤šæ¨¡æ€, Qwen, LLM, è¯­éŸ³è¯†åˆ«, è¯­éŸ³ç”Ÿæˆ, macOS]
---

**Qwen2.5-Omni**æ˜¯Qwenç³»åˆ—ä¸­å…¨æ–°çš„æ——èˆ°çº§ç«¯åˆ°ç«¯**å¤šæ¨¡æ€å¤§æ¨¡å‹**ï¼Œä¸“ä¸ºå…¨é¢çš„å¤šæ¨¡å¼æ„ŸçŸ¥è®¾è®¡ï¼Œæ— ç¼å¤„ç†åŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘åœ¨å†…çš„å„ç§è¾“å…¥ï¼ŒåŒæ—¶æ”¯æŒæµå¼çš„æ–‡æœ¬ç”Ÿæˆå’Œè‡ªç„¶è¯­éŸ³åˆæˆè¾“å‡ºã€‚

<!--more-->

ç‚¹å‡»ä¸‹æ–¹è§†é¢‘äº†è§£æ›´å¤šä¿¡æ¯å§ ğŸ˜ƒ

<a href="https://youtu.be/UF55yM67EH0" target="_blank">
  <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/video_cover.png" alt="Open Video"/>
</a>


## æ¦‚è§ˆ
### ç®€ä»‹
Qwen 2.5-Omniæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æ„ŸçŸ¥åŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘åœ¨å†…çš„å¤šç§æ¨¡æ€ï¼ŒåŒæ—¶ä»¥æµå¼çš„æ–¹å¼ç”Ÿæˆæ–‡æœ¬å’Œè‡ªç„¶è¯­éŸ³å“åº”ã€‚

![](/images/2025/Qwen2.5-Omni/Architecture.png)


### ä¸»è¦ç‰¹ç‚¹

* **å…¨èƒ½åˆ›æ–°æ¶æ„**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„Thinker-Talkeræ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨æ”¯æŒæ–‡æœ¬/å›¾åƒ/éŸ³é¢‘/è§†é¢‘çš„è·¨æ¨¡æ€ç†è§£ï¼ŒåŒæ—¶ä»¥æµå¼æ–¹å¼ç”Ÿæˆæ–‡æœ¬å’Œè‡ªç„¶è¯­éŸ³å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä½ç½®ç¼–ç æŠ€æœ¯ï¼Œç§°ä¸ºTMRoPEï¼ˆTime-aligned Multimodal RoPEï¼‰ï¼Œé€šè¿‡æ—¶é—´è½´å¯¹é½å®ç°è§†é¢‘ä¸éŸ³é¢‘è¾“å…¥çš„ç²¾å‡†åŒæ­¥ã€‚

* **å®æ—¶éŸ³è§†é¢‘äº¤äº’**ï¼šæ¶æ„æ—¨åœ¨æ”¯æŒå®Œå…¨å®æ—¶äº¤äº’ï¼Œæ”¯æŒåˆ†å—è¾“å…¥å’Œå³æ—¶è¾“å‡ºã€‚

* **è‡ªç„¶æµç•…çš„è¯­éŸ³ç”Ÿæˆ**ï¼šåœ¨è¯­éŸ³ç”Ÿæˆçš„è‡ªç„¶æ€§å’Œç¨³å®šæ€§æ–¹é¢è¶…è¶Šäº†è®¸å¤šç°æœ‰çš„æµå¼å’Œéæµå¼æ›¿ä»£æ–¹æ¡ˆã€‚

* **å…¨æ¨¡æ€æ€§èƒ½ä¼˜åŠ¿**ï¼šåœ¨åŒç­‰è§„æ¨¡çš„å•æ¨¡æ€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚Qwen2.5-Omniåœ¨éŸ³é¢‘èƒ½åŠ›ä¸Šä¼˜äºç±»ä¼¼å¤§å°çš„Qwen2-Audioï¼Œå¹¶ä¸Qwen2.5-VL-7Bä¿æŒåŒç­‰æ°´å¹³ã€‚

* **å“è¶Šçš„ç«¯åˆ°ç«¯è¯­éŸ³æŒ‡ä»¤è·Ÿéšèƒ½åŠ›**ï¼šQwen2.5-Omniåœ¨ç«¯åˆ°ç«¯è¯­éŸ³æŒ‡ä»¤è·Ÿéšæ–¹é¢è¡¨ç°å‡ºä¸æ–‡æœ¬è¾“å…¥å¤„ç†ç›¸åª²ç¾çš„æ•ˆæœï¼Œåœ¨MMLUé€šç”¨çŸ¥è¯†ç†è§£å’ŒGSM8Kæ•°å­¦æ¨ç†ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚


## ä¸‹è½½æ¨¡å‹
### Hugging Face

```bash
git clone https://huggingface.co/Qwen/Qwen2.5-Omni-3B
```


## å®‰è£…ç¯å¢ƒ

```bash
pip install torch torchvision torchaudio
pip install transformers
pip install accelerate
pip install "qwen-omni-utils"
pip install boto3 botocore
```

## ä½¿ç”¨ç¤ºä¾‹
### è¯­éŸ³è¾“å…¥ -> è¯­éŸ³è¾“å‡º

ç¼–è¾‘æ–‡ä»¶ï¼š`demo.py`

```python
import torch
from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor

model_path = "/Users/junjian/HuggingFace/Qwen/Qwen2.5-Omni-3B"
model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    # attn_implementation="flash_attention_2", # macOSä¸æ”¯æŒflash_attention_2
)
processor = Qwen2_5OmniProcessor.from_pretrained(model_path)

from qwen_omni_utils import process_mm_info

# @title inference function
def inference(audio_path):
    messages = [
        {"role": "system", "content": [
                {"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}
            ],},
        {"role": "user", "content": [
                {"type": "audio", "audio": audio_path},
            ]
        },
    ]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)
    inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", padding=True, use_audio_in_video=True)
    inputs = inputs.to(model.device).to(model.dtype)

    output = model.generate(**inputs, use_audio_in_video=True, return_audio=True)

    text = processor.batch_decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)
    audio = output[1]
    return text, audio


import librosa

from io import BytesIO
from urllib.request import urlopen

audio_path = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav"
audio = librosa.load(BytesIO(urlopen(audio_path).read()), sr=16000)[0]

# audio_path = "guess_age_gender.wav"
# audio = librosa.load(audio_path, sr=16000)[0]

# display(Audio(audio, rate=16000))

## Use a local HuggingFace model to inference.
response = inference(audio_path)
print(response[0][0])
# display(Audio(response[1], rate=24000))
```

`è¾“å‡º`ï¼š

```bash
system
You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.
user

assistant
Well, it's really hard to guess someone's age and gender just from their voice. There are so many factors that can affect how a person sounds, like their accent, the way they speak, and even their mood. But, you know, it's not impossible. Some people might be able to tell based on a lot of experience. But it's not something that can be done with 100% accuracy. So, what do you think? Do you have any other questions about this?
```

**ğŸ“Œ åœ¨æˆ‘çš„ MacBook Pro M2 Maxä¸Šè¿è¡Œï¼ŒèŠ±äº†å¥½å‡ åˆ†é’Ÿæ‰å®Œæˆæ¨ç†ã€‚ğŸ¢ğŸ¢ğŸ¢**


## å‚è€ƒèµ„æ–™
- [Qwen2.5-Omniï¼ˆä¸­æ–‡ï¼‰](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/README_CN.md)
- [Voice Chatting with Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/voice_chatting.ipynb)
