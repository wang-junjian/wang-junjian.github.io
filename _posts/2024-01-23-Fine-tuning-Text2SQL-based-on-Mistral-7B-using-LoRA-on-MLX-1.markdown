---
layout: post
title:  "åœ¨ MLX ä¸Šä½¿ç”¨ LoRA åŸºäº Mistral-7B å¾®è°ƒ Text2SQLï¼ˆä¸€ï¼‰"
date:   2024-01-23 08:00:00 +0800
categories: MLX LoRA
tags: [MLX, LoRA, Mistral-7B, Text2SQL, WikiSQL, MacBookProM2Max]
---

## å®‰è£…

```bash
git clone https://github.com/ml-explore/mlx-examples.git
cd mlx-examples/lora

pip install -r requirements.txt
```


## ä¸‹è½½æ¨¡å‹

- [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)

```bash
pip install huggingface_hub hf_transfer

export HF_HUB_ENABLE_HF_TRANSFER=1
huggingface-cli download \
    --local-dir-use-symlinks False \
    --local-dir mistralai/Mistral-7B-v0.1 \
    mistralai/Mistral-7B-v0.1
```
- [huggingface_hub Environment variables](https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables)


## æ•°æ®é›† WikiSQL

- [WikiSQL](https://github.com/salesforce/WikiSQL)
- [sqllama/sqllama-V0](https://huggingface.co/sqllama/sqllama-V0/blob/main/wikisql.ipynb)

### æ ·æœ¬æ ¼å¼

```json
{"text": "table: <table_name>
columns: <column_name1>, <column_name2>, <column_name3>
Q: <question>
A: SELECT <column_name2> FROM <table_name> WHERE <>"}
```

### æ ·æœ¬ç¤ºä¾‹

```json
{"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: What is the current series where the new series began in June 2011?\nA: SELECT Current series FROM 1-1000181-1 WHERE Notes = 'New series began in June 2011'"}
{"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: What is the format for South Australia?\nA: SELECT Format FROM 1-1000181-1 WHERE State/territory = 'South Australia'"}
{"text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: Name the background colour for the Australian Capital Territory\nA: SELECT Text/background colour FROM 1-1000181-1 WHERE State/territory = 'Australian Capital Territory'"}
```


## å¾®è°ƒ

### åŸºäºæ¨¡å‹ mistralai/Mistral-7B-v0.1 è¿›è¡Œ LoRA å¾®è°ƒ

åœ¨ MacBook Pro M2 Max ä¸Š 600 æ¬¡è¿­ä»£ï¼Œè€—æ—¶ 20 åˆ†é’Ÿï¼Œå ç”¨å†…å­˜ 47Gã€‚

```bash
python lora.py --model mistralai/Mistral-7B-v0.1 \
               --train \
               --iters 600
```
```
Loading pretrained model
Total parameters 7243.436M
Trainable parameters 1.704M
Loading datasets
Training
Iter 1: Val loss 2.376, Val took 27.120s
Iter 10: Train loss 2.276, It/sec 0.431, Tokens/sec 171.338
Iter 20: Train loss 1.758, It/sec 0.545, Tokens/sec 218.476
Iter 30: Train loss 1.543, It/sec 0.495, Tokens/sec 201.851
Iter 40: Train loss 1.380, It/sec 0.570, Tokens/sec 218.728
Iter 50: Train loss 1.248, It/sec 0.556, Tokens/sec 218.990
Iter 60: Train loss 1.133, It/sec 0.540, Tokens/sec 211.490
Iter 70: Train loss 1.173, It/sec 0.480, Tokens/sec 192.765
Iter 80: Train loss 1.179, It/sec 0.508, Tokens/sec 201.774
Iter 90: Train loss 1.149, It/sec 0.569, Tokens/sec 213.454
Iter 100: Train loss 1.208, It/sec 0.506, Tokens/sec 190.590
Iter 100: Saved adapter weights to adapters.npz.
Iter 110: Train loss 1.086, It/sec 0.606, Tokens/sec 231.982
Iter 120: Train loss 1.076, It/sec 0.527, Tokens/sec 203.822
Iter 130: Train loss 1.086, It/sec 0.548, Tokens/sec 214.598
Iter 140: Train loss 1.076, It/sec 0.526, Tokens/sec 202.990
Iter 150: Train loss 1.061, It/sec 0.482, Tokens/sec 194.181
Iter 160: Train loss 0.999, It/sec 0.520, Tokens/sec 202.290
Iter 170: Train loss 1.023, It/sec 0.515, Tokens/sec 210.128
Iter 180: Train loss 0.983, It/sec 0.577, Tokens/sec 207.285
Iter 190: Train loss 1.029, It/sec 0.545, Tokens/sec 216.047
Iter 200: Train loss 1.096, It/sec 0.540, Tokens/sec 203.309
Iter 200: Val loss 1.115, Val took 24.693s
Iter 200: Saved adapter weights to adapters.npz.
Iter 210: Train loss 1.066, It/sec 0.550, Tokens/sec 207.058
Iter 220: Train loss 0.993, It/sec 0.655, Tokens/sec 258.400
Iter 230: Train loss 0.992, It/sec 0.626, Tokens/sec 241.065
Iter 240: Train loss 0.933, It/sec 0.625, Tokens/sec 237.237
Iter 250: Train loss 0.987, It/sec 0.549, Tokens/sec 225.348
Iter 260: Train loss 0.877, It/sec 0.569, Tokens/sec 227.739
Iter 270: Train loss 0.843, It/sec 0.593, Tokens/sec 229.485
Iter 280: Train loss 0.881, It/sec 0.644, Tokens/sec 245.001
Iter 290: Train loss 0.950, It/sec 0.629, Tokens/sec 238.544
Iter 300: Train loss 0.819, It/sec 0.600, Tokens/sec 242.306
Iter 300: Saved adapter weights to adapters.npz.
Iter 310: Train loss 0.947, It/sec 0.646, Tokens/sec 239.077
Iter 320: Train loss 0.811, It/sec 0.568, Tokens/sec 218.934
Iter 330: Train loss 0.891, It/sec 0.562, Tokens/sec 224.256
Iter 340: Train loss 0.837, It/sec 0.590, Tokens/sec 224.871
Iter 350: Train loss 0.851, It/sec 0.628, Tokens/sec 238.298
Iter 360: Train loss 0.878, It/sec 0.597, Tokens/sec 236.297
Iter 370: Train loss 0.833, It/sec 0.550, Tokens/sec 222.876
Iter 380: Train loss 0.857, It/sec 0.593, Tokens/sec 225.336
Iter 390: Train loss 0.884, It/sec 0.622, Tokens/sec 238.801
Iter 400: Train loss 0.838, It/sec 0.534, Tokens/sec 224.068
Iter 400: Val loss 1.086, Val took 24.089s
Iter 400: Saved adapter weights to adapters.npz.
Iter 410: Train loss 0.825, It/sec 0.578, Tokens/sec 230.336
Iter 420: Train loss 0.827, It/sec 0.635, Tokens/sec 232.469
Iter 430: Train loss 0.797, It/sec 0.634, Tokens/sec 235.050
Iter 440: Train loss 0.853, It/sec 0.586, Tokens/sec 228.807
Iter 450: Train loss 0.804, It/sec 0.573, Tokens/sec 230.769
Iter 460: Train loss 0.850, It/sec 0.564, Tokens/sec 227.966
Iter 470: Train loss 0.774, It/sec 0.619, Tokens/sec 241.596
Iter 480: Train loss 0.810, It/sec 0.605, Tokens/sec 234.975
Iter 490: Train loss 0.764, It/sec 0.642, Tokens/sec 245.548
Iter 500: Train loss 0.805, It/sec 0.584, Tokens/sec 238.082
Iter 500: Saved adapter weights to adapters.npz.
Iter 510: Train loss 0.809, It/sec 0.572, Tokens/sec 229.507
Iter 520: Train loss 0.703, It/sec 0.593, Tokens/sec 231.239
Iter 530: Train loss 0.635, It/sec 0.566, Tokens/sec 226.033
Iter 540: Train loss 0.690, It/sec 0.635, Tokens/sec 240.545
Iter 550: Train loss 0.705, It/sec 0.650, Tokens/sec 253.223
Iter 560: Train loss 0.697, It/sec 0.590, Tokens/sec 232.188
Iter 570: Train loss 0.617, It/sec 0.618, Tokens/sec 236.465
Iter 580: Train loss 0.636, It/sec 0.621, Tokens/sec 239.133
Iter 590: Train loss 0.634, It/sec 0.591, Tokens/sec 230.575
Iter 600: Train loss 0.612, It/sec 0.589, Tokens/sec 239.730
Iter 600: Val loss 0.981, Val took 24.225s
Iter 600: Saved adapter weights to adapters.npz.
```

### è¯„ä¼°

è®¡ç®—æµ‹è¯•é›†å›°æƒ‘åº¦ï¼ˆPPLï¼‰å’Œäº¤å‰ç†µæŸå¤±ï¼ˆLossï¼‰ã€‚

```bash
python lora.py --model mistralai/Mistral-7B-v0.1 \
               --adapter-file adapters.npz \
               --test
```
```
Loading pretrained model
Total parameters 7243.436M
Trainable parameters 1.704M
Loading datasets
Testing
Test loss 1.341, Test ppl 3.822.
```

### ç”Ÿæˆ

æŸ¥çœ‹å¾®è°ƒåçš„ç”Ÿæˆæ•ˆæœï¼Œä½¿ç”¨å†…å­˜ 18Gã€‚

ğŸ“Œ æ²¡æœ‰ä½¿ç”¨æ¨¡å‹çš„æ ‡æ³¨æ ¼å¼ç”Ÿæˆæ•°æ®é›†ï¼Œå¯¼è‡´ä¸èƒ½ç»“æŸï¼Œç›´åˆ°ç”Ÿæˆæœ€å¤§çš„ Tokens æ•°é‡ã€‚

```bash
python lora.py --model mistralai/Mistral-7B-v0.1 \
               --adapter-file adapters.npz \
               --max-tokens 50 \
               --prompt "table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "
```
```
table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: SELECT Nationality FROM 1-10015132-16 WHERE Player = 'Terrence Ross'",
background: yellowtcx: SELECT Years in Toronto FROM 1-100151
```

```bash
python lora.py --model mistralai/Mistral-7B-v0.1 \
               --adapter-file adapters.npz \
               --max-tokens 50 \
               --prompt "table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: ç‰¹ä¼¦æ–¯Â·ç½—æ–¯çš„å›½ç±æ˜¯ä»€ä¹ˆ
A: "
```
```
table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: ç‰¹ä¼¦æ–¯Â·ç½—æ–¯çš„å›½ç±æ˜¯ä»€ä¹ˆ
A: SELECT Position FROM 1-10015132-16 WHERE Player = 'Specialst Ros'dSELECT Position FROM 1-10015132-16 WHERERE Player
```

```bash
â€‹python lora.py --model mistralai/Mistral-7B-v0.1 \
               --adapter-file adapters.npz \
               --max-tokens 50 \
               --prompt "è¡¨ï¼š1-10015132-16
åˆ—ï¼šçƒå‘˜ã€å·ç ã€å›½ç±ã€ä½ç½®ã€åœ¨å¤šä¼¦å¤šçš„å²æœˆã€å­¦æ ¡/ä¿±ä¹éƒ¨çƒé˜Ÿ
é—®ï¼šç‰¹ä¼¦æ–¯Â·ç½—æ–¯çš„å›½ç±æ˜¯ä»€ä¹ˆ
ç­”ï¼š"
```
```
è¡¨ï¼š1-10015132-16
åˆ—ï¼šçƒå‘˜ã€å·ç ã€å›½ç±ã€ä½ç½®ã€åœ¨å¤šä¼¦å¤šçš„å²æœˆã€å­¦æ ¡/ä¿±ä¹éƒ¨çƒé˜Ÿ
é—®ï¼šç‰¹ä¼¦æ–¯Â·ç½—æ–¯çš„å›½ç±æ˜¯ä»€ä¹ˆ
ç­”ï¼šSELECTå›½ï¿½ï¿½ FROM 1-10015132-16 WHERE å·ç  = '11' AND å§“å = 'ç‰¹ï¿½ï¿½æ–¯Â·ç½—æ–¯'q
SELECTå›½ï¿½ï¿½
```


## ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆ SQL

### ä½¿ç”¨ mlx-lm ç”Ÿæˆ

å®‰è£… mlx-lm

```bash
pip install mlx-lm
```

```bash
python -m mlx_lm.generate --model mistralai/Mistral-7B-v0.1 \
                          --max-tokens 50 \
                          --prompt "table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "
```
```
Prompt: table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: 
1-10015132-16

table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Cl
```

### ä½¿ç”¨ 0 å‚æ•°çš„ adapters.npz ç”Ÿæˆ

è½¬æ¢ 0 å‚æ•° adapters.npz çš„è„šæœ¬

```py
import numpy as np

# åŠ è½½npzæ–‡ä»¶
data = np.load('adapters.npz')

# åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ–°çš„npyæ–‡ä»¶
new_data = {}

# éå†npzæ–‡ä»¶ä¸­çš„æ¯ä¸ªnpyæ–‡ä»¶
for key in data.files:
    # å°†æ¯ä¸ªnpyæ–‡ä»¶çš„å€¼è®¾ç½®ä¸º0
    new_data[key] = np.zeros_like(data[key])

# ä¿å­˜æ–°çš„npzæ–‡ä»¶
np.savez('zero_adapters.npz', **new_data)
```

```bash
python lora.py --model mistralai/Mistral-7B-v0.1 \
               --adapter-file zero_adapters.npz \
               --max-tokens 50 \
               --prompt "table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "
```
```
table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: 1-10015132-16

table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Cl
```


## èåˆï¼ˆFuseï¼‰

```bash
python fuse.py --model mistralai/Mistral-7B-v0.1 \
               --adapter-file adapters.npz \
               --save-path lora_fused_model
```

èåˆåçš„æ¨¡å‹æ–‡ä»¶ç»“æ„å¦‚ä¸‹ï¼š

```
lora_fused_model
â”œâ”€â”€ config.json
â”œâ”€â”€ special_tokens_map.json
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer.model
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ weights.00.safetensors
```

### ç”Ÿæˆ

```bash
python -m mlx_lm.generate --model lora_fused_model \
                          --max-tokens 50 \
                          --prompt "table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "
```
```
Prompt: table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: 
SELECT Nationality FROM 1-10015132-16 WHERE Player = 'Terrence Ross'ï¿½ï¿½ FUNCTION school/club team
Q: How many years was terrence ross in school/
```


## å‚è€ƒèµ„æ–™
- [MLX Community](https://huggingface.co/mlx-community)
- [Fine-Tuning with LoRA or QLoRA](https://github.com/ml-explore/mlx-examples/tree/main/lora)
- [Generate Text with LLMs and MLX](https://github.com/ml-explore/mlx-examples/tree/main/llms)
- [Awesome Text2SQL](https://github.com/eosphoros-ai/Awesome-Text2SQL)
- [Awesome Text2SQLï¼ˆä¸­æ–‡ï¼‰](https://github.com/eosphoros-ai/Awesome-Text2SQL/blob/main/README.zh.md)
- [Mistral AI](https://huggingface.co/mistralai)
- [A Beginnerâ€™s Guide to Fine-Tuning Mistral 7B Instruct Model](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe)
- [Mistral Instruct 7B Finetuning on MedMCQA Dataset](https://saankhya.medium.com/mistral-instruct-7b-finetuning-on-medmcqa-dataset-6ec2532b1ff1)
- [Fine-tuning Mistral on your own data](https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb)
- [mlx-examples llms Mistral](https://github.com/ml-explore/mlx-examples/blob/main/llms/mistral/README.md)
