---
layout: post
title:  "Transformers Pipeline"
date:   2023-11-09 08:00:00 +0800
categories: Inference
tags: [Inference, Transformers, Pipeline, LLM.int8]
---

## ä½¿ç”¨ Transformers çš„ Pipeline è¿›è¡Œæ¨ç†
å®‰è£…ä¾èµ–åŒ…

```shell
pip install datasets evaluate transformers[sentencepiece]
```

### è‹±æ–‡æƒ…æ„Ÿåˆ†ç±»
```py
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

```shell
No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in production is not recommended.
[{'label': 'POSITIVE', 'score': 0.9598048329353333},
 {'label': 'NEGATIVE', 'score': 0.9994558691978455}]
```

### ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±»

æ¨¡å‹ï¼š[uer/roberta-base-finetuned-dianping-chinese](https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese)

```shell
from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline

checkpoint = "uer/roberta-base-finetuned-dianping-chinese"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
text_classification = pipeline('sentiment-analysis', tokenizer=tokenizer, model=model)

raw_inputs = [
    "æˆ‘ä¸€ç›´éƒ½åœ¨ç­‰å¾… HuggingFace è¯¾ç¨‹ã€‚",
    "æˆ‘éå¸¸è®¨åŒè¿™ä¸ªï¼",
]
for _ in range(500):
    text_classification(raw_inputs)
```

```shell
[{'label': 'positive (stars 4 and 5)', 'score': 0.8151589035987854},
 {'label': 'negative (stars 1, 2 and 3)', 'score': 0.9962126016616821}]
```

## Pipeline å†…éƒ¨åˆ†è§£

Pipeline å°†é¢„å¤„ç†ã€æ¨ç†å’Œåå¤„ç†ä¸‰éƒ¨åˆ†ç»„åˆåœ¨ä¸€èµ·

### â¶ é¢„å¤„ç†
<br>

```py
from transformers import AutoTokenizer

checkpoint = "uer/roberta-base-finetuned-dianping-chinese"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "æˆ‘ä¸€ç›´éƒ½åœ¨ç­‰å¾… HuggingFace è¯¾ç¨‹ã€‚",
    "æˆ‘éå¸¸è®¨åŒè¿™ä¸ªï¼",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

`ç»“æœï¼š`
```
{
    'input_ids': tensor([
        [  101,  2769,   671,  4684,  6963,  1762,  5023,  2521, 12199,  9949,  8221, 12122,  6440,  4923,   511,   102],
        [  101,  2769,  7478,  2382,  6374,  1328,  6821,   702,  8013,   102,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]
    ])
}
```

<!--_footer: '[uer/roberta-base-finetuned-dianping-chinese](https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese)'-->

---

### â· æ¨ç†

```py
from transformers import AutoModelForSequenceClassification

checkpoint = "uer/roberta-base-finetuned-dianping-chinese"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

outputs = model(**inputs)
print(outputs.logits)
```

`ç»“æœï¼š`
```
tensor([[-0.7019,  0.6344],
        [ 2.8446, -2.7260]], grad_fn=<AddmmBackward0>)
```

`logits` ä¸æ˜¯æ¦‚ç‡ï¼Œæ˜¯æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„åŸå§‹éæ ‡å‡†åŒ–åˆ†æ•°ã€‚

<!--_footer: '[uer/roberta-base-finetuned-dianping-chinese](https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese)'-->

---

### â¸ åå¤„ç†

```py
labels = model.config.id2label
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)

for input, prediction in zip(raw_inputs, predictions):
    score = torch.max(prediction).item()
    label = labels[torch.argmax(prediction).item()]
    print(f"input: {input}")
    print(f"label: {label.upper()}, score: {score}")
    print()
```

`ç»“æœï¼š`
```
input: æˆ‘ä¸€ç›´éƒ½åœ¨ç­‰å¾… HuggingFace è¯¾ç¨‹ã€‚
label: POSITIVE (STARS 4 AND 5), score: 0.7918757796287537

input: æˆ‘éå¸¸è®¨åŒè¿™ä¸ªï¼
label: NEGATIVE (STARS 1, 2 AND 3), score: 0.9962064027786255
```


## LLM.int8 èŠ‚çœæ˜¾å­˜ & åŠ é€Ÿæ¨ç†
å®‰è£…ä¾èµ–åŒ…

```shell
pip install bitsandbytes accelerate scipy
```

```py
from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline

checkpoint = "uer/roberta-base-finetuned-dianping-chinese"

tokenizer = AutoTokenizer.from_pretrained(checkpoint, device_map='auto', load_in_8bit=True)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, load_in_8bit=True)
text_classification = pipeline('sentiment-analysis', tokenizer=tokenizer, model=model)

raw_inputs = [
    "æˆ‘ä¸€ç›´éƒ½åœ¨ç­‰å¾… HuggingFace è¯¾ç¨‹ã€‚",
    "æˆ‘éå¸¸è®¨åŒè¿™ä¸ªï¼",
]
outputs = text_classification(raw_inputs)
print(outputs)
```

```shell
[{'label': 'positive (stars 4 and 5)', 'score': 0.8151589035987854}, 
 {'label': 'negative (stars 1, 2 and 3)', 'score': 0.9962126016616821}]
```


## [Colab](https://colab.research.google.com/) ä¸­æµ‹é‡æ‰§è¡Œæ—¶é—´

å®‰è£…æ’ä»¶

```shell
pip install ipython-autotime
```

åœ¨ `ä»£ç å•å…ƒæ ¼` ä¸­å¢åŠ ä»£ç 

```py
%load_ext autotime
```

* [How to check preprocessing time/speed in Colab?](https://stackoverflow.com/questions/63193743/how-to-check-preprocessing-time-speed-in-colab)

### ä¸ä½¿ç”¨ LLM.int8

```py
%load_ext autotime
from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline

checkpoint = "uer/roberta-base-finetuned-dianping-chinese"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
text_classification = pipeline('sentiment-analysis', tokenizer=tokenizer, model=model)

raw_inputs = [
    "æˆ‘ä¸€ç›´éƒ½åœ¨ç­‰å¾… HuggingFace è¯¾ç¨‹ã€‚",
    "æˆ‘éå¸¸è®¨åŒè¿™ä¸ªï¼",
]
for _ in range(500):
    outputs = text_classification(raw_inputs)

print('æ˜¾å­˜ï¼š', model.get_memory_footprint())
```
```
æ˜¾å­˜ï¼š 204546564
time: 2min 21s
```

### ä½¿ç”¨ LLM.int8

```py
%load_ext autotime
from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline

checkpoint = "uer/roberta-base-finetuned-dianping-chinese"

tokenizer = AutoTokenizer.from_pretrained(checkpoint, device_map='auto', load_in_8bit=True)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, load_in_8bit=True)
text_classification = pipeline('sentiment-analysis', tokenizer=tokenizer, model=model)

raw_inputs = [
    "æˆ‘ä¸€ç›´éƒ½åœ¨ç­‰å¾… HuggingFace è¯¾ç¨‹ã€‚",
    "æˆ‘éå¸¸è®¨åŒè¿™ä¸ªï¼",
]
for _ in range(500):
    text_classification(raw_inputs)

print('æ˜¾å­˜ï¼š', model.get_memory_footprint())
```
```
æ˜¾å­˜ï¼š 119022084
time: 1min 46s
```

### æ€»ç»“
* ä¸ä½¿ç”¨ `LLM.int8`ï¼š**æ˜¾å­˜ï¼š** 205MBï¼›**ç”¨æ—¶ï¼š** 141ç§’
* ä½¿ç”¨ `LLM.int8`ï¼š**æ˜¾å­˜ï¼š** 119MBï¼›**ç”¨æ—¶ï¼š** 106ç§’

**æ˜¾å­˜å‡å°‘ğŸ“‰ 72%ï¼Œé€Ÿåº¦æå‡ğŸ“ˆ 33%**
