---
layout: single
title:  "ä½¿ç”¨ FastChat åœ¨ CUDA ä¸Šéƒ¨ç½² LLM"
date:   2024-01-16 08:00:00 +0800
categories: FastChat vLLM
tags: [FastChat, vLLM, CUDA]
---

## å®‰è£… FastChat & vLLM
### å®‰è£… [FastChat](https://github.com/lm-sys/FastChat)
- [FastChat éƒ¨ç½²å¤šæ¨¡å‹]({% post_url 2023-10-24-fastchat-deploys-multi-model %})
- [Qwen (é€šä¹‰åƒé—®)]({% post_url 2023-12-25-Qwen %})
- [åœ¨ MacBook Pro M2 Max ä¸Šå®‰è£… FastChat]({% post_url 2024-01-11-Install-FastChat-on-MacBook-Pro-M2-Max %})

```shell
pip install "fschat[model_worker,webui]"
```

### å®‰è£… [FlashAttention](https://github.com/Dao-AILab/flash-attention)
- `Turing GPU T4` ä¸æ”¯æŒ FlashAttention 2ï¼Œéœ€è¦ä½¿ç”¨ FlashAttention 1.x ã€‚
- `Turing GPU T4` ä¸æ”¯æŒ `bf16`ï¼Œéœ€è¦ä½¿ç”¨ `fp16` ã€‚

### å®‰è£… [vLLM](https://github.com/vllm-project/vllm)
```shell
pip install vllm -i https://mirrors.aliyun.com/pypi/simple/
```

## å‡çº§ FastChat & vLLM
```shell
git pull
pip install -e ".[model_worker,webui]"
pip install -U vllm
```


## éƒ¨ç½² LLM
### è¿è¡Œ Controller
```shell
python -m fastchat.serve.controller
```

### è¿è¡Œ OpenAI API Server
```shell
python -m fastchat.serve.openai_api_server
```

### è¿è¡Œ Model Worker
#### Qwen-1_8B-Chat
```shell
export CUDA_VISIBLE_DEVICES=0
python -m fastchat.serve.model_worker \
  --model-path Qwen/Qwen-1_8B-Chat \
  --model-names gpt-3.5-turbo \
  --port 21002 \
  --worker-address http://localhost:21002
```
- `--port 21002` å’Œ `--worker-address http://localhost:21002` ç”¨äºæŒ‡å®šç«¯å£å’Œåœ°å€ï¼Œä»¥ä¾¿ Controller èƒ½å¤Ÿæ‰¾åˆ° Model Workerã€‚éœ€è¦åŒæ—¶è®¾ç½®ã€‚

#### Qwen-7B-Chatï¼ˆå¤šå¡ï¼‰
```shell
export CUDA_VISIBLE_DEVICES=0,1
python -m fastchat.serve.model_worker \
  --model-path Qwen/Qwen-7B-Chat \
  --model-names gpt-3.5-turbo \
  --num-gpus 2
```
- --num-gpus 2 ç”¨äºæŒ‡å®šä½¿ç”¨çš„ GPU æ•°é‡ã€‚

#### Qwen-7B-Chatï¼ˆINT8 é‡åŒ–ï¼‰
```shell
export CUDA_VISIBLE_DEVICES=0,1
python -m fastchat.serve.model_worker \
  --model-path Qwen/Qwen-7B-Chat \
  --model-names gpt-3.5-turbo \
  --load-8bit \
  --num-gpus 2
```

#### ChatGLM3-6B
```shell
python -m fastchat.serve.model_worker \
  --model-path THUDM/chatglm3-6b --port 21003 \
  --worker-address http://localhost:21003
```

#### Vicuna-7b-v1.5
```shell
python -m fastchat.serve.model_worker \
    --model-path /data/models/llm/vicuna-7b-v1.5 \
    --model-names vicuna-7b,gpt-3.5-turbo
```
- --model-names ç»™æ¨¡å‹å¤šä¸ªåç§°

### è¿è¡Œ vLLM Worker
#### Qwen-1_8B-Chat
```shell
python -m fastchat.serve.vllm_worker \
  --model-path Qwen/Qwen-1_8B-Chat \
  --model-names gpt-3.5-turbo
```
- --tensor-parallel-size è®¾ç½®ä½¿ç”¨çš„ GPU æ•°é‡`ï¼ˆé»˜è®¤ä¸º 1ï¼‰`ï¼Œ
- --dtype bfloat16
  - ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla T4 GPU has compute capability 7.5. `ä½¿ç”¨ float16 ä»£æ›¿ã€‚`
  - `torch.cuda.get_device_capability()` è¿”å› `(7, 5)`

  [å¦‚æœä¸æ”¯æŒ bfloat16ï¼Œåˆ™é™è‡³ float16](https://github.com/vllm-project/vllm/pull/1901)

#### Qwen-7B-Chat
```shell
python -m fastchat.serve.vllm_worker \
  --model-path Qwen/Qwen-7B-Chat \
  --model-names gpt-3.5-turbo \
  --tensor-parallel-size 2
```

## éƒ¨ç½²ï¼šåµŒå…¥æ¨¡å‹
### bge-base-zh-v1.5
```shell
python -m fastchat.serve.model_worker \
    --model-path bge-base-zh-v1.5 \
    --port 21100 \
    --worker-address http://localhost:21100 \
    --model-names text-embedding-ada-002
```

## éƒ¨ç½²ï¼šèŠå¤©æœºå™¨äºº

ğŸ“Œ æ¨¡å‹å˜åŠ¨éœ€è¦é‡æ–°éƒ¨ç½²èŠå¤©æœºå™¨äºº

### Gradio Web Serverï¼ˆé€‰æ‹©éƒ¨ç½²çš„ LLM è¿›è¡ŒèŠå¤©ï¼‰
```shell
python -m fastchat.serve.gradio_web_server \
    --host 0.0.0.0 --port 8001
```

![](/images/2024/FastChat/Web-Server.png)

### âš”ï¸ Multi-Tab Gradio Web Serverï¼ˆåŒæ—¶é€‰æ‹©ä¸¤ä¸ªä¸åŒçš„ LLM è¿›è¡ŒåŒæ—¶èŠå¤©ï¼‰
```shell
python -m fastchat.serve.gradio_web_server_multi \
    --host 0.0.0.0 --port 8002
```


## å…³é—­æœåŠ¡
### å…³é—­æ‰€æœ‰æœåŠ¡
```shell
python -m fastchat.serve.shutdown_serve --down all
```

### å…³é—­ Controller
```shell
python -m fastchat.serve.shutdown_serve --down controller
```

### å…³é—­ Model Worker
```shell
python -m fastchat.serve.shutdown_serve --down model_worker
```

### å…³é—­ OpenAI API Server
```shell
python -m fastchat.serve.shutdown_serve --down openai_api_server
```

### æ€æ­»æ‰€æœ‰æœåŠ¡
```shell
kill -9 $(pgrep -f fastchat)
```


## æ¸…é™¤æ—¥å¿—
`rmlog.sh`
```shell
rm *-conv.json
rm controller.log*
rm model_worker_*.log
rm gradio_web_server.log*
rm gradio_web_server_multi.log*
rm openai_api_server.log*
```


## [curl æµ‹è¯•]({% post_url 2024-01-19-use-lama-cpp-to-build-compatible-openai-services %}#curl-è°ƒç”¨-api)
