---
layout: single
title:  "æ™ºèƒ½ä¼šè®®ç³»ç»Ÿ Jetson Thor ä¸Šéƒ¨ç½²æ¨¡å‹æœåŠ¡æŒ‡å—"
date:   2025-11-24 08:00:00 +0800
categories: æ™ºèƒ½ä¼šè®®ç³»ç»Ÿ Jetson
tags: [æ™ºèƒ½ä¼šè®®ç³»ç»Ÿ, Jetson, Thor, llama-server, ASR, Qwen3]
---

<!--more-->

**å†…ç½‘IP**ï¼š`27.41.19.62`

| æœåŠ¡ | è¯´æ˜ | ç«¯å£ | æ¨¡å‹ | å¤‡æ³¨ |
| ---- | ---- | ---- | ---- | ---- |
| whisperlivekit | å®æ—¶è¯­éŸ³è¯†åˆ«æœåŠ¡ | 8000 | **Whisper**<br/>`small` (é»˜è®¤)<br/>`large-v3-turbo` | |
| llama-server | GGUF æ¨¡å‹æ¨ç†æœåŠ¡ | 8080 | `Qwen3-8B-Q5_K_M.gguf` | **æ¨¡å‹å**ï¼šqwen3<br/>**ä¸Šä¸‹æ–‡é•¿åº¦**ï¼š32K |


## ç³»ç»Ÿè®¾ç½®

### ç³»ç»Ÿä¼˜åŒ–

#### æœ€å¤§åŠŸç‡æ¨¡å¼ï¼ˆä¸€æ¬¡æ€§è®¾ç½®ï¼‰

```bash
sudo nvpmodel -m 0
```

#### å¯åŠ¨æœ€é«˜é¢‘ç‡ï¼ˆæ¯æ¬¡é‡å¯åè®¾ç½®ï¼‰

```bash
sudo jetson_clocks
```

### æ¸…ç†å†…å­˜

```bash
sync && echo 3 | sudo tee /proc/sys/vm/drop_caches
```


## WhisperLiveKit

* [WhisperLiveKit - å®æ—¶è¯­éŸ³è¯†åˆ«]({% post_url 2025-11-10-WhisperLiveKit %})

### éƒ¨ç½²æœåŠ¡

```bash
tmux new -s wlk
```

#### é»˜è®¤å®¹å™¨å†…åº”ç”¨ï¼ˆæ ‡ç‚¹è¯†åˆ«æœ‰æ—¶ä¼šå¤±çµï¼‰

```bash
docker run -it \
  --ipc=host \
  --net=host \
  --runtime=nvidia \
  -e MODEL=small \
  -e PORT=8000 \
  -e LANG=zh \
  -e DIAR=true \
  wangjunjian/whisperlivekit
```

#### ä¸»æœºæ–°å»ºåº”ç”¨

```bash
cd /home/lnsoft/wjj/whisperlivekit
touch entrypoint.sh
chmod +x entrypoint.sh
```

ç¼–è¾‘æ–‡ä»¶ï¼š`entrypoint.sh`

```shell
#!/usr/bin/env bash

MODEL="${MODEL:-small}"
PORT="${PORT:-8000}"
LANG="${LANG:-zh}"
DIAR="${DIAR:-false}"

EXTRA_ARGS=()
if [[ "$DIAR" == "true" || "$DIAR" == "1" ]]; then
    EXTRA_ARGS+=("--diarization")
fi

exec whisperlivekit-server \
    --model "$MODEL" \
    --host 0.0.0.0 --port "$PORT" \
    --ssl-certfile /root/.cert/cert.pem \
    --ssl-keyfile /root/.cert/key.pem \
    --language "$LANG" \
    --init-prompt "å¤§å®¶å¥½ï¼Œæˆ‘ä»¬è¦å¼€å§‹å¼€ä¼šäº†ã€‚" \
    --warmup-file /root/warmup.wav \
    "${EXTRA_ARGS[@]}"
```

ğŸ“Œ è¿è¡Œå®¹å™¨ï¼š

```bash
docker run -it \
  --ipc=host \
  --net=host \
  --runtime=nvidia \
  -v /home/lnsoft/wjj/whisperlivekit/entrypoint.sh:/root/entrypoint.sh \
  -e MODEL=large-v3-turbo \
  -e PORT=8000 \
  -e LANG=zh \
  -e DIAR=true \
  wangjunjian/whisperlivekit
```


## llama-server

```bash
cd /models/llama.cpp
```

### Dockerfile

```dockerfile
# åŸºé•œåƒ
FROM nvcr.io/nvidia/pytorch:25.10-py3

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# æš´éœ²æœåŠ¡ç«¯å£
EXPOSE 8000

# æ‹·è´ models ç›®å½•åŠå…¶å†…å®¹ã€‚
# æ³¨æ„ï¼šè¿™è¦æ±‚æœ¬åœ°å­˜åœ¨ä¸€ä¸ªåä¸º models çš„ç›®å½•ã€‚
COPY ./models /models

# æ‹·è´ llama-server å¯æ‰§è¡Œæ–‡ä»¶å’Œæ‰€éœ€çš„åŠ¨æ€åº“ (*.so æ–‡ä»¶)ã€‚
# å‡è®¾å®ƒä»¬ä½äºæœ¬åœ°çš„ build/bin/ ç›®å½•ä¸‹ã€‚
COPY ./build/bin/ /app/bin/

# ã€é‡è¦ã€‘å°† /app/bin ç›®å½•æ·»åŠ åˆ° LD_LIBRARY_PATHï¼Œ
# ç¡®ä¿ llama-server è¿è¡Œæ—¶èƒ½æ‰¾åˆ°æ‰€éœ€çš„åŠ¨æ€åº“ (*.so)ã€‚
ENV LD_LIBRARY_PATH=/app/bin:$LD_LIBRARY_PATH

# è®¾ç½®å®¹å™¨å¯åŠ¨æ—¶æ‰§è¡Œçš„å‘½ä»¤ã€‚
# ä½¿ç”¨ CMD ä»¥ä¾¿åœ¨è¿è¡Œæ—¶å¯ä»¥è½»æ¾è¦†ç›–è¿™äº›å‚æ•°ã€‚
CMD ["/app/bin/llama-server", \
  "--model", "/models/Qwen3-8B-Q5_K_M.gguf", \
  "--alias", "qwen3", \
  "--host", "0.0.0.0", \
  "--port", "8000", \
  "--ctx-size", "0", \
  "--no-kv-offload", \
  "--no-op-offload", \
  "--no-mmap", \
  "--mlock", \
  "--jinja", \
  "--reasoning-budget", "0"]
```

- `--no-mmap` å’Œ `--mlock` ç¡®ä¿æ¨¡å‹æƒé‡è¢«å®Œå…¨åŠ è½½åˆ°ç‰©ç†å†…å­˜å¹¶é”å®šï¼Œé¿å…äº†å†…å­˜æ˜ å°„å¯èƒ½å¸¦æ¥çš„æ–‡ä»¶ç³»ç»Ÿæˆ–è™šæ‹Ÿå†…å­˜é—®é¢˜ã€‚
- `--no-kv-offload` å’Œ `--no-op-offload` åˆ™ç¡®ä¿äº† KV Cache å’Œæ“ä½œéƒ½åœ¨ GPU ä¸Šæ‰§è¡Œï¼Œæ¶ˆé™¤äº†åœ¨ VRAM å’Œç³»ç»Ÿ RAM ä¹‹é—´ç§»åŠ¨æ•°æ®å¯èƒ½å¯¼è‡´çš„åŒæ­¥æˆ–å¯»å€é—®é¢˜ã€‚

> é˜²æ­¢æœåŠ¡å´©æºƒã€‚

### æ„å»ºé•œåƒ

```bash
docker build -t wangjunjian/llama-server .
```

### ğŸ“Œ éƒ¨ç½²æœåŠ¡

```bash
tmux new -s llm
```

```bash
docker run -it --runtime=nvidia -p 8080:8000 wangjunjian/llama-server
```

### æµ‹è¯•æœåŠ¡

```bash
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3",
    "messages": [{"role": "user", "content": "ä½ å¥½ï¼ŒJetson AGX Thor!"}],
    "max_tokens": 64
  }'
```

### å¯¼å‡ºé•œåƒ

```bash
docker save wangjunjian/llama-server -o llama-server.tar
```

### å¯¼å…¥é•œåƒ

```bash
docker load -i llama-server.tar
```


## vLLM

### è¿è¡Œå®¹å™¨

```bash
docker run -it \
  --ipc=host \
  --net=host \
  --runtime=nvidia \
  --name=vllm \
  -v /home/lnsoft/wjj/models:/models \
  nvcr.io/nvidia/vllm:25.09-py3 \
  bash
```

### å¯åŠ¨æœåŠ¡

```bash
VLLM_DISABLED_KERNELS=MacheteLinearKernel \
vllm serve /models/okwinds/Qwen3-Coder-30B-A3B-Instruct-Int4-W4A16 \
    --served-model-name qwen3 \
    --host 0.0.0.0 \
    --port 8080 \
    --max-model-len 16000 \
    --gpu-memory-utilization 0.9
```
