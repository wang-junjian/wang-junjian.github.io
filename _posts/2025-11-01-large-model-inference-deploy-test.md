---
layout: single
title:  "å¤§æ¨¡å‹ï¼ˆè¯­è¨€ã€è§†è§‰è¯­è¨€ã€è¯­éŸ³ï¼‰æ¨ç†æœåŠ¡éƒ¨ç½²ä¸æµ‹è¯•"
date:   2025-11-01 08:00:00 +0800
categories: LLM æ¨ç†
tags: [LLM, æ¨ç†, CUDA, vLLM, SGLang, llama.cpp, whisper.cpp, curl, ASR, JetsonThor]
---

<!--more-->


## æ¨ç†æœåŠ¡

### [CUDA GPU Compute Capabilityï¼ˆè®¡ç®—èƒ½åŠ›ï¼‰](https://developer.nvidia.com/cuda-gpus)

`è®¡ç®—èƒ½åŠ›ï¼ˆCCï¼‰`å®šä¹‰äº†æ¯ç§ NVIDIA GPU æ¶æ„çš„**ç¡¬ä»¶ç‰¹æ€§**å’Œ**æ”¯æŒçš„æŒ‡ä»¤**ã€‚åœ¨ä¸‹è¡¨ä¸­æŸ¥æ‰¾æ‚¨çš„GPUçš„è®¡ç®—èƒ½åŠ›ã€‚

![](/images/2025/Jetson/llama-server/cc-110.png)

### vLLM

```bash
docker run -it --rm \
  --ipc=host \
  --net=host \
  --runtime=nvidia \
  --name=vllm-test \
  -v /models:/models \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -v ~/.cache/modelscope:/root/.cache/modelscope \
  nvcr.io/nvidia/vllm:25.10-py3 \
  bash
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæ¨¡å‹æœªæŒ‡å‘æœ‰æ•ˆçš„æœ¬åœ°ç›®å½•ï¼Œå®ƒå°†ä» Hugging Face Hub ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚è¦ä» ModelScope ä¸‹è½½æ¨¡å‹ï¼Œè¯·åœ¨è¿è¡Œå‘½ä»¤ä¹‹å‰è¿›è¡Œå¦‚ä¸‹è®¾ç½®ï¼š

```bash
export VLLM_USE_MODELSCOPE=true
```

```bash
vllm serve /models/Qwen/Qwen3-8B \
  --served-model-name qwen3 \
  --chat-template /models/Qwen/Qwen3-8B/qwen3_nonthinking.jinja
```

### SGLang

```bash
docker run -it --rm \
  --ipc=host \
  --net=host \
  --runtime=nvidia \
  --name=sglang-test \
  -v /models:/models \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -v ~/.cache/modelscope:/root/.cache/modelscope \
  nvcr.io/nvidia/sglang:25.10-py3 \
  bash
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœ `--model-path` æœªæŒ‡å‘æœ‰æ•ˆçš„æœ¬åœ°ç›®å½•ï¼Œå®ƒå°†ä» Hugging Face Hub ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚è¦ä» ModelScope ä¸‹è½½æ¨¡å‹ï¼Œè¯·åœ¨è¿è¡Œå‘½ä»¤ä¹‹å‰è¿›è¡Œå¦‚ä¸‹è®¾ç½®ï¼š

```bash
export SGLANG_USE_MODELSCOPE=true
```

```bash
python -m sglang.launch_server \
  --model-path /models/Qwen/Qwen3-8B \
  --served-model-name qwen3 \
  --host 0.0.0.0 --port 8000 \
  --cuda-graph-bs 2
```

### [llama.cpp](https://github.com/ggml-org/llama.cpp)

#### ç¼–è¯‘

```bash
git clone https://github.com/ggml-org/llama.cpp.git
cd llama.cpp

cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES="110"
cmake --build build -j --config Release
```

#### éƒ¨ç½²ï¼ˆllama-serverï¼‰

```bash
./build/bin/llama-server \
  --model /models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf \
  --alias qwen3 \
  --host 0.0.0.0 \
  --port 8000 \
  --ctx-size 0
```

### [whisper.cpp](https://github.com/ggml-org/whisper.cpp)

#### ç¼–è¯‘

```bash
git clone https://github.com/ggml-org/whisper.cpp.git
cd whisper.cpp

cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES="110"
cmake --build build -j --config Release
```

#### éƒ¨ç½²ï¼ˆwhisper-serverï¼‰

```bash
./whisper-server \
    --model /models/whisper.cpp/models/ggml-large-v3-turbo.bin \
    --host 0.0.0.0 \
    --port 8080 \
    --flash-attn \
    --language auto
```


## è¯­è¨€

**Jetson Thor**

| æ¨¡å‹ | é‡åŒ–ç²¾åº¦ | vLLM (tok/s) | SGLang (tok/s) | llama.cpp (tok/s) |
| --- | --- |  ---: | ---: | ---: |
| Qwen3-8B | bfloat16 | 10.5 | 11.3 | â›” |
| Qwen3-8B-FP8 | fp8_w8a8 | âŒ | 27.3 | â›” |
| Qwen3-8B-FP4 | modelopt_fp4 | 33.3 | 29.0 | â›” |
| Qwen3-8B-GGUF | Q5_K_M | â›” | â›” | 37.4 ğŸš€ |
| Qwen3-Coder-30B-A3B-Instruct-FP8 | fp8_w8a8 | âŒ | âŒ | â›” |
| Qwen3-Coder-30B-A3B-Instruct-GGUF | Q5_K_M | â›” | â›” | 58.7 ğŸš€ |

> âŒ éƒ¨ç½²åä¸èƒ½æœ‰æ•ˆè¾“å‡ºï¼ˆ`ä¹±ç `ã€`!!!...`ã€`å´©æºƒ`ç­‰ï¼‰ã€‚â›” ä¸æ”¯æŒã€‚
> 
> `Qwen3-Coder-30B-A3B-Instruct-FP8` **vLLM** å’Œ **SGLang** éƒ¨ç½²åï¼Œå‘ç”Ÿéæ³•å†…å­˜è®¿é—®ï¼ŒCUDA å´©æºƒã€‚

### æ¨¡å‹
- [Qwen3-8B](https://www.modelscope.cn/models/Qwen/Qwen3-8B)
- [Qwen3-8B-FP4](https://www.modelscope.cn/models/nv-community/Qwen3-8B-FP4)
- [Qwen3-8B-FP8](https://www.modelscope.cn/models/Qwen/Qwen3-8B-FP8)
- [Qwen3-8B-GGUF](https://www.modelscope.cn/models/Qwen/Qwen3-8B-GGUF)
- [Qwen3-Coder-30B-A3B-Instruct-FP8](https://www.modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8)
- [Qwen3-Coder-30B-A3B-Instruct-GGUF](https://www.modelscope.cn/models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF)

è¦å®Œå…¨ç¦ç”¨æ€è€ƒåŠŸèƒ½ï¼Œä½ å¯ä»¥åœ¨å¯åŠ¨æ¨¡å‹æ—¶ä½¿ç”¨[è‡ªå®šä¹‰èŠå¤©æ¨¡æ¿](https://qwen.readthedocs.io/en/latest/_downloads/c101120b5bebcc2f12ec504fc93a965e/qwen3_nonthinking.jinja)ï¼š

```bash
wget https://qwen.readthedocs.io/en/latest/_downloads/c101120b5bebcc2f12ec504fc93a965e/qwen3_nonthinking.jinja
```

èŠå¤©æ¨¡æ¿ä¼šé˜»æ­¢æ¨¡å‹ç”Ÿæˆæ€è€ƒå†…å®¹ï¼Œå³ä½¿ç”¨æˆ·é€šè¿‡ `/think` æŒ‡ç¤ºæ¨¡å‹è¿™æ ·åšã€‚

- [Qwen - vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html)
- [Qwen - SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html)

### vLLM

#### Qwen3-8B

- ä¸æ€è€ƒ

```bash
vllm serve /models/Qwen/Qwen3-8B \
  --served-model-name qwen3 \
  --chat-template /models/Qwen/Qwen3-8B/qwen3_nonthinking.jinja
```

- æ€è€ƒ ğŸ’¬

```bash
vllm serve /models/Qwen/Qwen3-8B \
  --served-model-name qwen3
```

#### Qwen3-8B-FP4

> repetition_penalty è§£å†³é‡å¤ä¸åœè¾“å‡ºçš„é—®é¢˜ã€‚

- ä¸æ€è€ƒ

```bash
vllm serve /models/nv-community/Qwen3-8B-FP4 \
  --served-model-name qwen3 \
  --override-generation-config '{
    "repetition_penalty": 1.2,
    "temperature": 0.8,
    "top_p": 0.95
  }' \
  --chat-template /models/Qwen/Qwen3-8B/qwen3_nonthinking.jinja
```

- æ€è€ƒ ğŸ’¬

```bash
vllm serve /models/nv-community/Qwen3-8B-FP4 \
  --served-model-name qwen3 \
  --override-generation-config '{
    "repetition_penalty": 1.2,
    "temperature": 0.8,
    "top_p": 0.95
  }'
```

#### Qwen3-Coder-30B-A3B-Instruct-FP8

```bash
vllm serve /models/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8 \
  --served-model-name qwen3 \
  --max-model-len 65536 \
  --enforce-eager \
  --override-generation-config '{
    "repetition_penalty": 1.05,
    "temperature": 0.7,
    "top_p": 0.8,
    "top_k": 20
  }'
```

> `--enforce-eager` è§£å†³ï¼štorch.AcceleratorError: CUDA error: an illegal memory access was encountered

### SGLang

#### Qwen3-8B

- ä¸æ€è€ƒ

```bash
python -m sglang.launch_server \
  --model-path /models/Qwen/Qwen3-8B \
  --served-model-name qwen3 \
  --host 0.0.0.0 --port 8000 \
  --chat-template /models/Qwen/Qwen3-8B/qwen3_nonthinking.jinja
```

- æ€è€ƒ ğŸ’¬

```bash
python -m sglang.launch_server \
  --model-path /models/Qwen/Qwen3-8B \
  --served-model-name qwen3 \
  --host 0.0.0.0 --port 8000
```

#### Qwen3-8B-FP8

> å¥½ä¸å®¹æ˜“è°ƒå¥½äº†ï¼Œä¸€ç›´æœ‰ä¹±ç ã€é‡å¤ç­‰é—®é¢˜ã€‚

- ä¸æ€è€ƒ

```bash
python -m sglang.launch_server \
  --model-path /models/Qwen/Qwen3-8B-FP8 \
  --served-model-name qwen3 \
  --host 0.0.0.0 --port 8000 \
  --max-running-requests 2 \
  --preferred-sampling-params '{
      "repetition_penalty": 1.2,
      "temperature": 0.7,
      "top_p": 0.8,
      "top_k": 20,
      "min_p": 0.0
    }' \
  --chat-template /models/Qwen/Qwen3-8B/qwen3_nonthinking.jinja
```

- æ€è€ƒ ğŸ’¬

```bash
python -m sglang.launch_server \
  --model-path /models/Qwen/Qwen3-8B-FP8 \
  --served-model-name qwen3 \
  --host 0.0.0.0 --port 8000 \
  --max-running-requests 2 \
  --preferred-sampling-params '{
      "repetition_penalty": 1.2,
      "temperature": 0.6,
      "top_p": 0.95,
      "top_k": 20,
      "min_p": 0.0
    }'
```

#### Qwen3-8B-FP4

- ä¸æ€è€ƒ

```bash
python -m sglang.launch_server \
  --model-path /models/nv-community/Qwen3-8B-FP4 \
  --served-model-name qwen3 \
  --host 0.0.0.0 --port 8000 \
  --quantization modelopt_fp4 \
  --max-running-requests 2 \
  --chat-template /models/Qwen/Qwen3-8B/qwen3_nonthinking.jinja
```

- æ€è€ƒ ğŸ’¬

```bash
python -m sglang.launch_server \
  --model-path /models/nv-community/Qwen3-8B-FP4 \
  --served-model-name qwen3 \
  --host 0.0.0.0 --port 8000 \
  --quantization modelopt_fp4 \
  --max-running-requests 2
```

#### Qwen3-Coder-30B-A3B-Instruct-FP8

```bash
python -m sglang.launch_server \
  --model-path /models/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8 \
  --served-model-name qwen3 \
  --host 0.0.0.0 --port 8000 \
  --max-running-requests 1 \
  --max-total-tokens 65536 \
  --disable-cuda-graph \
  --preferred-sampling-params '{
      "repetition_penalty": 1.05,
      "temperature": 0.7,
      "top_p": 0.8,
      "top_k": 20
    }'
```

> `--disable-cuda-graph` è§£å†³ï¼štorch.AcceleratorError: CUDA error: an illegal memory access was encountered

### llama.cpp

#### Qwen3-8B-GGUF

- ä¸æ€è€ƒ
  
```bash
./build/bin/llama-server \
  --model /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_K_M.gguf \
  --alias qwen3 \
  --host 0.0.0.0 \
  --port 8000 \
  --ctx-size 0 \
  --jinja \
  --reasoning-budget 0
```

- æ€è€ƒ ğŸ’¬

```bash
./build/bin/llama-server \
  --model /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_K_M.gguf \
  --alias qwen3 \
  --host 0.0.0.0 \
  --port 8000 \
  --ctx-size 0
```

#### Qwen3-Coder-30B-A3B-Instruct-GGUF

```bash
./build/bin/llama-server \
  --model /models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf \
  --alias qwen3 \
  --host 0.0.0.0 \
  --port 8000 \
  --ctx-size 0
```


## è¯­éŸ³

### æ¨¡å‹

- [whisper-small](https://www.modelscope.cn/models/openai-mirror/whisper-small)
- [whisper-large-v3-turbo](https://www.modelscope.cn/models/openai-mirror/whisper-large-v3-turbo)

```bash
vllm serve /models/openai-mirror/whisper-small --served-model-name whisper
```

### æµ‹è¯•ï¼ˆcurlï¼‰

#### vllm

```bash
curl "http://127.0.0.1:8000/v1/audio/transcriptions" \
    --form "file=@/models/llama.cpp/tools/mtmd/test-2.mp3" \
    --form "model=whisper" \
    --form "response_format=json" \
    --form "language=zh"
```

#### whisper.cpp

```bash
curl 127.0.0.1:8080/inference \
    -H "Content-Type: multipart/form-data" \
    -F file="@samples/jfk.wav" \
    -F response_format="json"
```


## è§†è§‰-è¯­è¨€

### æ¨¡å‹

- [Qwen2.5-VL-7B-Instruct-AWQ](https://www.modelscope.cn/models/Qwen/Qwen2.5-VL-7B-Instruct-AWQ)
- [Qwen3-VL-30B-A3B-Instruct-GGUF](https://www.modelscope.cn/models/yairpatch/Qwen3-VL-30B-A3B-Instruct-GGUF)

```bash
vllm serve /models/Qwen/Qwen2.5-VL-7B-Instruct-AWQ --served-model-name qwen3
```

### æµ‹è¯•ï¼ˆcurlï¼‰

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "image_url", "image_url": {"url": "http://book.d2l.ai/_images/catdog.jpg"}},
          {"type": "text", "text": "è§£é‡Šå›¾åƒ"}
        ]
      }
    ]
  }'
```

```bash
{
  "id": "chatcmpl-ca34716637f34580a80a95610ea43557",
  "object": "chat.completion",
  "created": 1760853120,
  "model": "qwen3",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "è¿™å¼ å›¾åƒå±•ç¤ºäº†ä¸¤åªå¯çˆ±çš„å® ç‰©â€”â€”ä¸€åªå°ç‹—å’Œä¸€åªå°çŒ«ã€‚å°ç‹—ä½äºå›¾åƒçš„å·¦ä¾§ï¼Œå®ƒçš„æ¯›è‰²æ˜¯æµ…æ£•è‰²ï¼Œè€³æœµæ˜¯é»‘è‰²çš„ï¼Œçœ¼ç›å¤§è€Œæ˜äº®ï¼Œè¡¨æƒ…æ˜¾å¾—å¾ˆä¸“æ³¨ã€‚å°çŒ«ä½äºå›¾åƒçš„å³ä¾§ï¼Œå®ƒçš„æ¯›è‰²æ˜¯ç°è‰²å’Œç™½è‰²ç›¸é—´çš„æ¡çº¹ï¼Œå˜´å·´å¼ å¼€ï¼Œä¼¼ä¹åœ¨å«å”¤æˆ–å‘å‡ºå£°éŸ³ã€‚èƒŒæ™¯æ˜¯çº¯ç™½è‰²çš„ï¼Œä½¿å¾—å°ç‹—å’Œå°çŒ«æˆä¸ºå›¾åƒçš„ç„¦ç‚¹ã€‚æ•´ä½“æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€å¹…æ¸©é¦¨ä¸”å……æ»¡æ´»åŠ›çš„ç”»é¢ï¼Œå±•ç°äº†å® ç‰©ä¹‹é—´çš„å’Œè°å…±å¤„ã€‚",
        "refusal": null,
        "annotations": null,
        "audio": null,
        "function_call": null,
        "tool_calls": [],
        "reasoning_content": null
      },
      "logprobs": null,
      "finish_reason": "stop",
      "stop_reason": null
    }
  ],
  "service_tier": null,
  "system_fingerprint": null,
  "usage": {
    "prompt_tokens": 543,
    "total_tokens": 651,
    "completion_tokens": 108,
    "prompt_tokens_details": null
  },
  "prompt_logprobs": null,
  "kv_transfer_params": null
}
```

```bash
vllm serve /models/Qwen/Qwen2.5-VL-7B-Instruct-AWQ --served-model-name qwen3 --allowed-local-media-path
```

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3",
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "image_url", "image_url": {"url": "file:///models/iic/SenseVoiceSmall/fig/asr_results.png"}},
          {"type": "text", "text": "è§£é‡Šå›¾åƒ"}
        ]
      }
    ]
  }'
```


## å‚è€ƒèµ„æ–™
- [FunASR Github](https://github.com/modelscope/FunASR)
- [FunASR å®˜ç½‘](https://www.funasr.com/)
