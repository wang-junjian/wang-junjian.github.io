---
layout: post
title:  "åœ¨ MLX ä¸Šä½¿ç”¨ LoRA / QLoRA å¾®è°ƒ Text2SQLï¼ˆä¸ƒï¼‰ï¼šMLX å¾®è°ƒçš„æ¨¡å‹è½¬æ¢ä¸º GGUF æ¨¡å‹"
date:   2024-01-28 08:00:00 +0800
categories: MLX Text2SQL
tags: [MLX, LoRA, Mistral-7B, Text2SQL, WikiSQL, MacBookProM2Max]
---

å°† MLX å¾®è°ƒçš„æ¨¡å‹è½¬æ¢ä¸º GGUF æ¨¡å‹æœ€å¤§çš„æ„ä¹‰æ˜¯å¯ä»¥èå…¥ GGUF çš„ç”Ÿæ€ç³»ç»Ÿï¼Œå¯ä»¥åœ¨æ›´å¤šçš„å¹³å°ä¸Šä½¿ç”¨ã€‚

## LoRA å¾®è°ƒ

### å¤§æ¨¡å‹ Mistral-7B-v0.1
- [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)

### æ•°æ®é›† WikiSQL

- [WikiSQL](https://github.com/salesforce/WikiSQL)
- [sqllama/sqllama-V0](https://huggingface.co/sqllama/sqllama-V0/blob/main/wikisql.ipynb)

ä¿®æ”¹è„šæœ¬ `mlx-examples/lora/data/wikisql.py`
```py
if __name__ == "__main__":
    # ......
    for dataset, name, size in datasets:
        with open(f"data/{name}.jsonl", "w") as fid:
            for e, t in zip(range(size), dataset):
                t = t[3:]
                json.dump({"text": t}, fid)
                fid.write("\n")
```

æ‰§è¡Œè„šæœ¬ `data/wikisql.py` ç”Ÿæˆæ•°æ®é›†ã€‚
    
```bash
data/wikisql.py
```

### å®‰è£… mlx-lm

```bash
pip install mlx-lm
```

### å¾®è°ƒ

```bash
time python -m mlx_lm.lora --model mistralai/Mistral-7B-v0.1 --train --iters 1000
```
```
Loading pretrained model
Total parameters 7243.436M
Trainable parameters 1.704M
Loading datasets
Training
Iter 1: Val loss 2.344, Val took 31.766s
Iter 100: Train loss 1.206, It/sec 0.526, Tokens/sec 200.354
Iter 200: Train loss 1.098, It/sec 0.527, Tokens/sec 200.672
Iter 200: Val loss 1.119, Val took 29.064s
Iter 300: Train loss 0.827, It/sec 0.514, Tokens/sec 209.714
Iter 400: Train loss 0.834, It/sec 0.473, Tokens/sec 200.296
Iter 400: Val loss 1.085, Val took 26.985s
Iter 500: Train loss 0.774, It/sec 0.469, Tokens/sec 193.179
Iter 600: Train loss 0.601, It/sec 0.528, Tokens/sec 217.035
Iter 600: Val loss 0.983, Val took 30.665s
Iter 700: Train loss 0.580, It/sec 0.466, Tokens/sec 189.138
Iter 800: Train loss 0.540, It/sec 0.507, Tokens/sec 206.917
Iter 800: Val loss 0.971, Val took 26.399s
Iter 900: Train loss 0.496, It/sec 0.467, Tokens/sec 188.202
Iter 1000: Train loss 0.479, It/sec 0.562, Tokens/sec 215.943
Iter 1000: Val loss 0.967, Val took 31.298s
python lora.py --model mistralai/Mistral-7B-v0.1 --train --iters 1000  85.61s user 390.20s system 22% cpu 35:09.24 total
```

| Iteration | Train Loss | Val Loss | Tokens/sec |
| :-------: | ----------:| --------:| ----------:|
| 1         |            | 2.343    |            |
| 100       | 1.206      |          | 200.354    |
| 200       | 1.098      | 1.119    | 200.672    |
| 300       | 0.827      |          | 209.714    |
| 400       | 0.834      | 1.085    | 200.296    |
| 500       | 0.774      |          | 193.179    |
| 600       | 0.601      | 0.983    | 217.035    |
| 700       | 0.580      |          | 189.138    |
| 800       | 0.540      | 0.971    | 206.917    |
| 900       | 0.496      |          | 188.202    |
| 1000      | 0.479      | 0.967    | 215.943    |

è€—æ—¶ 35 åˆ† 9 ç§’ã€‚

### è¯„ä¼°

è®¡ç®—æµ‹è¯•é›†å›°æƒ‘åº¦ï¼ˆPPLï¼‰å’Œäº¤å‰ç†µæŸå¤±ï¼ˆLossï¼‰ã€‚

```bash
for i in {100..1000..100}; do python -m mlx_lm.lora --model mistralai/Mistral-7B-v0.1 --test --adapter-file adapters/$i.npz; done
```
```
100  Test loss 1.352, Test ppl 3.865.
200  Test loss 1.323, Test ppl 3.756.
300  Test loss 1.350, Test ppl 3.857.
400  Test loss 1.366, Test ppl 3.919.
500  Test loss 1.345, Test ppl 3.839.
600  Test loss 1.355, Test ppl 3.879.
700  Test loss 1.364, Test ppl 3.911.
800  Test loss 1.397, Test ppl 4.043.
900  Test loss 1.419, Test ppl 4.132.
1000 Test loss 1.426, Test ppl 4.161.
```

| Iteration | Test Loss | Test PPL |
| :-------: | ---------:| --------:|
| 100       | 1.352     | 3.865    |
| 200       | 1.323     | 3.756    |
| 300       | 1.350     | 3.857    |
| 400       | 1.366     | 3.919    |
| 500       | 1.345     | 3.839    |
| 600       | 1.355     | 3.879    |
| 700       | 1.364     | 3.911    |
| 800       | 1.397     | 4.043    |
| 900       | 1.419     | 4.132    |
| 1000      | 1.426     | 4.161    |

### èåˆï¼ˆFuseï¼‰

```bash
python fuse.py --model mistralai/Mistral-7B-v0.1 \
               --adapter-file adapters.npz \
               --save-path lora_fused_model \
               --de-quantize
```

èåˆåçš„ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š

```bash
lora_fused_model
â”œâ”€â”€ config.json
â”œâ”€â”€ model-00001-of-00003.safetensors
â”œâ”€â”€ model-00002-of-00003.safetensors
â”œâ”€â”€ model-00003-of-00003.safetensors
â”œâ”€â”€ special_tokens_map.json
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer.model
â””â”€â”€ tokenizer_config.json
```

âŒ `fuse.py` æœ‰å¼‚å¸¸é”™è¯¯ `IndexError: list index out of range`ã€‚

```bash
Loading pretrained model
Traceback (most recent call last):
  File "/Users/junjian/GitHub/ml-explore/mlx-examples/lora/fuse.py", line 98, in <module>
    model.update_modules(tree_unflatten(de_quantize_layers))
  File "/Users/junjian/GitHub/ml-explore/mlx-examples/env/lib/python3.10/site-packages/mlx/utils.py", line 123, in tree_unflatten
    int(tree[0][0].split(".", maxsplit=1)[0])
IndexError: list index out of range
```

ä¿®æ”¹è„šæœ¬ `/Users/junjian/GitHub/ml-explore/mlx-examples/env/lib/python3.10/site-packages/mlx/utils.py`

```py
    try:
        int(tree[0][0].split(".", maxsplit=1)[0])
        is_list = True
    except ValueError:
        is_list = False
    except IndexError:  # <--- Add this line
        is_list = False
```

âŒ ä½¿ç”¨ `mlx_lm.fuse` æœ‰ç‚¹é—®é¢˜ï¼Œçœ‹ä¸åˆ°å¾®è°ƒåçš„æ•ˆæœã€‚

```bash
python -m mlx_lm.generate --model lora_fused_model \     
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: How old is Wang Junjian?
A: "
==========
Prompt: table: students
columns: Name, Age, School, Grade, Height, Weight
Q: How old is Wang Junjian?
A: 
14

table: teachers
columns: Name, Age, School, Title, Salary
Q: How old is Jane?
A: 35

table: students
columns: Name, Age, School, Grade,
==========
Prompt: 85.823 tokens-per-sec
Generation: 50.520 tokens-per-sec
```

âŒ æ²¡æœ‰ä½¿ç”¨ `--de-quantize` é€‰é¡¹ï¼Œä¼šæŠ¥é”™ã€‚

```bash
[INFO] Loading model from ../../lora/lora_fused_model/ggml-model-f16.gguf
Traceback (most recent call last):
  File "/Users/junjian/GitHub/ml-explore/mlx-examples/llms/gguf_llm/generate.py", line 85, in <module>
    model, tokenizer = models.load(args.gguf, args.repo)
  File "/Users/junjian/GitHub/ml-explore/mlx-examples/llms/gguf_llm/models.py", line 321, in load
    dequantize("model.embed_tokens")
  File "/Users/junjian/GitHub/ml-explore/mlx-examples/llms/gguf_llm/models.py", line 314, in dequantize
    scales = weights.pop(f"{k}.scales")
KeyError: 'model.embed_tokens.scales'
```

### ç”Ÿæˆ

æŸ¥è¯¢å§“åï¼Œå¹´é¾„ï¼Œå­¦æ ¡ï¼Œå¹´çº§ç­‰ä¿¡æ¯ï¼Œæ¡ä»¶ä¸ºå§“åç­‰äºç‹å†›å»ºä¸”å¹´é¾„å°äº20å²ã€‚

```bash
python -m mlx_lm.generate --model lora_fused_model \
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Query information such as name, age, school, grade, etc. The condition is that the name is equal to Wang Junjian and the age is less than 20 years old.
A: "
```

```
SELECT Name, Age, School, Grade, Height, Weight FROM students WHERE Name = 'Wang Junjian' AND Age < 20
```


## è½¬æ¢ GGUF æ¨¡å‹
- [How do I create a GGUF model file?](https://www.secondstate.io/articles/convert-pytorch-to-gguf/)

### æ„å»º llama.cpp
- [ä½¿ç”¨ llama.cpp æ„å»ºæœ¬åœ°èŠå¤©æœåŠ¡]({% post_url 2023-12-16-building-a-local-chat-service-using-llama-cpp %})
- [ä½¿ç”¨ llama.cpp æ„å»ºå…¼å®¹ OpenAI API æœåŠ¡]({% post_url 2024-01-19-use-lama-cpp-to-build-compatible-openai-services %})

```bash
# clone
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
# make
make -j
# install packages
pip install -r requirements.txt
```

### è½¬æ¢ GGUF æ¨¡å‹ï¼ˆf32ï¼‰

ä¸ºäº†æ›´å¥½çš„ä¿ç•™ç²¾åº¦ï¼Œä½¿ç”¨ f32 æ¨¡å‹ã€‚

```bash
python convert.py --outtype f32 \
    --outfile Mistral-7B-v0.1-LoRA-Text2SQL-f32.gguf \
    /Users/junjian/GitHub/ml-explore/mlx-examples/lora/lora_fused_model
```

### é‡åŒ– GGUF æ¨¡å‹

- Q8_0

```bash
./quantize Mistral-7B-v0.1-LoRA-Text2SQL-f32.gguf \
           Mistral-7B-v0.1-LoRA-Text2SQL.Q8_0.gguf \
           Q8_0
```

- Q4_1

```bash
./quantize Mistral-7B-v0.1-LoRA-Text2SQL-f32.gguf \
           Mistral-7B-v0.1-LoRA-Text2SQL.Q4_1.gguf \
           Q4_1
```

- Q4_0

```bash
./quantize Mistral-7B-v0.1-LoRA-Text2SQL-f32.gguf \
           Mistral-7B-v0.1-LoRA-Text2SQL.Q4_0.gguf \
           Q4_0
```

### ä½¿ç”¨ llama.cpp ç”Ÿæˆ

- Q8_0

```bash
./main -m Mistral-7B-v0.1-LoRA-Text2SQL.Q8_0.gguf \
       -e -p "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Query information such as name, age, school, grade, etc. The condition is that the name is equal to Wang Junjian and the age is less than 20 years old.
A: "
```

```
SELECT Name, Age, School, Grade FROM students WHERE Name = 'Wang Junjian' AND Age < 20 [end of text]

llama_print_timings:        load time =     287.98 ms
llama_print_timings:      sample time =       2.44 ms /    28 runs   (    0.09 ms per token, 11456.63 tokens per second)
llama_print_timings: prompt eval time =     169.18 ms /    65 tokens (    2.60 ms per token,   384.20 tokens per second)
llama_print_timings:        eval time =     685.85 ms /    27 runs   (   25.40 ms per token,    39.37 tokens per second)
llama_print_timings:       total time =     860.76 ms /    92 tokens
```

- Q4_1

```bash
./main -m Mistral-7B-v0.1-LoRA-Text2SQL.Q4_1.gguf \
       -e -p "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Query information such as name, age, school, grade, etc. The condition is that the name is equal to Wang Junjian and the age is less than 20 years old.
A: "
```

```
SELECT Name, Age, School, Grade FROM students WHERE Name = 'Wang Junjin' AND Age < 20 [end of text]

llama_print_timings:        load time =     232.59 ms
llama_print_timings:      sample time =       2.45 ms /    28 runs   (    0.09 ms per token, 11428.57 tokens per second)
llama_print_timings: prompt eval time =     173.11 ms /    65 tokens (    2.66 ms per token,   375.49 tokens per second)
llama_print_timings:        eval time =     465.36 ms /    27 runs   (   17.24 ms per token,    58.02 tokens per second)
llama_print_timings:       total time =     644.05 ms /    92 tokens
```

- Q4_0

```bash
./main -m Mistral-7B-v0.1-LoRA-Text2SQL.Q4_0.gguf \
       -e -p "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Query information such as name, age, school, grade, etc. The condition is that the name is equal to Wang Junjian and the age is less than 20 years old.
A: "
```

ä¸èƒ½å¾ˆå¥½çš„æŠŠéœ€è¦çš„åˆ—åéƒ½ç”Ÿæˆå‡ºæ¥ã€‚

```
SELECT Name FROM Students WHERE Name = 'Wang Junjian' AND Age < 20 [end of text]

llama_print_timings:        load time =     225.26 ms
llama_print_timings:      sample time =       1.85 ms /    22 runs   (    0.08 ms per token, 11904.76 tokens per second)
llama_print_timings: prompt eval time =     173.36 ms /    65 tokens (    2.67 ms per token,   374.94 tokens per second)
llama_print_timings:        eval time =     336.15 ms /    21 runs   (   16.01 ms per token,    62.47 tokens per second)
llama_print_timings:       total time =     513.64 ms /    86 tokens
```

### ä½¿ç”¨ MLX ç”Ÿæˆ

> MLX èƒ½å¤Ÿç›´æ¥ä» GGUF è¯»å–å¤§å¤šæ•°é‡åŒ–æ ¼å¼ã€‚ä½†æ˜¯ï¼Œä»…ç›´æ¥æ”¯æŒå°‘æ•°é‡åŒ–ï¼šQ4_0ã€Q4_1 å’Œ Q8_0ã€‚ä¸æ”¯æŒçš„é‡åŒ–å°†è½¬æ¢ä¸º float16ã€‚
- [LLMs in MLX with GGUF](https://github.com/ml-explore/mlx-examples/blob/main/llms/gguf_llm/README.md)

- Q8_0

```bash
python generate.py --gguf Mistral-7B-v0.1-LoRA-Text2SQL.Q8_0.gguf \
                   --max-tokens 50 \
                   --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Query information such as name, age, school, grade, etc. The condition is that the name is equal to Wang Junjian and the age is less than 20 years old.
A: "
[INFO] Loading model from /Users/junjian/GitHub/ggerganov/llama.cpp/Mistral-7B-v0.1-LoRA-Text2SQL.Q8_0.gguf
SELECT Name, Age, School, Grade, Height, Weight FROM students WHERE Name = 'Wang Junjian' AND Age < 20
==========
Prompt: 121.242 tokens-per-sec
Generation: 17.895 tokens-per-sec
```

ä¸Šé¢çš„æç¤ºè¯ `Query information such as name, age, school, grade, etc.` æœ‰ `etc`ï¼Œ ç”Ÿæˆçš„ SQL æ˜¯æ‰€æœ‰çš„åˆ—ã€‚ä¸‹é¢çš„ç¤ºä¾‹ç§»é™¤äº† `etc`ï¼Œç”Ÿæˆå°±æ˜¯æŒ‡å®šçš„åˆ—ã€‚

```bash
python generate.py --gguf Mistral-7B-v0.1-LoRA-Text2SQL.Q8_0.gguf \       
                   --max-tokens 50 \
                   --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Query information such as name, age, school, grade. The condition is that the name is equal to Wang Junjian and the age is less than 20 years old.
A: "
[INFO] Loading model from /Users/junjian/GitHub/ggerganov/llama.cpp/Mistral-7B-v0.1-LoRA-Text2SQL.Q8_0.gguf
SELECT Name, Age, School, Grade FROM students WHERE Name = 'Wang Junjian' AND Age < 20
==========
Prompt: 137.376 tokens-per-sec
Generation: 17.581 tokens-per-sec
```

- Q4_1
    
```bash
python generate.py --gguf Mistral-7B-v0.1-LoRA-Text2SQL.Q4_1.gguf \       
                   --max-tokens 50 \
                   --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Query information such as name, age, school, grade. The condition is that the name is equal to Wang Junjian and the age is less than 20 years old.
A: "
[INFO] Loading model from /Users/junjian/GitHub/ggerganov/llama.cpp/Mistral-7B-v0.1-LoRA-Text2SQL.Q4_1.gguf
ACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHEACHE
==========
Prompt: 122.293 tokens-per-sec
Generation: 16.854 tokens-per-sec
```

- Q4_0
    
```bash
python generate.py --gguf Mistral-7B-v0.1-LoRA-Text2SQL.Q4_0.gguf \       
                   --max-tokens 50 \
                   --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Query information such as name, age, school, grade. The condition is that the name is equal to Wang Junjian and the age is less than 20 years old.
A: "
[INFO] Loading model from /Users/junjian/GitHub/ggerganov/llama.cpp/Mistral-7B-v0.1-LoRA-Text2SQL.Q4_0.gguf
SELECT Name, Age, School, Grade FROM students WHERE Name = 'Wang Junjian' AND Age < 20
==========
Prompt: 144.101 tokens-per-sec
Generation: 16.231 tokens-per-sec
```

### æ€»ç»“
- é‡åŒ– `Q8_0` çš„æ¨¡å‹ï¼šä½¿ç”¨ llama.cpp ç”Ÿæˆé€Ÿåº¦ï¼ˆ39.37 tokens/sï¼‰æ¯” MLX ç”Ÿæˆé€Ÿåº¦ï¼ˆ17.58 tokens/sï¼‰å¿« 2 å€å¤šã€‚ğŸš€ğŸš€
- é‡åŒ– `Q4_1` çš„æ¨¡å‹ï¼šä½¿ç”¨ llama.cpp ç”Ÿæˆé€Ÿåº¦ï¼ˆ58.02 tokens/sï¼‰æ¯” MLX ç”Ÿæˆé€Ÿåº¦ï¼ˆ16.85 tokens/sï¼‰å¿« 3 å€å¤šã€‚ğŸš€ğŸš€ğŸš€
- é‡åŒ– `Q4_1` çš„æ¨¡å‹ï¼Œä½¿ç”¨ MLX ç”Ÿæˆå‡ºçš„æ˜¯æ— æ•ˆçš„ SQL è¯­å¥ï¼Œç”šè‡³æ˜¯æ— æ„ä¹‰çš„æ–‡æœ¬ã€‚âŒ
- é‡åŒ– `Q4_0` çš„æ¨¡å‹ï¼Œä½¿ç”¨ llama.cpp ç”Ÿæˆå‡ºçš„ SQL è¯­å¥æ²¡æœ‰æŒ‡å®šçš„åˆ—ã€‚âŒ
- é‡åŒ–çš„ç²¾åº¦è¶…ä½ï¼Œæ„Ÿè§‰ç”Ÿæˆçš„æ—¶å€™è¶Šå®¹æ˜“å‡ºé—®é¢˜ï¼Œç‰¹åˆ«ä¸ç¨³å®šã€‚
- æ— è®ºç²¾åº¦æ˜¯å¤šå°‘ï¼Œä½¿ç”¨ MLX ç”Ÿæˆçš„é€Ÿåº¦éƒ½æ˜¯ä¸€æ ·çš„ï¼Œè¿™åº”è¯¥æ˜¯æœ‰é—®é¢˜çš„ã€‚


## å‚è€ƒèµ„æ–™
- [MLX Community](https://huggingface.co/mlx-community)
- [Fine-Tuning with LoRA or QLoRA](https://github.com/ml-explore/mlx-examples/tree/main/lora)
- [Generate Text with LLMs and MLX](https://github.com/ml-explore/mlx-examples/tree/main/llms)
- [Awesome Text2SQL](https://github.com/eosphoros-ai/Awesome-Text2SQL)
- [Awesome Text2SQLï¼ˆä¸­æ–‡ï¼‰](https://github.com/eosphoros-ai/Awesome-Text2SQL/blob/main/README.zh.md)
- [Mistral AI](https://huggingface.co/mistralai)
- [A Beginnerâ€™s Guide to Fine-Tuning Mistral 7B Instruct Model](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe)
- [Mistral Instruct 7B Finetuning on MedMCQA Dataset](https://saankhya.medium.com/mistral-instruct-7b-finetuning-on-medmcqa-dataset-6ec2532b1ff1)
- [Fine-tuning Mistral on your own data](https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb)
- [mlx-examples llms Mistral](https://github.com/ml-explore/mlx-examples/blob/main/llms/mistral/README.md)
- [deepseek-ai/deepseek-coder-7b-base-v1.5](https://huggingface.co/deepseek-ai/deepseek-coder-7b-base-v1.5)
- [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954)
- [Benchmarking Appleâ€™s MLX vs. llama.cpp](https://medium.com/@andreask_75652/benchmarking-apples-mlx-vs-llama-cpp-bbbebdc18416)
- [Apple MLX â€”> GGUF](https://twitter.com/ivanfioravanti/status/1749929493442056656)
- [A Simple Voice Assistant Script](https://github.com/linyiLYi/voice-assistant)
- [I made an app that runs Mistral 7B 0.2 LLM locally on iPhone Pros](https://news.ycombinator.com/item?id=38906966)
