---
layout: single
title:  "Docker AI"
date:   2025-07-01 08:00:00 +0800
categories: Docker AI
tags: [Docker, MCPToolkit, MCP, MCPServer, AskGordon, ModelRunner, OpenAI API, LLM]
---

本文档详细介绍了 **Docker AI** 的新功能，旨在通过集成人工智能提升开发者的体验。主要亮点包括 **Docker Model Runner**，它简化了 AI 模型的管理和部署，支持从各种注册表拉取和运行模型，并通过兼容 OpenAI 的 API 提供服务。此外，**MCP Toolkit** 实现了容器化 MCP 服务器的无缝设置和管理，而 **Ask Gordon** 则作为嵌入式 AI 助手，在 Docker Desktop 和 CLI 中提供上下文帮助，包括改进 Dockerfile 和故障排除等。这些功能共同旨在简化 AI 驱动型应用程序的开发和部署。

<!--more-->

## Docker AI 配置

![](/images/2025/DockerMCPToolkit/Settings-BetaFeatures-EnableDockerAI.jpeg)

- **✅ Enable Docker AI (启用 Docker AI)**
    * 在 `Docker Desktop` 和`命令行（CLI）`中启用 “`Ask Gordon`” 功能。
- **✅ Enable Docker Model Runner (启用 Docker 模型运行器)**
    * 启用 GPU 加速的推理引擎，用于运行 AI 模型。
- **✅ Enable Docker MCP Toolkit (启用 Docker MCP 工具套件)**
    * 在 Docker Desktop 中启用 “`MCP Toolkit`” 功能。

![](/images/2025/DockerMCPToolkit/Settings-DockerEngine.jpeg)

- **registry-mirrors**: https://registry.cn-hangzhou.aliyuncs.com
    - 这是 Docker 镜像的镜像地址，国内使用阿里云的镜像加速器可以提高拉取镜像的速度。


## Docker Model Runner

![](/images/2025/DockerMCPToolkit/Models-Local.jpeg)

**Docker Model Runner** 让您能够使用 Docker 轻松管理、运行和部署 AI 模型。Docker Model Runner 专为开发者设计，简化了直接从 Docker Hub 或任何符合 OCI 标准的注册表拉取、运行和提供大型语言模型 (LLM) 及其他 AI 模型的流程。

通过与 Docker Desktop 和 Docker Engine 无缝集成，您可以通过兼容 OpenAI 的 API 提供模型服务，将 GGUF 文件打包为 OCI Artifacts，并从命令行和图形界面与模型进行交互。

### 主要功能

  * **从 Docker Hub 拉取和推送模型**：轻松管理您的模型版本。
  * **在兼容 OpenAI 的 API 上提供模型服务**：方便与现有应用程序集成。
  * **将 GGUF 文件打包为 OCI Artifacts 并发布到任何容器注册表**：灵活地分发您的模型。
  * **直接从命令行或 Docker Desktop GUI 运行和交互 AI 模型**：提供多种操作方式。
  * **管理本地模型并显示日志**：便于监控和调试。

### 工作原理

模型首次使用时会从 Docker Hub 拉取并存储在本地。它们只在运行时收到请求时才加载到内存中，并在不使用时卸载以优化资源。由于模型可能很大，首次拉取可能需要一些时间，但之后它们会缓存在本地以实现更快的访问。您可以使用`兼容 OpenAI API`与模型进行交互。

### 在 Docker Engine 中启用 DMR

1.  确保您已安装 `Docker Engine`。

2.  DMR 可作为软件包使用。要安装它，请运行：

    - Ubuntu/Debian
    ```console
    $ sudo apt-get update
    $ sudo apt-get install docker-model-plugin
    ````

    - RPM-based distributions
    ```console
    $ sudo dnf update
    $ sudo dnf install docker-model-plugin
    ````

3.  测试安装：

    ```console
    $ docker model version
    $ docker model run ai/smollm2
    ```

### 拉取模型

模型会在本地缓存。使用 Docker CLI 时，您还可以直接从 [HuggingFace](https://huggingface.co/) 拉取模型。

#### Docker Desktop

![](/images/2025/DockerMCPToolkit/Models-DockerHub.jpeg)


#### Docker CLI

- 从 Docker Hub 拉取
```bash
docker model pull ai/smollm2:360M-Q4_K_M
````

- 从 HuggingFace 拉取
```bash
docker model pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF
```


### 运行模型

#### Docker Desktop

![](/images/2025/DockerMCPToolkit/Models-Local-Smollm2.jpeg)

![](/images/2025/DockerMCPToolkit/Models-Local-Smollm2-Chat.jpeg)

#### Docker CLI

```bash
docker model run ai/smollm2
````

### 发布模型

> 这适用于任何支持 OCI Artifacts 的容器注册表，而不仅仅是 Docker Hub。

您可以为现有模型打上新标签并将其发布到不同的命名空间和仓库：

```console
# 为已拉取的模型打上新标签
$ docker model tag ai/smollm2 myorg/smollm2

# 推送到 Docker Hub
$ docker model push myorg/smollm2
````

您还可以直接将 GGUF 格式的模型文件打包为 OCI Artifact 并将其发布到 Docker Hub。

```console
# 下载 GGUF 格式的模型文件，例如从 HuggingFace
$ curl -L -o model.gguf https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf

# 将其打包为 OCI Artifact 并推送到 Docker Hub
$ docker model package --gguf "$(pwd)/model.gguf" --push myorg/mistral-7b-v0.1:Q4_K_M
```

### 示例：将 Docker Model Runner 集成到您的软件开发生命周期中

您现在可以开始构建由 Docker Model Runner 提供支持的生成式 AI 应用程序。

如果您想尝试现有的 GenAI 应用程序，请按照以下说明操作。

1.  设置示例应用程序。克隆并运行以下仓库：

    ```console
    $ git clone https://github.com/docker/hello-genai.git
    ```

2.  在您的终端中，导航到 `hello-genai` 目录。

3.  运行 `run.sh` 以拉取所选模型并运行应用程序：

4.  在浏览器中打开您的应用程序，地址在仓库的 [README](https://github.com/docker/hello-genai) 中指定。

您将看到 GenAI 应用程序的界面，您可以在其中开始输入您的提示。

您现在可以与您自己的 GenAI 应用程序进行交互，该应用程序由本地模型提供支持。尝试一些提示，并注意响应的速度——所有这些都在您的机器上使用 Docker 运行。

**常见问题**

### 有哪些模型可用？

所有可用模型都托管在 [`ai` 的公共 Docker Hub 命名空间](https://hub.docker.com/u/ai)。

### 有哪些 CLI 命令可用？

请参阅[参考文档](https://docs.docker.com/reference/cli/docker/model/)。

### 有哪些 API 端点可用？

启用 OpenAI API

![](/images/2025/DockerMCPToolkit/Settings-BetaFeatures-OpenAIAPI.jpeg)

启用该功能后，新的 API 端点将在以下基本 URL 下可用：

- Docker Desktop
   - 容器：`http://model-runner.docker.internal/`
   - 主机进程：`http://localhost:12434/`

- Docker Engine
   - 从容器：`http://172.17.0.1:12434/`（其中 `172.17.0.1` 表示主机网关地址）
   - 从主机进程：`http://localhost:12434/`

> `172.17.0.1` 接口默认可能不适用于 Compose 项目中的容器。
> 在这种情况下，请在您的 Compose 服务 YAML 中添加 `extra_hosts` 指令：
>
> ```yaml
> extra_hosts:
>   - "model-runner.docker.internal:host-gateway"
> ```
> 然后您可以通过 `http://model-runner.docker.internal:12434/` 访问 Docker Model Runner API。

Docker 模型管理端点：

```text
POST /models/create
GET /models
GET /models/{namespace}/{name}
DELETE /models/{namespace}/{name}
```

OpenAI 端点：

```text
GET /engines/llama.cpp/v1/models
GET /engines/llama.cpp/v1/models/{namespace}/{name}
POST /engines/llama.cpp/v1/chat/completions
POST /engines/llama.cpp/v1/completions
POST /engines/llama.cpp/v1/embeddings
```

要通过 Unix 套接字 (`/var/run/docker.sock`) 调用这些端点，请在其路径前加上 `/exp/vDD4.40`。

> 您可以省略路径中的 `llama.cpp`。例如：`POST /engines/v1/chat/completions`。

### 如何通过 OpenAI API 进行交互？

#### 在容器内部

要从另一个容器内部使用 `curl` 调用 `chat/completions` OpenAI 端点：

```bash
#!/bin/sh

curl http://model-runner.docker.internal/engines/llama.cpp/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "ai/smollm2",
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": "Please write 500 words about the fall of Rome."
            }
        ]
    }'

```

#### 从主机使用 TCP

要通过 TCP 从主机调用 `chat/completions` OpenAI 端点：

1.  从 Docker Desktop GUI 或通过 [Docker Desktop CLI](https://docs.docker.com/desktop/features/desktop-cli/) 启用主机端 TCP 支持。
    例如：`docker desktop enable model-runner --tcp <port>`。

    如果您在 Windows 上运行，还要启用 GPU 加速推理。
    请参阅[启用 Docker Model Runner](https://docs.docker.com/ai/model-runner/#enable-dmr-in-docker-desktop)。

2.  如上一节所述，使用 `localhost` 和正确的端口进行交互。

```bash
#!/bin/sh

	curl http://localhost:12434/engines/llama.cpp/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "ai/smollm2",
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": "Please write 500 words about the fall of Rome."
            }
        ]
    }'
```

![](/images/2025/DockerMCPToolkit/docker-model-command.png)

#### 从主机使用 Unix 套接字

要通过 Docker 套接字从主机使用 `curl` 调用 `chat/completions` OpenAI 端点：

```bash
#!/bin/sh

curl --unix-socket $HOME/.docker/run/docker.sock \
    localhost/exp/vDD4.40/engines/llama.cpp/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "ai/smollm2",
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": "Please write 500 words about the fall of Rome."
            }
        ]
    }'
```


## MCP Toolkit

Docker MCP Toolkit 实现了容器化 MCP 服务器及其与 AI 代理连接的无缝设置、管理和运行。通过提供安全默认配置、一键式设置以及对日益增长的基于 LLM 的客户端生态系统的支持，消除了工具使用障碍。这是从发现 MCP 工具到本地执行的最快捷路径。

![](/images/2025/DockerMCPToolkit/mcp_servers.png)

MCP 服务器目录：

![](/images/2025/DockerMCPToolkit/MCPToolkit-Catalog.jpeg)

运行的 MCP 服务器：

![](/images/2025/DockerMCPToolkit/MCPToolkit-Servers.jpeg)

配置连接到 MCP 服务器的客户端：

![](/images/2025/DockerMCPToolkit/MCPToolkit-Clients.jpeg)


## Ask Gordon
Ask Gordon 是您在 Docker Desktop 和 Docker CLI 中嵌入的个人 AI 助手。它旨在简化您的工作流程，帮助您充分利用 Docker 生态系统。

Docker Desktop 界面：

![](/images/2025/DockerMCPToolkit/AskGordon.jpeg)

![](/images/2025/DockerMCPToolkit/AskGordon-Toolbox-MCPToolkit.jpeg)

![](/images/2025/DockerMCPToolkit/AskGordon-Calling-MCPServer.jpeg)

命令行界面：

![](/images/2025/DockerMCPToolkit/docker-ai.jpeg)

### 什么是 Ask Gordon？

Ask Gordon 在 Docker 工具中提供 AI 驱动的协助。它为以下任务提供上下文帮助：

- 改进 Dockerfile
- 运行和故障排除容器
- 与您的镜像和代码交互
- 查找漏洞或配置问题

它了解您的本地环境，包括源代码、Dockerfile 和镜像，以提供个性化和可操作的指导。

这些功能默认情况下未启用，并且尚未准备用于生产环境。您可能还会遇到术语"Docker AI"作为对此技术的更广泛引用。

#### Gordon 访问哪些数据？

当您使用Ask Gordon 时，它访问的数据取决于您查询的上下文：

- 本地文件：如果您使用 `docker ai` 命令，Ask Gordon 可以访问执行命令的当前工作目录中的文件和目录。在 Docker Desktop 中，如果您在 **Ask Gordon** 视图中询问特定文件或目录，系统会提示您选择相关上下文。
- 本地镜像：Gordon 与 Docker Desktop 集成，可以查看本地镜像存储中的所有镜像。这包括您构建或从注册表拉取的镜像。

为了提供准确的响应，Ask Gordon 可能会将相关文件、目录或镜像元数据与您的查询一起发送到 Gordon 后端。这种数据传输通过网络进行，但从不持久存储或与第三方共享。它仅用于处理您的请求并制定响应。

所有传输的数据在传输过程中都进行了加密。

#### 如何收集和使用您的数据

Docker 从您与 Ask Gordon 的交互中收集匿名数据以增强服务。这包括以下内容：

- 您的查询：您向 Gordon 提出的问题。
- 响应：Gordon 提供的答案。
- 反馈：点赞和点踩评级。

为确保隐私和安全：

- 数据经过匿名化处理，无法追溯到您或您的账户。
- Docker 不使用此数据来训练 AI 模型或与第三方共享。

通过使用Ask Gordon，您帮助改进了 Docker AI 的可靠性和准确性，使其对所有用户更加有效。

如果您对数据收集或使用有疑虑，可以随时禁用此功能。

### 启用 Ask Gordon

1. 登录您的 Docker 账户。
2. 导航到设置中的 **Beta 功能** 选项卡。
3. 选中 **启用 Docker AI** 复选框。

   将显示 Docker AI 服务条款协议。您必须同意条款才能启用此功能。查看条款并选择 **接受并启用** 以继续。

4. 选择 **应用并重启**。

### 使用 Ask Gordon

Docker AI 功能的主要界面是通过 Docker Desktop 中的 **Ask Gordon** 视图，或者如果您更喜欢使用 CLI：`docker ai` CLI 命令。

启用 Docker AI 功能后，您还会在 Docker Desktop 用户界面的其他各个地方找到 **Ask Gordon** 的引用。每当您在用户界面中遇到带有 **星光** (✨) 图标的按钮时，都可以使用该按钮从 Ask Gordon 获得上下文支持。

### 示例工作流程

Ask Gordon 是一个通用 AI 助手，旨在帮助您完成所有与 Docker 相关的任务和工作流程。如果您需要一些灵感，以下是您可以尝试的几种方法：

- 故障排除崩溃的容器
- 获取运行容器的帮助
- 改进 Dockerfile

有关更多示例，请尝试直接 Ask Gordon。例如：

```console
$ docker ai "你能做什么？"
```

#### 故障排除崩溃的容器

如果您尝试使用无效配置或命令启动容器，可以使用 Ask Gordon 来故障排除错误。例如，尝试在不指定数据库密码的情况下启动 Postgres 容器：

```console
$ docker run postgres
Error: Database is uninitialized and superuser password is not specified.
       You must specify POSTGRES_PASSWORD to a non-empty value for the
       superuser. For example, "-e POSTGRES_PASSWORD=password" on "docker run".

       You may also use "POSTGRES_HOST_AUTH_METHOD=trust" to allow all
       connections without a password. This is *not* recommended.

       See PostgreSQL documentation about "trust":
       https://www.postgresql.org/docs/current/auth-trust.html
```

在 Docker Desktop 的 **容器** 视图中，选择容器名称旁边的 ✨ 图标，或检查容器并打开 **Ask Gordon** 选项卡。

#### 获取运行容器的帮助

如果您想运行特定镜像但不确定如何操作，Gordon 可能能够帮助您进行设置：

1. 从 Docker Hub 拉取镜像（例如，`postgres`）。
2. 打开 Docker Desktop 中的 **镜像** 视图并选择镜像。
3. 选择 **运行** 按钮。

在 **运行新容器** 对话框中，您应该看到有关 **Ask Gordon** 的消息。

#### 改进 Dockerfile

Gordon 可以分析您的 Dockerfile 并建议改进。要让 Gordon 使用 `docker ai` 命令评估您的 Dockerfile：

1. 导航到您的项目目录：

   ```console
   $ cd path/to/my/project
   ```

2. 使用 `docker ai` 命令评估您的 Dockerfile：

   ```console
   $ docker ai rate my Dockerfile
   ```

Gordon 将分析您的 Dockerfile 并在几个维度上识别改进机会：

- 构建缓存优化
- 安全性
- 镜像大小效率
- 最佳实践合规性
- 可维护性
- 可重现性
- 可移植性
- 资源效率


## 参考资料
- [MCP Toolkit](https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/)
- [Ask Gordon](https://docs.docker.com/ai/gordon/)
- [Ask Gordon and Model Context Protocol (MCP) – Working Together](https://collabnix.com/ask-gordon-and-model-context-protocol-mcp-working-together/)
