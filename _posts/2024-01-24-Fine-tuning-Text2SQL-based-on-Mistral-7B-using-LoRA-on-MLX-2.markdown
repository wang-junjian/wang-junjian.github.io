---
layout: post
title:  "åœ¨ MLX ä¸Šä½¿ç”¨ LoRA / QLoRA å¾®è°ƒ Text2SQLï¼ˆäºŒï¼‰ï¼šä½¿ç”¨ LoRA åŸºäº Mistral-7B å¾®è°ƒ"
date:   2024-01-24 08:00:00 +0800
categories: MLX Text2SQL
tags: [MLX, LoRA, Mistral-7B, Text2SQL, WikiSQL, MacBookProM2Max]
---

## [mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL](https://huggingface.co/mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL)

æœ¬æ¬¡å¾®è°ƒçš„æ¨¡å‹æˆ‘å·²ç»ä¸Šä¼ åˆ°äº† HuggingFace Hub ä¸Šï¼Œå¤§å®¶å¯ä»¥è¿›è¡Œå°è¯•ã€‚

### å®‰è£… mlx-lm

```bash
pip install mlx-lm
```

### ç”Ÿæˆ SQL
```
python -m mlx_lm.generate --model mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL \
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Which school did Wang Junjian come from?
A: "
```
```
SELECT School FROM Students WHERE Name = 'Wang Junjian'
```


## [åœ¨ MLX ä¸Šä½¿ç”¨ LoRA åŸºäº Mistral-7B å¾®è°ƒ Text2SQLï¼ˆä¸€ï¼‰]({% post_url 2024-01-23-Fine-tuning-Text2SQL-based-on-Mistral-7B-using-LoRA-on-MLX-1 %})

ğŸ“Œ æ²¡æœ‰ä½¿ç”¨æ¨¡å‹çš„æ ‡æ³¨æ ¼å¼ç”Ÿæˆæ•°æ®é›†ï¼Œå¯¼è‡´ä¸èƒ½ç»“æŸï¼Œç›´åˆ°ç”Ÿæˆæœ€å¤§çš„ Tokens æ•°é‡ã€‚

è¿™æ¬¡æˆ‘ä»¬æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

## æ•°æ®é›† WikiSQL

- [WikiSQL](https://github.com/salesforce/WikiSQL)
- [sqllama/sqllama-V0](https://huggingface.co/sqllama/sqllama-V0/blob/main/wikisql.ipynb)

### ä¿®æ”¹è„šæœ¬ mlx-examples/lora/data/wikisql.py
```py
if __name__ == "__main__":
    # ......
    for dataset, name, size in datasets:
        with open(f"data/{name}.jsonl", "w") as fid:
            for e, t in zip(range(size), dataset):
                """
                t å˜é‡çš„æ–‡æœ¬æ˜¯è¿™æ ·çš„ï¼š
                ------------------------
                <s>table: 1-1058787-1
                columns: Approximate Age, Virtues, Psycho Social Crisis, Significant Relationship, Existential Question [ not in citation given ], Examples
                Q: How many significant relationships list Will as a virtue?
                A: SELECT COUNT Significant Relationship FROM 1-1058787-1 WHERE Virtues = 'Will'</s>                
                """
                t = t[3:] # å»æ‰å¼€å¤´çš„ <s>ï¼Œå› ä¸º tokenizer ä¼šè‡ªåŠ¨æ·»åŠ  <s>
                json.dump({"text": t}, fid)
                fid.write("\n")
```

æ‰§è¡Œè„šæœ¬ `data/wikisql.py` ç”Ÿæˆæ•°æ®é›†ã€‚

### æ ·æœ¬ç¤ºä¾‹

```
table: 1-10753917-1
columns: Season, Driver, Team, Engine, Poles, Wins, Podiums, Points, Margin of defeat
Q: Which podiums did the alfa romeo team have?
A: SELECT Podiums FROM 1-10753917-1 WHERE Team = 'Alfa Romeo'</s>
```


## å¾®è°ƒ

- é¢„è®­ç»ƒæ¨¡å‹ [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) 

### LoRA å¾®è°ƒ

```bash
python lora.py --model mistralai/Mistral-7B-v0.1 \
               --train \
               --iters 600
```
```
Loading pretrained model
Total parameters 7243.436M
Trainable parameters 1.704M
Loading datasets
Training
Iter 1: Val loss 2.343, Val took 24.272s
Iter 10: Train loss 2.237, It/sec 0.412, Tokens/sec 165.740
Iter 20: Train loss 1.688, It/sec 0.510, Tokens/sec 206.577
Iter 30: Train loss 1.475, It/sec 0.526, Tokens/sec 216.519
Iter 40: Train loss 1.359, It/sec 0.539, Tokens/sec 208.807
Iter 50: Train loss 1.243, It/sec 0.567, Tokens/sec 225.619
Iter 60: Train loss 1.125, It/sec 0.567, Tokens/sec 224.679
Iter 70: Train loss 1.177, It/sec 0.485, Tokens/sec 196.413
Iter 80: Train loss 1.180, It/sec 0.512, Tokens/sec 205.216
Iter 90: Train loss 1.152, It/sec 0.593, Tokens/sec 224.874
Iter 100: Train loss 1.204, It/sec 0.581, Tokens/sec 221.348
Iter 100: Saved adapter weights to adapters.npz.
Iter 110: Train loss 1.080, It/sec 0.567, Tokens/sec 219.234
Iter 120: Train loss 1.065, It/sec 0.563, Tokens/sec 219.935
Iter 130: Train loss 1.083, It/sec 0.536, Tokens/sec 211.902
Iter 140: Train loss 1.072, It/sec 0.546, Tokens/sec 212.716
Iter 150: Train loss 1.061, It/sec 0.472, Tokens/sec 192.188
Iter 160: Train loss 0.991, It/sec 0.512, Tokens/sec 201.292
Iter 170: Train loss 1.028, It/sec 0.535, Tokens/sec 220.537
Iter 180: Train loss 0.978, It/sec 0.594, Tokens/sec 215.790
Iter 190: Train loss 1.033, It/sec 0.537, Tokens/sec 214.972
Iter 200: Train loss 1.091, It/sec 0.545, Tokens/sec 207.353
Iter 200: Val loss 1.111, Val took 30.101s
Iter 200: Saved adapter weights to adapters.npz.
Iter 210: Train loss 1.056, It/sec 0.573, Tokens/sec 217.968
Iter 220: Train loss 0.987, It/sec 0.552, Tokens/sec 220.129
Iter 230: Train loss 0.984, It/sec 0.578, Tokens/sec 225.119
Iter 240: Train loss 0.929, It/sec 0.593, Tokens/sec 227.224
Iter 250: Train loss 0.984, It/sec 0.504, Tokens/sec 209.164
Iter 260: Train loss 0.871, It/sec 0.529, Tokens/sec 213.830
Iter 270: Train loss 0.843, It/sec 0.549, Tokens/sec 214.504
Iter 280: Train loss 0.866, It/sec 0.606, Tokens/sec 233.129
Iter 290: Train loss 0.946, It/sec 0.564, Tokens/sec 216.089
Iter 300: Train loss 0.818, It/sec 0.574, Tokens/sec 234.182
Iter 300: Saved adapter weights to adapters.npz.
Iter 310: Train loss 0.939, It/sec 0.610, Tokens/sec 228.415
Iter 320: Train loss 0.811, It/sec 0.536, Tokens/sec 208.765
Iter 330: Train loss 0.890, It/sec 0.514, Tokens/sec 207.142
Iter 340: Train loss 0.825, It/sec 0.494, Tokens/sec 190.312
Iter 350: Train loss 0.845, It/sec 0.552, Tokens/sec 211.589
Iter 360: Train loss 0.872, It/sec 0.553, Tokens/sec 221.311
Iter 370: Train loss 0.832, It/sec 0.502, Tokens/sec 205.400
Iter 380: Train loss 0.855, It/sec 0.565, Tokens/sec 217.207
Iter 390: Train loss 0.873, It/sec 0.593, Tokens/sec 229.769
Iter 400: Train loss 0.837, It/sec 0.491, Tokens/sec 207.763
Iter 400: Val loss 1.076, Val took 31.449s
Iter 400: Saved adapter weights to adapters.npz.
Iter 410: Train loss 0.821, It/sec 0.556, Tokens/sec 223.608
Iter 420: Train loss 0.828, It/sec 0.593, Tokens/sec 219.316
Iter 430: Train loss 0.787, It/sec 0.573, Tokens/sec 214.802
Iter 440: Train loss 0.842, It/sec 0.529, Tokens/sec 208.544
Iter 450: Train loss 0.794, It/sec 0.531, Tokens/sec 215.918
Iter 460: Train loss 0.832, It/sec 0.520, Tokens/sec 212.107
Iter 470: Train loss 0.767, It/sec 0.578, Tokens/sec 228.089
Iter 480: Train loss 0.794, It/sec 0.548, Tokens/sec 215.279
Iter 490: Train loss 0.737, It/sec 0.612, Tokens/sec 236.395
Iter 500: Train loss 0.774, It/sec 0.542, Tokens/sec 223.036
Iter 500: Saved adapter weights to adapters.npz.
Iter 510: Train loss 0.750, It/sec 0.524, Tokens/sec 212.472
Iter 520: Train loss 0.636, It/sec 0.562, Tokens/sec 221.322
Iter 530: Train loss 0.587, It/sec 0.541, Tokens/sec 218.441
Iter 540: Train loss 0.631, It/sec 0.589, Tokens/sec 225.624
Iter 550: Train loss 0.661, It/sec 0.580, Tokens/sec 228.000
Iter 560: Train loss 0.686, It/sec 0.537, Tokens/sec 213.582
Iter 570: Train loss 0.630, It/sec 0.543, Tokens/sec 210.104
Iter 580: Train loss 0.632, It/sec 0.588, Tokens/sec 228.862
Iter 590: Train loss 0.632, It/sec 0.517, Tokens/sec 203.740
Iter 600: Train loss 0.609, It/sec 0.531, Tokens/sec 218.118
Iter 600: Val loss 1.001, Val took 30.002s
Iter 600: Saved adapter weights to adapters.npz.
python lora.py --model mistralai/Mistral-7B-v0.1 --train --iters 600  50.58s user 214.71s system 21% cpu 20:26.04 total
```

å¾®è°ƒä¸‡åˆ†ä¹‹ 2.35 ï¼ˆ1.704M / 7243.436M * 10000ï¼‰çš„æ¨¡å‹å‚æ•°ã€‚

LoRA å¾®è°ƒ 600 æ¬¡è¿­ä»£ï¼Œè€—æ—¶ 20 åˆ† 26 ç§’ï¼Œå ç”¨å†…å­˜ 46Gã€‚

| Iteration | Train Loss | Val Loss | Tokens/sec |
| :-------: | ---------: | -------: | ---------: |
| 1         |            | 2.343    |            |
| 100       | 1.204      |          | 221.348    |
| 200       | 1.091      | 1.111    | 207.353    |
| 300       | 0.818      |          | 234.182    |
| 400       | 0.837      | 1.076    | 207.763    |
| 500       | 0.774      |          | 223.036    |
| 600       | 0.609      | 1.001    | 218.118    |


## è¯„ä¼°

è®¡ç®—æµ‹è¯•é›†å›°æƒ‘åº¦ï¼ˆPPLï¼‰å’Œäº¤å‰ç†µæŸå¤±ï¼ˆLossï¼‰ã€‚

```bash
python lora.py --model mistralai/Mistral-7B-v0.1 \
               --adapter-file adapters.npz \
               --test
```
```
Iter 100: Test loss 1.351, Test ppl 3.862.
Iter 200: Test loss 1.327, Test ppl 3.770.
Iter 300: Test loss 1.353, Test ppl 3.869.
Iter 400: Test loss 1.355, Test ppl 3.875.
Iter 500: Test loss 1.294, Test ppl 3.646.
Iter 600: Test loss 1.351, Test ppl 3.863.
```

|     | Iteration | Test Loss | Test PPL |
| --- | :-------: | --------: | -------: |
|     | 100       | 1.351     | 3.862    |
|     | 200       | 1.327     | 3.770    |
|     | 300       | 1.353     | 3.869    |
|     | 400       | 1.355     | 3.875    |
| ğŸ‘  | 500       | 1.294     | 3.646    |
|     | 600       | 1.351     | 3.863    |

è¯„ä¼°å ç”¨å†…å­˜ 26Gã€‚


## èåˆï¼ˆFuseï¼‰

```bash
python fuse.py --model mistralai/Mistral-7B-v0.1 \
               --adapter-file adapters.npz \
               --save-path lora_fused_model
```

è¿™é‡Œä½¿ç”¨äº† `Iter 500` çš„æ¨¡å‹å‚æ•°ï¼Œå› ä¸ºå®ƒçš„æµ‹è¯•é›†å›°æƒ‘åº¦æœ€ä½ã€‚


## ç”Ÿæˆ SQL

### ç‹å†›å»ºçš„å§“åæ˜¯ä»€ä¹ˆï¼Ÿ

```bash
python -m mlx_lm.generate --model lora_fused_model \
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: What is Wang Junjian's name?
A: "
```
```
SELECT Name FROM students WHERE Name = 'Wang Junjian'
==========
Prompt: 88.790 tokens-per-sec
Generation: 16.811 tokens-per-sec
```


### ç‹å†›å»ºçš„å¹´é¾„æ˜¯å¤šå°‘ï¼Ÿ

```bash
python -m mlx_lm.generate --model lora_fused_model \
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: How old is Wang Junjian?
A: "
```
```
SELECT Age FROM Students WHERE Name = 'Wang Junjian'
==========
Prompt: 84.460 tokens-per-sec
Generation: 16.801 tokens-per-sec
```

### ç‹å†›å»ºæ¥è‡ªå“ªæ‰€å­¦æ ¡ï¼Ÿ

```bash
python -m mlx_lm.generate --model lora_fused_model \
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Which school did Wang Junjian come from?
A: "
```
```
SELECT School FROM Students WHERE Name = 'Wang Junjian'
==========
Prompt: 89.124 tokens-per-sec
Generation: 16.718 tokens-per-sec
```

### æŸ¥è¯¢ç‹å†›å»ºçš„å§“åã€å¹´é¾„ã€å­¦æ ¡ä¿¡æ¯ã€‚

```bash
python -m mlx_lm.generate --model lora_fused_model \
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Query Wang Junjianâ€™s name, age, and school information.
A: "
```
```
SELECT Name, Age, School FROM Students WHERE Name = 'Wang Junjian'
==========
Prompt: 100.919 tokens-per-sec
Generation: 17.139 tokens-per-sec
```

### æŸ¥è¯¢ç‹å†›å»ºçš„æ‰€æœ‰ä¿¡æ¯ã€‚

```bash
python -m mlx_lm.generate --model lora_fused_model \
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Query all information about Wang Junjian.
A: "
```
```
SELECT Name FROM students WHERE Name = 'Wang Junjian'
==========
Prompt: 88.225 tokens-per-sec
Generation: 16.781 tokens-per-sec
```

å¯èƒ½è®­ç»ƒæ•°æ®ä¸è¶³ã€‚

### ç»Ÿè®¡ä¸€ä¸‹ä¹å¹´çº§æœ‰å¤šå°‘å­¦ç”Ÿã€‚

```bash
python -m mlx_lm.generate --model lora_fused_model \
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Count how many students there are in ninth grade.
A: "
```
```
SELECT COUNT Name FROM Students WHERE Grade = '9th'
==========
Prompt: 93.829 tokens-per-sec
Generation: 16.546 tokens-per-sec
```

### ç»Ÿè®¡ä¸€ä¸‹ä¹å¹´çº§æœ‰å¤šå°‘å­¦ç”Ÿï¼ˆä¹å¹´çº§çš„å€¼æ˜¯9ï¼‰ã€‚

```bash
python -m mlx_lm.generate --model lora_fused_model \
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
The value for ninth grade is 9.
Q: Count how many students there are in ninth grade.
A: "
```

```bash
python -m mlx_lm.generate --model lora_fused_model \
                          --max-tokens 50 \
                          --prompt "table: students
columns: Name, Age, School, Grade, Height, Weight
Q: Count how many students there are in ninth grade.ï¼ˆThe value for ninth grade is 9.ï¼‰
A: "
```

```
SELECT COUNT Name FROM students WHERE Grade = 9
==========
Prompt: 117.893 tokens-per-sec
Generation: 16.298 tokens-per-sec
```

é™„åŠ çš„æç¤ºä¿¡æ¯å¯ä»¥è½»æ¾æ·»åŠ ï¼Œä¸ç”¨å¤ªåœ¨æ„æ”¾ç½®çš„ä½ç½®ã€‚


## ä¸Šä¼ æ¨¡å‹åˆ° HuggingFace Hub

1. åŠ å…¥ [MLX Community](https://huggingface.co/mlx-community) ç»„ç»‡

2. åœ¨ MLX Community ç»„ç»‡ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ [mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL](https://huggingface.co/mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL)

3. å…‹éš†ä»“åº“ [mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL](https://huggingface.co/mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL)

```bash
git clone https://huggingface.co/mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL
```

4. å°†ç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶ï¼ˆ`lora_fused_model` ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼‰å¤åˆ¶åˆ°ä»“åº“ç›®å½•ä¸‹

5. ä¸Šä¼ æ¨¡å‹åˆ° HuggingFace Hub

```bash
git add .
git commit -m "Fine tuning Text2SQL based on Mistral-7B using LoRA on MLX" 
git push
```

- [å…±äº«é¢„è®­ç»ƒæ¨¡å‹](https://huggingface.co/learn/nlp-course/zh-CN/chapter4/3?fw=pt)

### git push é”™è¯¯

1. ä¸èƒ½ push

é”™è¯¯ä¿¡æ¯ï¼š

```
Uploading LFS objects:   0% (0/2), 0 B | 0 B/s, done.                                                                                                                                                                                              
batch response: Authorization error.
error: failed to push some refs to 'https://huggingface.co/mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL'
```

è§£å†³æ–¹æ³•ï¼š

```bash
vim .git/config
```
```conf
[remote "origin"]
    url = https://wangjunjian:write_token@huggingface.co/mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL
    fetch = +refs/heads/*:refs/remotes/origin/*
```

2. ä¸èƒ½ä¸Šä¼ å¤§äº 5GB çš„æ–‡ä»¶

é”™è¯¯ä¿¡æ¯ï¼š

```
warning: current Git remote contains credentials                                                                                                                                                                                                   
batch response: 
You need to configure your repository to enable upload of files > 5GB.
Run "huggingface-cli lfs-enable-largefiles ./path/to/your/repo" and try again.
```


è§£å†³æ–¹æ³•ï¼š

```bash
huggingface-cli longin
huggingface-cli lfs-enable-largefiles /Users/junjian/HuggingFace/mlx-community/Mistral-7B-v0.1-LoRA-Text2SQL
```

- [Canâ€™t Push to New Space](https://discuss.huggingface.co/t/cant-push-to-new-space/35319)


## å‚è€ƒèµ„æ–™
- [MLX Community](https://huggingface.co/mlx-community)
- [Fine-Tuning with LoRA or QLoRA](https://github.com/ml-explore/mlx-examples/tree/main/lora)
- [Generate Text with LLMs and MLX](https://github.com/ml-explore/mlx-examples/tree/main/llms)
- [Awesome Text2SQL](https://github.com/eosphoros-ai/Awesome-Text2SQL)
- [Awesome Text2SQLï¼ˆä¸­æ–‡ï¼‰](https://github.com/eosphoros-ai/Awesome-Text2SQL/blob/main/README.zh.md)
- [Mistral AI](https://huggingface.co/mistralai)
- [A Beginnerâ€™s Guide to Fine-Tuning Mistral 7B Instruct Model](https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe)
- [Mistral Instruct 7B Finetuning on MedMCQA Dataset](https://saankhya.medium.com/mistral-instruct-7b-finetuning-on-medmcqa-dataset-6ec2532b1ff1)
- [Fine-tuning Mistral on your own data](https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb)
- [mlx-examples llms Mistral](https://github.com/ml-explore/mlx-examples/blob/main/llms/mistral/README.md)
