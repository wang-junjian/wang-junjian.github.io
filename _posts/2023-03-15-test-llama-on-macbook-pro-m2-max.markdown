---
layout: single
title:  "åœ¨ MacBook Pro M2 Max ä¸Šæµ‹è¯• LLaMA"
date:   2023-03-15 08:00:00 +0800
categories: LLaMA
tags: [LLM, GPT, MacBookProM2Max]
---

## [LLaMA](https://github.com/facebookresearch/llama)
LLaMA-13B åœ¨å¤§å¤šæ•°åŸºå‡†ä¸Šçš„è¡¨ç°ä¼˜äº GPT-3ï¼ˆ175Bï¼‰ï¼ŒLLaMA-65B ä¸æœ€å¥½çš„å‹å· Chinchilla-70B å’Œ PaLM-540B å…·æœ‰ç«äº‰åŠ›ã€‚
* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1)

### å…‹éš†
```shell
git clone https://github.com/facebookresearch/llama
cd llama
```

### ä¸‹è½½æ¨¡å‹
ä¿®æ”¹ download.shï¼Œé…ç½®ä¸‹è½½æ¨¡å‹çš„ `åœ°å€(PRESIGNED_URL)` å’Œ `ä¸‹è½½ç›®å½•(TARGET_FOLDER)`ã€‚
```shell
vim download.sh
```
```
PRESIGNED_URL="https://agi.gpt4.org/llama/LLaMA/*"             # replace with presigned url from email
TARGET_FOLDER="./"             # where all files should end up
```

```shell
bash download.sh
```

## [llama.cpp](https://github.com/ggerganov/llama.cpp)

### æ„å»º
```shell
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
```

### æ‹·è´ LLaMA æ¨¡å‹åˆ°å½“å‰ç›®å½•
```shell
ls ./models
65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model
```

### å®‰è£… Python ä¾èµ–
```shell
pip install torch numpy sentencepiece
```

### convert the 7B model to ggml FP16 format
```shell
python convert-pth-to-ggml.py models/7B/ 1
```

### quantize the model to 4-bits
```shell
./quantize.sh 7B
```

### æ¨ç†
```shell
./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 128
```
```
main: seed = 1678918444
llama_model_load: loading model from './models/7B/ggml-model-q4_0.bin' - please wait ...
llama_model_load: n_vocab = 32000
llama_model_load: n_ctx   = 512
llama_model_load: n_embd  = 4096
llama_model_load: n_mult  = 256
llama_model_load: n_head  = 32
llama_model_load: n_layer = 32
llama_model_load: n_rot   = 128
llama_model_load: f16     = 2
llama_model_load: n_ff    = 11008
llama_model_load: n_parts = 1
llama_model_load: ggml ctx size = 4529.34 MB
llama_model_load: memory_size =   512.00 MB, n_mem = 16384
llama_model_load: loading model part 1/1 from './models/7B/ggml-model-q4_0.bin'
llama_model_load: .................................... done
llama_model_load: model size =  4017.27 MB / num tensors = 291

system_info: n_threads = 8 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | 

main: prompt: 'She'
main: number of tokens in prompt = 2
     1 -> ''
 13468 -> 'She'

sampling parameters: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000


Shepherds Bush International station was once again the destination of choice for our Sunday afternoon outing, following another successful event on Saturday. We split into two groups this time to explore what is possibly London's most exciting and vibrant district: Shepherdâ€™s Market (it actually sounds a lot like Soho) lies just north-east from the station itself with some of its oldest buildings dating back over 200 years. The market has been in continuous existence since at least the early part of this century â€“ when it was called 'The Pig Alley' - and it is still home to a

main: mem per token = 14434244 bytes
main:     load time =   671.87 ms
main:   sample time =   113.01 ms
main:  predict time =  5871.94 ms / 45.52 ms per token
main:    total time =  6939.42 ms
```
* å†…å­˜ 4.1G

### äº¤äº’æ¨¡å¼
```shell
./main -m ./models/7B/ggml-model-q4_0.bin -t 8 -n 256 --repeat_penalty 1.0 --color -i -r "User:" -p \
"Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.

User: Hello, Bob.
Bob: Hello. How may I help you today?
User: Please tell me the largest city in Europe.
Bob: Sure. The largest city in Europe is Moscow, the capital of Russia.
User:"
```

### æµ‹è¯•è¯¦æƒ…

| æ¨¡å‹  | å¤§å°  | é‡åŒ–(4ä½) | å†…å­˜   |
| :--: | ----: | -------: | ----: |
|  7B  |  13G  |     3.9G |  4.0G |
| 13B  |  24G  |     7.6G |  7.8G |
| 30B  |  61G  |      19G | 19.4G |
| 65B  | 122G  |      38G | 38.5G |


## GGUF æ ¼å¼çš„å¤§æ¨¡å‹

### GGUF ä»‹ç»

GGUF æ˜¯ä¸€ç§äºŒè¿›åˆ¶æ ¼å¼ï¼Œæ—¨åœ¨å¿«é€ŸåŠ è½½å’Œä¿å­˜æ¨¡å‹ã€‚å®ƒæ˜¯ GGMLã€GGMF å’Œ GGJT çš„åç»§æ–‡ä»¶æ ¼å¼ï¼Œé€šè¿‡åŒ…å«åŠ è½½æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯æ¥ç¡®ä¿æ˜ç¡®æ€§ã€‚ å®ƒè¿˜è¢«è®¾è®¡ä¸ºå¯æ‰©å±•çš„ï¼Œä»¥ä¾¿å¯ä»¥åœ¨ä¸ç ´åå…¼å®¹æ€§çš„æƒ…å†µä¸‹å°†æ–°ä¿¡æ¯æ·»åŠ åˆ°æ¨¡å‹ä¸­ã€‚
- GGMLï¼ˆæ— ç‰ˆæœ¬ï¼‰ï¼šåŸºçº¿æ ¼å¼ï¼Œæ²¡æœ‰ç‰ˆæœ¬æ§åˆ¶æˆ–å¯¹é½ã€‚
- GGMFï¼ˆç‰ˆæœ¬åŒ–ï¼‰ï¼šä¸ GGML ç›¸åŒï¼Œä½†å…·æœ‰ç‰ˆæœ¬åŒ–ã€‚ 
- GGJTï¼šå¯¹é½å¼ é‡ä»¥å…è®¸ä¸éœ€è¦å¯¹é½çš„ mmap ä¸€èµ·ä½¿ç”¨ã€‚ v1ã€v2 å’Œ v3 ç›¸åŒï¼Œä½†åé¢çš„ç‰ˆæœ¬ä½¿ç”¨ä¸ä»¥å‰ç‰ˆæœ¬ä¸å…¼å®¹çš„ä¸åŒé‡åŒ–æ–¹æ¡ˆã€‚

[What is GGUF and GGML?](https://medium.com/@phillipgimmi/what-is-gguf-and-ggml-e364834d241c)

### ä¸‹è½½ GGUF æ¨¡å‹ï¼ˆHuggingFace [TheBloke](https://huggingface.co/TheBloke) ä»“åº“ï¼‰

```shell
REPO_ID=TheBloke/CodeLlama-7B-GGUF
FILENAME=codellama-7b.Q4_K_M.gguf
```

#### [huggingface-cli](https://huggingface.co/docs/huggingface_hub/guides/cli)
```shell
pip install "huggingface_hub[cli]"
```

```shell
huggingface-cli download ${REPO_ID} ${FILENAME} \
    --local-dir . --local-dir-use-symlinks False
```

#### wget
```shell
wget https://huggingface.co/${REPO_ID}/resolve/main/${FILENAME}\?download\=true -O ${FILENAME}
```

### æ¨¡å‹è½¬æ¢ä¸º GGUF

â¶ ç¼–è¯‘ llama.cpp
```shell
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

make -j

pip install -r requirements.txt
```

â· è½¬æ¢ä¸º `GGUF` æ¨¡å‹
```python
python convert.py vicuna-7b-v1.5 \
    --outfile vicuna-7b-v1.5.gguf \
    --outtype q8_0
```

### Llama2 é‡åŒ–æ€§èƒ½

| Model | Measure | F16 | Q2_K | Q3_K_M | Q4_K_S | Q5_K_S | Q6_K |
| ----: | ------- | --: | ---: | -----: | -----: | -----: | ---: |
|    7B | perplexity             | 5.9066 | 6.7764 | 6.1503 | 6.0215 | 5.9419 | 5.9110 |
|    7B | file size              |  13.0G |  2.67G |  3.06G |  3.56G |  4.33G |  5.15G |
|    7B | ms/tok @ 4th, M2 Max   |    116 |     56 |     69 |     50 |     70 |     75 |
|    7B | ms/tok @ 8th, M2 Max   |    111 |     36 |     36 |     36 |     44 |     51 |
|    7B | ms/tok @ 4th, RTX-4080 |     60 |   15.5 |   17.0 |   15.5 |   16.7 |   18.3 |
|    7B | ms/tok @ 4th, Ryzen    |    214 |     57 |     61 |     68 |     81 |     93 |
|   13B | perplexity             | 5.2543 | 5.8545 | 5.4498 | 5.3404 | 5.2785 | 5.2568 |
|   13B | file size              |  25.0G |  5.13G |  5.88G |  6.80G |  8.36G |  9.95G |
|   13B | ms/tok @ 4th, M2 Max   |    216 |    103 |    148 |     95 |    132 |    142 |
|   13B | ms/tok @ 8th, M2 Max   |    213 |     67 |     77 |     68 |     81 |     95 |
|   13B | ms/tok @ 4th, RTX-4080 |      - |   25.3 |   29.3 |   26.2 |   28.6 |   30.0 |
|   13B | ms/tok @ 4th, Ryzen    |    414 |    109 |    118 |    130 |    156 |    180 |

[k-quants](https://github.com/ggerganov/llama.cpp/pull/1684)

å›°æƒ‘åº¦(Perplexity, PPL)æ˜¯ä¸€ç§ç”¨æ¥è¯„ä»·è¯­è¨€æ¨¡å‹å¥½åçš„æŒ‡æ ‡ã€‚

ç›´è§‚ä¸Šç†è§£ï¼Œå½“æˆ‘ä»¬ç»™å®šä¸€æ®µéå¸¸æ ‡å‡†çš„ï¼Œé«˜è´¨é‡çš„ï¼Œç¬¦åˆäººç±»è‡ªç„¶è¯­è¨€ä¹ æƒ¯çš„æ–‡æ¡£ä½œä¸ºæµ‹è¯•é›†æ—¶ï¼Œæ¨¡å‹ç”Ÿæˆè¿™æ®µæ–‡æœ¬çš„æ¦‚ç‡è¶Šé«˜ï¼Œå°±è®¤ä¸ºæ¨¡å‹çš„å›°æƒ‘åº¦è¶Šå°ï¼Œæ¨¡å‹ä¹Ÿå°±è¶Šå¥½ã€‚

[LLMè¯„ä¼°æŒ‡æ ‡å›°æƒ‘åº¦(perplexity)çš„ç†è§£](https://zhuanlan.zhihu.com/p/651410752)

### ä» [TheBloke](https://huggingface.co/TheBloke) ä¸‹è½½å·²è½¬æ¢å’Œé‡åŒ–çš„ [Meta Llama 2 æ¨¡å‹](https://huggingface.co/meta-llama)
- [Llama 2 7B base](https://huggingface.co/TheBloke/Llama-2-7B-GGUF)
- [Llama 2 13B base](https://huggingface.co/TheBloke/Llama-2-13B-GGUF)
- [Llama 2 70B base](https://huggingface.co/TheBloke/Llama-2-70B-GGUF)
- [Llama 2 7B chat](https://huggingface.co/TheBloke/Llama-2-7B-chat-GGUF)
- [Llama 2 13B chat](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF)
- [Llama 2 70B chat](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF)

### èŠå¤©
```shell
./main -n 1000 -e -m llama-2-7b-chat.Q4_K_M.gguf -p "ç³–æœçš„åˆ¶ä½œæ­¥éª¤"
```
```
 ç³–æœçš„åˆ¶ä½œæ­¥éª¤

1. é€‰æ‹©ä¼˜è´¨çš„ç³–æœï¼šé€‰æ‹©é«˜è´¨é‡çš„ç³–æœï¼Œå¯ä»¥å¢åŠ ç³–æœçš„ç²¾åº¦å’Œçƒ˜åŸ¹è´¨åœ°ã€‚
2. å°†ç³–æœéš”å¼€ï¼šå°†ç³–æœæŒ‰ç…§å¤§å°å’Œå½¢çŠ¶åˆ†æˆä¸åŒçš„é¢œè‰²ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½åœ°æ§åˆ¶ç³–æœçš„æ‰è½é€Ÿåº¦å’Œåå¡Œæƒ…å†µã€‚
3. æ·‹ä¸Šç³–æœï¼šå°†ç³–æœæ·‹åœ¨æ¿å­ä¸Šï¼Œç¡®ä¿æ¯ä¸ªç³–æœéƒ½å¤Ÿå¥½åœ°æ·‹åœ¨æ¿å­ä¸Šï¼Œè¿™æ ·å¯ä»¥å‡å°‘ç³–æœçš„è½å¡Œå’ŒæŸåã€‚
4. å‡åŒ€åˆ†é…ï¼šå°†ç³–æœå‡åŒ€åˆ†é…åˆ°æ¿å­ä¸Šï¼Œç¡®ä¿æ¯ä¸ªç³–æœéƒ½æœ‰ç›¸åŒçš„å¤§å°å’Œå½¢çŠ¶ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½åœ°æ§åˆ¶ç³–æœçš„æ‰è½é€Ÿåº¦å’Œåå¡Œæƒ…å†µã€‚
5. çƒ˜åŸ¹ï¼šå°†æ·‹ä¸Šçš„ç³–æœæ™’åœ¨çƒ˜åŸ¹æœºä¸­ï¼Œè®¾ç½®æ­£ç¡®çš„æ—¶é—´å’Œæ¸©åº¦ï¼Œä»¥ä¾¿ç³–æœèƒ½å¤Ÿå®Œå…¨çƒ˜åŸ¹ã€‚
6. å†»ç»“ï¼šå°†çƒ˜åŸ¹åçš„ç³–æœå†»ç»“åœ¨å†°ç®±ä¸­ï¼Œä»¥ä¾¿ä¿å­˜å’Œä½¿ç”¨ã€‚
7. é¢„è§ˆï¼šå¯ä»¥é€šè¿‡æ£€æŸ¥ç³–æœçš„é¢œè‰²ã€å½¢çŠ¶å’Œè´¨åœ°æ¥é¢„è§ˆç³–æœçš„åˆ¶ä½œç»“æœã€‚
8. ä¿®æ­£ï¼šå¦‚æœå‘ç°ç³–æœçš„é¢œè‰²æˆ–å½¢çŠ¶ä¸åŒ¹é…ï¼Œå¯ä»¥é€šè¿‡ä¿®æ­£ç³–æœçš„çƒ˜åŸ¹æ—¶é—´å’Œæ¸©åº¦æ¥å®ç°ä¿®æ­£ã€‚
```

## [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) [ğŸ“œ](https://llama-cpp-python.readthedocs.io/en/latest/)

### ä½¿ç”¨ Metal (MPS) è¿›è¡Œå®‰è£…
```shell
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
pip install fastapi uvicorn sse-starlette pydantic-settings starlette-context
```

### OpenAI å…¼å®¹çš„ Web æœåŠ¡å™¨
```shell
python -m llama_cpp.server --model llama-2-7b-chat.Q4_K_M.gguf --model_alias Llama-2-7B-chat
```
```
--model MODEL     The path to the model to use for generating completions. (default: PydanticUndefined)
--model_alias MODEL_ALIAS The alias of the model to use for generating completions.
--n_ctx N_CTX     The context size. (default: 2048)
--host HOST       Listen address (default: localhost)
--port PORT       Listen port (default: 8000)
```

### [OpenAPI æ–‡æ¡£](http://localhost:8000/docs)
- POST /v1/completions
- POST /v1/embeddings
- POST /v1/chat/completions
- GET /v1/models

### æ›´å¤šåŠŸèƒ½
- [Local Copilot replacement](https://llama-cpp-python.readthedocs.io/en/latest/server/#code-completion)
- [Function Calling support](https://llama-cpp-python.readthedocs.io/en/latest/server/#function-calling)
- [Vision API support](https://llama-cpp-python.readthedocs.io/en/latest/server/#multimodal-models)

### curl è°ƒç”¨ API
`POST` `/v1/completions`
```shell
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "prompt": "ä¸­å›½çš„é¦–éƒ½æ˜¯ï¼Ÿ",
        "temperature": 0.3
    }'
```

`POST` `/v1/chat/completions`
```shell
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{ 
        "messages": [ 
            { "role": "user", "content": "ä¸­å›½çš„é¦–éƒ½æ˜¯ï¼Ÿ" }
        ]
    }'
```


## å‚è€ƒèµ„æ–™
- [ChatLLaMA](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama)
- [nebullvm](https://github.com/nebuly-ai/nebullvm)
* [LLaMA](https://github.com/facebookresearch/llama)
* [llama.cpp](https://github.com/ggerganov/llama.cpp)
* [Using LLaMA with M1 Mac](https://dev.l1x.be/posts/2023/03/12/using-llama-with-m1-mac/)
* [Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp](https://til.simonwillison.net/llms/llama-7b-m2)
* [Simon Willison: TIL](https://til.simonwillison.net/)
* [k-quants](https://github.com/ggerganov/llama.cpp/pull/1684)
* [Variable bit rate quantization #1256](https://github.com/ggerganov/llama.cpp/issues/1256)
* [QX_4 quantization #1240](https://github.com/ggerganov/llama.cpp/issues/1240)
