---
layout: single
title:  "FastChat éƒ¨ç½²å¤šæ¨¡å‹"
date:   2023-10-24 08:00:00 +0800
categories: FastChat
tags: [FastChat, LangChain, Vicuna, ChatGLM2-6B, åˆ†å¸ƒå¼]
---

<hr>
* [Chatbot Arena](https://chat.lmsys.org/)
* [FastChat](https://github.com/lm-sys/FastChat)
* [LMSYS BLOG](https://lmsys.org/blog/)
* [Use AutoGen for Local LLMs](https://microsoft.github.io/autogen/blog/2023/07/14/Local-LLMs/)

## å®‰è£…
### pip
```shell
pip install "fschat[model_worker,webui]"
```

### æºä»£ç 

**è¿™ç§æ–¹å¼å®‰è£…æ¯”è¾ƒå®¹æ˜“è°ƒè¯•ï¼Œé€‚åˆå¼€å‘è€…ã€‚**

å…‹éš†ä»£ç 

```shell
git clone https://github.com/lm-sys/FastChat.git
cd FastChat
```

åˆ›å»ºç¯å¢ƒ

```shell
python -m venv env
source env/bin/activate
```

å®‰è£…

```shell
pip install --upgrade pip  # enable PEP 660 support
pip install -e ".[model_worker,webui]"
pip install -U transformers==4.33.0 # AttributeError: 'ChatGLMTokenizer' object has no attribute 'tokenizer'
```

* [AttributeError: 'ChatGLMTokenizer' object has no attribute 'tokenizer'](https://huggingface.co/THUDM/chatglm2-6b/discussions/87)


## æ¨¡å‹
### vicuna-7b-v1.5

[HuggingFace vicuna-7b-v1.5](https://huggingface.co/lmsys/vicuna-7b-v1.5)

æŒ‡å®š HuggingFace æ¨¡å‹åç§°ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ã€‚

```shell
python -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5 --device cpu
```

[ModelScope vicuna-7b-v1.5](https://www.modelscope.cn/models/AI-ModelScope/vicuna-7b-v1.5/)

HuggingFace åœ¨å›½å†…ä¸èƒ½è®¿é—®ï¼Œå¯ä»¥ä½¿ç”¨ ModelScope çš„é•œåƒã€‚

```shell
git clone https://modelscope.cn/AI-ModelScope/vicuna-7b-v1.5.git
```

* [ModelScope æ¨¡å‹çš„ä¸‹è½½](https://www.modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD)

### ChatGLM2-6B

ç›´æ¥ä» HuggingFace ä¸‹è½½æ¨¡å‹

```shell
git clone https://huggingface.co/THUDM/chatglm2-6b
```


## æ¨ç†
### å‘½ä»¤è¡Œ

```shell
python3 -m fastchat.serve.cli --model-path chatglm2-6b --device cpu
```


## éƒ¨ç½²æœåŠ¡

### FastChat Server æ¶æ„å›¾

![](/images/2023/fastchat/fastchat-server-architecture.png)


### åˆ†æ­¥éƒ¨ç½²
#### å¯åŠ¨ Controller

```shell
python -m fastchat.serve.controller
```

#### å¯åŠ¨ Model Worker

å¯ä»¥éƒ¨ç½²å¤šä¸ªï¼ŒåŒ…æ‹¬ç›¸åŒçš„æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥éƒ¨ç½²ä¸åŒçš„æ¨¡å‹ã€‚`--gpus` å‚æ•°å‡ºé”™ï¼Œè¿˜æœªè§£å†³ï¼Œæ‰€ä»¥è¿™é‡Œåœ¨å‘½ä»¤å‰ä½¿ç”¨äº†ç¯å¢ƒå˜é‡ `CUDA_VISIBLE_DEVICES`ã€‚

```shell
CUDA_VISIBLE_DEVICES=2 python -m fastchat.serve.model_worker --model-path chatglm2-6b --port 21002 --worker-address http://localhost:21002
CUDA_VISIBLE_DEVICES=3 python -m fastchat.serve.model_worker --model-path chatglm2-6b --port 21003 --worker-address http://localhost:21003
python -m fastchat.serve.model_worker --model-names "gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002" --model-path vicuna-7b-v1.5 --device cpu --port 21004 --worker-address http://localhost:21004
```

å¦‚æœä½¿ç”¨äº†ä¸‹é¢çš„ `ä»£ç ä¿®æ”¹` åï¼Œå¯ä»¥ä¸ç”¨æŒ‡å®š `--worker-address` å‚æ•°ã€‚

```shell
CUDA_VISIBLE_DEVICES=2 python -m fastchat.serve.model_worker --model-path chatglm2-6b --port 21002
CUDA_VISIBLE_DEVICES=3 python -m fastchat.serve.model_worker --model-path chatglm2-6b --port 21003
python -m fastchat.serve.model_worker --model-names "gpt-3.5-turbo,text-davinci-003,text-embedding-ada-002" --model-path vicuna-7b-v1.5 --device cpu --port 21004
```

**vicuna-7b-v1.5 ç”¨äº† 26G çš„å†…å­˜**

8ä½é‡åŒ–æ¨¡å‹ï¼Œéœ€è¦æ·»åŠ å‚æ•° `--load-8bit`ã€‚

```shell
python -m fastchat.serve.model_worker \
    --model-path chatglm2-6b --port 21004 \
    --worker-address http://localhost:21004 \
    --load-8bit
```

æ˜¾å­˜ä½¿ç”¨æƒ…å†µ

- **åˆå§‹ï¼š** 6446MiB / 15360MiB
- **æ˜¾å­˜ï¼š** 7458MiB / 15360MiB
- **Volatile GPU-Utilï¼š** 100%


#### å¯åŠ¨ OpenAI API Server
å¯ä»¥ç”¨äºå’Œ LangChain é›†æˆã€‚

```shell
python -m fastchat.serve.openai_api_server --host 0.0.0.0 --port 8000
```

#### æµ‹è¯•

* models æ¥å£

```shell
curl http://localhost:8000/v1/models | jq
```

* completions æ¥å£

```shell
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "chatglm2-6b",
        "prompt": "ä½ å¥½",
        "temperature": 0.3
    }' | jq
```

```json
{
  "id": "cmpl-3LSrp7xjZmCxuqUqCHdweR",
  "object": "text_completion",
  "created": 1699926717,
  "model": "chatglm2-6b",
  "choices": [
    {
      "index": 0,
      "text": "ï¼Œæˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚ æ ¹æ®ä½ çš„æè¿°ï¼Œä½ æƒ³è¦æŸ¥è¯¢å…³äºâ€œäººå·¥æ™ºèƒ½â€",
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 4,
    "total_tokens": 20,
    "completion_tokens": 16
  }
}
```

* chat/completions æ¥å£

```shell
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "chatglm2-6b",
        "messages": [{"role": "user", "content": "ä½ å¥½"}],
        "temperature": 0.3
    }' | jq
```

```json
{
  "id": "chatcmpl-3vzgGYm2QVSnDJqrLSe6Hp",
  "object": "chat.completion",
  "created": 1699926765,
  "model": "chatglm2-6b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 17,
    "total_tokens": 45,
    "completion_tokens": 28
  }
}
```

* embeddings æ¥å£

```shell
curl http://localhost:8000/v1/embeddings \
    -H "Content-Type: application/json" \
    -d '{
        "input": "ç”¨äºç”ŸæˆåµŒå…¥æ–‡æœ¬çš„å­—ç¬¦ä¸²ã€‚",
        "model": "chatglm2-6b"
    }' | jq
```

```json
{
    "object": "list",
    "data": [
        {
        "object": "embedding",
        "embedding": [
            -0.0038084269035607576,
            0.0007477423641830683,
            ......
            -0.002232820028439164,
            0.0010205521248281002
        ],
        "index": 0
        }
    ],
    "model": "chatglm2-6b",
    "usage": {
        "prompt_tokens": 11,
        "total_tokens": 11
    }
}
```

### ä¸€é”®éƒ¨ç½²

#### ä»£ç ä¿®æ”¹

è¿™ä¸ªé¡¹ç›®éƒ¨ç½²èµ·æ¥å¤ªéº»çƒ¦äº†ï¼Œéœ€è¦ä¿®æ”¹ä»£ç ï¼Œæ‰èƒ½éƒ¨ç½²æˆåŠŸã€‚æˆ‘è¿™é‡Œä¸»è¦æ˜¯æƒ³å®ç°ï¼š
1. éƒ¨ç½²å¤šä¸ª ChatGLM2-6B æ¨¡å‹
2. æ¯ä¸ªæ¨¡å‹éƒ¨ç½²åœ¨ä¸åŒçš„ GPU ä¸Š

[fastchat/serve/model_worker.py](https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/model_worker.py)

å‚æ•° `--worker-address` ä½¿ç”¨äº†é»˜è®¤å€¼ï¼Œå¹¶ä¸ä¼šè·Ÿç€ `--host` å’Œ `--port` çš„æ”¹å˜è€Œæ”¹å˜ã€‚

```py
parser.add_argument("--worker-address", type=str, default="http://localhost:21002")
```

```py
def create_model_worker():
    #......
    args = parser.parse_args()
    args.worker_address = f"http://localhost:{args.port}" # å¢åŠ ä»£ç 
    #......
```

[fastchat/serve/launch_all_serve.py](https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/launch_all_serve.py)

```py
# å¢åŠ äº† gpus å‚æ•°
def launch_worker(item, gpus=None):
    #......
    args.model_path, args.worker_host, args.worker_port = item.split("@")
    if gpus: # ç»™ args.gpus èµ‹å€¼
        args.gpus = gpus # è¿™é‡Œç›´æ¥ä½¿ç”¨ 2 æˆ– 3 ä¼šæŠ¥é”™ï¼Œéœ€è¦ä½¿ç”¨ 2,3ï¼Œå’Œ fastchat.serve.model_worker ä¸­çš„ä»£ç æœ‰å…³
    #......

def _gpus_sequence_adjust(gpus, first_gpu):
    """
    gpus é¡ºåºè°ƒæ•´ï¼ŒæŠŠ first_gpu æ”¾åœ¨ç¬¬ä¸€ä¸ªä½ç½®ã€‚
    å¦‚ï¼šgpus=0,1,2 first_gpu=1
        gpus = _gpus_sequence_adjust(gpus, first_gpu)
    è°ƒæ•´åï¼šgpus=1,0,2
    """
    gpus = gpus.split(',')
    gpus.remove(first_gpu)
    gpus = [first_gpu] + gpus
    return ','.join(gpus)

def launch_all():
    #......

    if isinstance(args.model_path_address, str):
        launch_worker(args.model_path_address)
    else:
        gpus = args.gpus.split(',')
        if len(args.model_path_address) == len(gpus):
            # æ¯ä¸ªæ¨¡å‹éƒ¨ç½²åœ¨ä¸åŒçš„ GPU ä¸Š
            for model_path_address, gpu in zip(args.model_path_address, gpus):
                print(f"loading model:{model_path_address}, gpu id:{gpu}")
                launch_worker(model_path_address, _gpus_sequence_adjust(args.gpus, gpu))
        else:
            for idx, item in enumerate(args.model_path_address):
                print(f"loading {idx}th model:{item}")
                launch_worker(item)
    #......
```

#### éƒ¨ç½²

`model_path_address æ ¼å¼ä¸ºï¼šæ¨¡å‹è·¯å¾„@æ¨¡å‹åœ°å€@æ¨¡å‹ç«¯å£`

```shell
python -m fastchat.serve.launch_all_serve \
    --controller-host localhost --controller-port 21001 \
    --model-path-address chatglm2-6b@localhost@21002 chatglm2-6b@localhost@21003 \
    --gpus 2,3 \
    --limit-worker-concurrency 10 \
    --server-host 0.0.0.0 --server-port 8000
```

### é«˜çº§é…ç½®
#### ç¯å¢ƒå˜é‡

* è¶…æ—¶è®¾ç½®ï¼ˆTimeoutï¼‰ï¼šå¦‚æœé‡åˆ°è¶…æ—¶é”™è¯¯ï¼Œå¯ä»¥è°ƒæ•´è¶…æ—¶æ—¶é•¿ã€‚
```shell
export FASTCHAT_WORKER_API_TIMEOUT=<larger timeout in seconds>
```

* æ‰¹é‡å¤§å°ï¼ˆBatch Sizeï¼‰ï¼šå¦‚æœé‡åˆ°å†…å­˜ä¸è¶³ (OOM) é”™è¯¯ï¼Œå¯ä»¥è®¾ç½®è¾ƒå°çš„æ‰¹é‡å¤§å°ã€‚
```shell
export FASTCHAT_WORKER_API_EMBEDDING_BATCH_SIZE=1
```

* [Vicuna LLM: Why It's the Next Big Thing in LocalLLM](https://cheatsheet.md/llm-leaderboard/vicuna-llm)

#### å‚æ•°

* --model-path æœ¬åœ°æ–‡ä»¶å¤¹æˆ– Hugging Face æ¨¡å‹åç§°ã€‚ï¼ˆcan be a local folder or a Hugging Face repo name.ï¼‰
* --limit-worker-concurrency é™åˆ¶æ¯ä¸ªæ¨¡å‹çš„å¹¶å‘æ•°ï¼Œé»˜è®¤ï¼š5

## LLM.int8 æ··åˆç²¾åº¦é‡åŒ–
### å®‰è£…ä¾èµ–åŒ…
```shell
pip install bitsandbytes
pip install accelerate
pip install scipy
```

### ä¿®æ”¹ä»£ç  `fastchat/model/model_adapter.py`

- FastChat 0.2.32

```py
 315     if (device == "cuda" and num_gpus == 1 and not cpu_offloading) or device in (
 316         "mps",
 317         "xpu",
 318         "npu",
 319     ):
 320         pass #model.to(device)

 733         model = AutoModel.from_pretrained(
 734             model_path, trust_remote_code=True, load_in_8bit=True, **from_pretrained_kwargs
 735         )   
```

- FastChat 0.2.33

```py
 321     if (device == "cuda" and num_gpus == 1 and not cpu_offloading) or device in (
 322         "mps",
 323         "xpu",
 324         "npu",
 325     ):
 326         pass #model.to(device)                                                                 # wjj


 749 class ChatGLMAdapter(BaseModelAdapter):
 750     """The model adapter for THUDM/chatglm-6b, THUDM/chatglm2-6b"""
 751 
 752     def match(self, model_path: str):
 753         return "chatglm" in model_path.lower()
 754 
 755     def load_model(self, model_path: str, from_pretrained_kwargs: dict):
 756         revision = from_pretrained_kwargs.get("revision", "main")
 757         if "chatglm3" in model_path.lower():
 758             tokenizer = AutoTokenizer.from_pretrained(
 759                 model_path,
 760                 encode_special_tokens=True,
 761                 trust_remote_code=True,
 762                 revision=revision,
 763             )
 764         else:
 765             tokenizer = AutoTokenizer.from_pretrained(
 766                 model_path, trust_remote_code=True, revision=revision
 767             )
 768         model = AutoModel.from_pretrained(
 769             model_path, trust_remote_code=True, load_in_8bit=True, **from_pretrained_kwargs    # wjj
 770         )
 771         return model, tokenizer
```

### æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
- **åˆå§‹ï¼š** 7404MiB / 15360MiB
- **æ˜¾å­˜ï¼š** 7658MiB / 15360MiB
- **Volatile GPU-Utilï¼š** 40%

**æ„Ÿè§‰æ•ˆæœæ¯”ä¸Šé¢çš„ `--load-8bit` é‡åŒ–å¥½ä¸€äº›ï¼Œä¸è¿‡è¦ä¿®æ”¹æºä»£ç ã€‚**

## vLLM é›†æˆ

* [vLLM Quickstart](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html)
* [vLLM Integration](https://github.com/lm-sys/FastChat/blob/main/docs/vllm_integration.md)

### [å®‰è£…](https://vllm.readthedocs.io/en/latest/getting_started/installation.html)
```shell
pip install vllm
```

### å¯åŠ¨ vLLM Worker
```shell
python -m fastchat.serve.vllm_worker --model-path vicuna-7b-v1.5
```

FastChat çš„ç‰ˆæœ¬æ›´æ–°ä¸åŠæ—¶ï¼ŒvLLM æ–°ç‰ˆæœ¬ä¸­å·²ç»åˆ é™¤äº† AsyncLLMEngineï¼Œæ‰€ä»¥ä¼šæŠ¥é”™ï¼š

```shell
    from vllm import AsyncLLMEngine
ImportError: cannot import name 'AsyncLLMEngine' from 'vllm' (unknown location)
```
* [vLLM v0.2.0](https://github.com/vllm-project/vllm/releases/tag/v0.2.0)


## LangChain é›†æˆ

FastChat API æœåŠ¡å™¨å¯ä»¥é€šè¿‡ OpenAI API åè®®ä¸åŸºäº OpenAI API çš„åº”ç”¨ç¨‹åºè¿›è¡Œäº¤äº’ã€‚è¿™æ„å‘³ç€å¼€æ”¾æ¨¡å‹å¯ä»¥ç”¨ä½œæ›¿ä»£å“ï¼Œè€Œæ— éœ€ä¿®æ”¹ä»£ç ã€‚

### OpenAI å…¼å®¹ API

![](/images/2023/fastchat/openai-compatible-api-overview.png)

### é›†æˆ

```py
from langchain.llms import OpenAIChat
llm = OpenAIChat(model_name="chatglm2-6b", 
                 openai_api_base="http://localhost:8000/v1", 
                 openai_api_key="EMPTY") # EMPTY or NULL
```

* [Building a Truly "Open" OpenAI API Server with Open Models Locally](https://lmsys.org/blog/2023-06-09-api-server/)
* [Local LangChain with FastChat](https://github.com/lm-sys/FastChat/blob/main/docs/langchain_integration.md)
* [LangChain 101: Part 1. Building Simple Q&A App](https://towardsai.net/p/machine-learning/langchain-101-part-1-building-simple-qa-app)

## å…³é—­æœåŠ¡
### å…³é—­æ‰€æœ‰æœåŠ¡

```shell
python -m fastchat.serve.shutdown_serve --down all
```

### å…³é—­ Controller

```shell
python -m fastchat.serve.shutdown_serve --down controller
```

### å…³é—­ Model Worker

```shell
python -m fastchat.serve.shutdown_serve --down model_worker
```

### å…³é—­ OpenAI API Server

```shell
python -m fastchat.serve.shutdown_serve --down openai_api_server
```

### æ€è¿›ç¨‹çš„æ–¹æ³•

`process_name` ä¸ºè¿è¡Œçš„å‘½ä»¤è¡Œï¼Œå¯ä»¥æ˜¯éƒ¨åˆ†ã€‚

* æ–¹æ³•ä¸€
```shell
kill -9 $(pgrep -f process_name)
```

* æ–¹æ³•äºŒ
```shell
ps -eo user,pid,command|grep process_name|grep -v grep|awk '{{print $2}}'|xargs kill -9
```

## å‚è€ƒèµ„æ–™
* [fastchat/serve/controller.py](https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/controller.py)
* [fastchat/serve/model_worker.py](https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/model_worker.py)
* [fastchat/serve/launch_all_serve.py](https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/launch_all_serve.py)
* [fastchat/serve/shutdown_serve.py](https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/shutdown_serve.py)
* [fastchat/serve/openai_api_server.py](https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/openai_api_server.py)
* [Langchain-Chatchat/server/llm_api_stale.py](https://github.com/chatchat-space/Langchain-Chatchat/blob/master/server/llm_api_stale.py)
* [FastChatâ€”â€”ä¸€ä¸ªç”¨äºè®­ç»ƒã€éƒ¨ç½²å’Œè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„èŠå¤©æœºå™¨äººçš„å¼€æ”¾å¹³å°](https://cloud.tencent.com/developer/article/2297923)
* [ä½¿ç”¨fastchatå®ç°å¤§æ¨¡å‹é«˜å¹¶å‘å¯¹è¯](https://blog.csdn.net/jining11/article/details/133076780)
* [æ¯”HuggingFaceå¿«24å€ï¼Œä¼¯å…‹åˆ©ç¥çº§LLMæ¨ç†ç³»ç»Ÿå¼€æºï¼Œç¢¾å‹SOTAï¼Œè®©GPUç åŠ](https://36kr.com/p/2310989263285510)
* [vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://vllm.ai/)
* [Welcome to vLLM!](https://vllm.readthedocs.io/en/latest/index.html)
* [vLLM Supported Models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)
