---
layout: single
title:  "llama.cpp å®æˆ˜æŒ‡å—ï¼ˆJetson Thor å¹³å°ï¼‰ï¼šä»æºç ç¼–è¯‘åˆ° GGUF æ¨¡å‹éƒ¨ç½²ä¸æ€§èƒ½åŸºå‡†æµ‹è¯•"
date:   2025-10-15 08:00:00 +0800
categories: Jetson LLM
tags: [JetsonThor, Jetson, Thor, Benchmark, llama.cpp, llama-server, gguf, Qwen3, gpt-oss, LLM, NVIDIA]
---

æœ¬æ–‡å°†ä»‹ç»å¦‚ä½•åœ¨ Jetson Thor å¹³å°ä¸Šç¼–è¯‘ã€éƒ¨ç½²å’Œæµ‹è¯• llama.cpp é¡¹ç›®ä¸­çš„ GGUF æ ¼å¼çš„å¤§æ¨¡å‹ã€‚

<!--more-->

## [æºç ç¼–è¯‘](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md)

### å…‹éš† [llama.cpp](https://github.com/ggml-org/llama.cpp)

```
git clone https://github.com/ggml-org/llama.cpp.git
cd llama.cpp
```

### [CUDA GPU Compute Capabilityï¼ˆè®¡ç®—èƒ½åŠ›ï¼‰](https://developer.nvidia.com/cuda-gpus)

`è®¡ç®—èƒ½åŠ›ï¼ˆCCï¼‰`å®šä¹‰äº†æ¯ç§ NVIDIA GPU æ¶æ„çš„**ç¡¬ä»¶ç‰¹æ€§**å’Œ**æ”¯æŒçš„æŒ‡ä»¤**ã€‚åœ¨ä¸‹è¡¨ä¸­æŸ¥æ‰¾æ‚¨çš„GPUçš„è®¡ç®—èƒ½åŠ›ã€‚

![](/images/2025/Jetson/llama-server/cc-110.png)

### ç¼–è¯‘

```bash
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES="110"
cmake --build build --config Release -j $(nproc)
```


## æ¨¡å‹éƒ¨ç½²
### è¿è¡Œ llama-server

#### [Qwen3-8B-GGUF](https://www.modelscope.cn/models/Qwen/Qwen3-8B-GGUF)

```bash
./build/bin/llama-server \
    --model /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_K_M.gguf \
    --alias qwen3 \
    --host 0.0.0.0 \
    --port 8000 \
    --reasoning-budget 0 \
    --gpu-layers -1 \
    --ctx-size 0 \
    --parallel $(nproc) \
    --threads $(nproc) \
    --flash-attn on \
    --no-kv-offload \
    --no-op-offload \
    --no-mmap \
    --mlock
```

#### [Qwen3-Coder-30B-A3B-Instruct-GGUF](https://www.modelscope.cn/models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF)

```bash
./build/bin/llama-server \
    --model /models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf \
    --alias qwen3 \
    --host 0.0.0.0 \
    --port 8000 \
    --jinja \
    --reasoning-format none \
    --gpu-layers -1 \
    --ctx-size 0 \
    --parallel $(nproc) \
    --threads $(nproc) \
    --flash-attn on \
    --no-kv-offload \
    --no-op-offload \
    --no-mmap \
    --mlock
```

#### [gpt-oss-120b-GGUF](https://www.modelscope.cn/models/ggml-org/gpt-oss-120b-GGUF)
- [gpt-oss-120b & gpt-oss-20b Model Card](https://www.modelscope.cn/papers/2508.10925)
- [guide : running gpt-oss with llama.cpp #15396](https://github.com/ggml-org/llama.cpp/discussions/15396)
- [llama : add gpt-oss #15091](https://github.com/ggml-org/llama.cpp/pull/15091)

```bash
./build/bin/llama-server \
    --model /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf \
    --alias qwen3 \
    --host 0.0.0.0 \
    --port 8000 \
    --jinja \
    --gpu-layers -1 \
    --ctx-size 0 \
    --parallel $(nproc) \
    --threads $(nproc) \
    --flash-attn on \
    --no-kv-offload \
    --no-op-offload \
    --no-mmap \
    --mlock
```

### llama-server å‘½ä»¤è¡Œå‚æ•°è§£é‡Š

#### æ¨¡å‹è·¯å¾„ä¸åˆ«åè®¾ç½®

*   **`.llama-server`**
    *   è¿™æ˜¯ `llama.cpp` é¡¹ç›®ä¸­ç”¨äºå¯åŠ¨ REST API æœåŠ¡çš„å¯æ‰§è¡Œæ–‡ä»¶ [i]ã€‚
*   **`--model /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf`**
    *   ä½¿ç”¨æ­¤é€‰é¡¹ï¼ˆ`-m, --model FNAME`ï¼‰æŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„ã€‚
*   **`--alias qwen3`**
    *   ä½¿ç”¨æ­¤é€‰é¡¹ï¼ˆ`-a, --alias STRING`ï¼‰**ä¸ºæ¨¡å‹åç§°è®¾ç½®åˆ«å**ï¼Œè¯¥åˆ«åå°†ç”¨äº REST APIã€‚

#### æœåŠ¡é…ç½®

*   **`--host 0.0.0.0`**
    *   æŒ‡å®šæœåŠ¡å™¨è¦ç›‘å¬çš„ IP åœ°å€ã€‚é»˜è®¤æ˜¯ `127.0.0.1`ã€‚
*   **`--port 8000`**
    *   æŒ‡å®šæœåŠ¡å™¨è¦ç›‘å¬çš„ç«¯å£ã€‚é»˜è®¤æ˜¯ `8080`ã€‚
*   **`--jinja`**
    *   å¯ç”¨æ­¤æ ‡å¿—ï¼Œ**åœ¨èŠå¤©ä¸­å°†ä½¿ç”¨ Jinja æ¨¡æ¿**ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ­¤åŠŸèƒ½æ˜¯ç¦ç”¨çš„ã€‚
*   **`--reasoning-format none`**
    *   æ­¤é€‰é¡¹ï¼ˆ`-rf, --reasoning-format FORMAT`ï¼‰ç”¨äº**æ§åˆ¶æ˜¯å¦å…è®¸ä»¥åŠå¦‚ä½•ä»æ¨¡å‹çš„å“åº”ä¸­æå–â€œæ€ç»´æ ‡ç­¾â€**ã€‚
    *   å½“å°†æ­¤å‚æ•°è®¾ç½®ä¸º none æ—¶ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ€ç»´å†…å®¹ï¼ˆthoughtsï¼‰å°†ä¿æŒæœªè§£æçŠ¶æ€ï¼Œå¹¶ä¿ç•™åœ¨ message.content å­—æ®µä¸­ã€‚ é»˜è®¤å€¼æ˜¯ `none`ã€‚
*   **`--reasoning-budget 0`**
    *   å½“è®¾ç½®ä¸º `0` æ—¶ï¼Œæ€ç»´å†…å®¹ï¼ˆthoughtsï¼‰å°†è¢«ç¦ç”¨ã€‚é»˜è®¤å€¼æ˜¯ `-1`ã€‚

#### èµ„æºä¸å¹¶å‘æ§åˆ¶

*   **`--gpu-layers -1`**
    *   æ­¤é€‰é¡¹ï¼ˆ`-ngl, --gpu-layers, --n-gpu-layers N`ï¼‰è®¾ç½®**å­˜å‚¨åœ¨ VRAM ä¸­çš„æœ€å¤§å±‚æ•°**ã€‚
    *   è®¾ç½®ä¸º `-1` é€šå¸¸æ„å‘³ç€å°è¯•å°†æ‰€æœ‰å¯èƒ½çš„å±‚åŠ è½½åˆ° VRAM ä¸­ã€‚
*   **`--ctx-size 0`**
    *   æ­¤é€‰é¡¹ï¼ˆ`-c, --ctx-size N`ï¼‰è®¾ç½®æç¤ºä¸Šä¸‹æ–‡çš„å¤§å°ã€‚
    *   **å½“è®¾ç½®ä¸º `0` æ—¶ï¼Œä¸Šä¸‹æ–‡å¤§å°å°†ä»æ¨¡å‹ä¸­åŠ è½½**ã€‚é»˜è®¤å€¼æ˜¯ 4096ã€‚
*   **`--parallel $(nproc)`**
    *   æ­¤é€‰é¡¹ï¼ˆ`-np, --parallel N`ï¼‰è®¾ç½®**å¹¶è¡Œè§£ç çš„åºåˆ—æ•°é‡**ã€‚
    *   `$(nproc)` æ˜¯ä¸€ä¸ª shell å‘½ä»¤ï¼Œè¡¨ç¤ºå°†å¹¶è¡Œåºåˆ—æ•°è®¾ç½®ä¸ºç³»ç»Ÿå¯ç”¨çš„å¤„ç†å™¨æ ¸å¿ƒæ•°ã€‚é»˜è®¤å€¼æ˜¯ 1ã€‚
*   **`--threads $(nproc)`**
    *   æ­¤é€‰é¡¹ï¼ˆ`-t, --threads N`ï¼‰è®¾ç½®**åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä½¿ç”¨çš„çº¿ç¨‹æ•°é‡**ã€‚
    *   `$(nproc)` æ„å‘³ç€å°†çº¿ç¨‹æ•°è®¾ç½®ä¸ºç³»ç»Ÿå¯ç”¨çš„å¤„ç†å™¨æ ¸å¿ƒæ•°ã€‚

#### æ€§èƒ½ä¼˜åŒ–ä¸å†…å­˜ç®¡ç†

*   **`--flash-attn on`**
    *   æ­¤é€‰é¡¹ï¼ˆ`-fa, --flash-attn [on|off|auto]`ï¼‰**è®¾ç½® Flash Attention çš„ä½¿ç”¨**ã€‚
    *   åœ¨æ­¤é…ç½®ä¸­ï¼Œå®ƒè¢«æ˜¾å¼è®¾ç½®ä¸ºâ€œå¼€å¯â€ï¼ˆ`on`ï¼‰ã€‚
*   **`--no-kv-offload`**
    *   æ­¤é€‰é¡¹ï¼ˆ`-nkvo, --no-kv-offload`ï¼‰ç”¨äº**ç¦ç”¨ KV (Key/Value) å¸è½½**ã€‚
*   **`--no-op-offload`**
    *   æ­¤é€‰é¡¹ç”¨äº**ç¦ç”¨å°†ä¸»æœºå¼ é‡æ“ä½œå¸è½½åˆ°è®¾å¤‡**ï¼ˆé»˜è®¤å€¼ä¸º `false`ï¼‰ã€‚
*   **`--no-mmap`**
    *   æ­¤é€‰é¡¹ç”¨äº**ä¸è¿›è¡Œå†…å­˜æ˜ å°„æ¨¡å‹**ã€‚è¿™ä¼šå¯¼è‡´åŠ è½½é€Ÿåº¦å˜æ…¢ï¼Œä½†å¦‚æœæœªä½¿ç”¨ `mlock`ï¼Œåˆ™å¯èƒ½å‡å°‘æ¢é¡µã€‚
*   **`--mlock`**
    *   æ­¤é€‰é¡¹ç”¨äº**å¼ºåˆ¶ç³»ç»Ÿå°†æ¨¡å‹ä¿ç•™åœ¨ RAM ä¸­**ï¼Œè€Œä¸æ˜¯è¿›è¡Œäº¤æ¢æˆ–å‹ç¼©ã€‚

### ç¼–å†™è„šæœ¬ llama-server.sh

```bash
vim /models/llama.cpp/llama-server.sh
```

```bash
#!/bin/bash

# é»˜è®¤é…ç½®
DEFAULT_MODEL="/models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf"
DEFAULT_HOST="0.0.0.0"
DEFAULT_PORT="8000"
DEFAULT_PARALLEL=$(nproc)
DEFAULT_THREADS=$(nproc)

# å‡½æ•°ï¼šåˆ—å‡ºæ‰€æœ‰ gguf æ¨¡å‹
list_gguf_models() {
    echo "æ‰¾åˆ°çš„ GGUF æ¨¡å‹æ–‡ä»¶ï¼š"
    echo "================================================"
    find /models/ -name "*.gguf" -not -path "/models/llama.cpp/*" 2>/dev/null | sort
    echo "================================================"
    echo "æ€»è®¡: $(find /models/ -name "*.gguf" -not -path "/models/llama.cpp/*" 2>/dev/null | wc -l) ä¸ªæ¨¡å‹æ–‡ä»¶"
}

# å‡½æ•°ï¼šæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    echo "ä½¿ç”¨æ–¹æ³•: $0 [é€‰é¡¹]"
    echo "é€‰é¡¹:"
    echo "  -l, --list                åˆ—å‡ºæ‰€æœ‰ GGUF æ¨¡å‹ï¼ˆæ’é™¤ /models/llama.cpp ç›®å½•ï¼‰"
    echo "  -m, --model MODEL_PATH    æŒ‡å®šæ¨¡å‹è·¯å¾„"
    echo "  -H, --host HOST           æŒ‡å®šä¸»æœºåœ°å€ (é»˜è®¤: $DEFAULT_HOST)"
    echo "  -p, --port PORT           æŒ‡å®šç«¯å£ (é»˜è®¤: $DEFAULT_PORT)"
    echo "  -P, --parallel PARALLEL   å¹¶è¡Œæ•° (é»˜è®¤: $(nproc))"
    echo "  -t, --threads THREADS     çº¿ç¨‹æ•° (é»˜è®¤: $(nproc))"
    echo "  -h, --help                æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"
}

# è§£æå‘½ä»¤è¡Œå‚æ•°
SHOW_LIST=false
MODEL_PATH=""
HOST=""
PORT=""
PARALLEL=""
THREADS=""

while [[ $# -gt 0 ]]; do
    case $1 in
        -l|--list)
            SHOW_LIST=true
            shift
            ;;
        -m|--model)
            MODEL_PATH="$2"
            shift 2
            ;;
        -H|--host)
            HOST="$2"
            shift 2
            ;;
        -p|--port)
            PORT="$2"
            shift 2
            ;;
        -P|--parallel)
            PARALLEL="$2"
            shift 2
            ;;
        -t|--threads)
            THREADS="$2"
            shift 2
            ;;
        -h|--help)
            show_help
            exit 0
            ;;
        *)
            echo "æœªçŸ¥å‚æ•°: $1"
            show_help
            exit 1
            ;;
    esac
done

# å¦‚æœæŒ‡å®šäº† --listï¼Œåªåˆ—å‡ºæ¨¡å‹å¹¶é€€å‡º
if [ "$SHOW_LIST" = true ]; then
    list_gguf_models
    exit 0
fi

# è®¾ç½®é»˜è®¤å€¼
MODEL_PATH="${MODEL_PATH:-$DEFAULT_MODEL}"
HOST="${HOST:-$DEFAULT_HOST}"
PORT="${PORT:-$DEFAULT_PORT}"
PARALLEL="${PARALLEL:-$DEFAULT_PARALLEL}"
THREADS="${THREADS:-$DEFAULT_THREADS}"

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if [ ! -f "$MODEL_PATH" ]; then
    echo "é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ - $MODEL_PATH"
    echo "ä½¿ç”¨ -l å‚æ•°æŸ¥çœ‹å¯ç”¨æ¨¡å‹:"
    echo "  $0 -l"
    exit 1
fi

echo "å¯åŠ¨ llama-server..."
echo "æ¨¡å‹è·¯å¾„: $MODEL_PATH"
echo "ä¸»æœºåœ°å€: $HOST"
echo "ç«¯å£: $PORT"
echo "å¹¶è¡Œæ•°: $PARALLEL"
echo "çº¿ç¨‹æ•°: $THREADS"

# è¿è¡Œ llama-server å‘½ä»¤
./build/bin/llama-server \
    --model "$MODEL_PATH" \
    --alias qwen3 \
    --host "$HOST" \
    --port "$PORT" \
    --jinja \
    --reasoning-budget 0 \
    --reasoning-format none \
    --gpu-layers -1 \
    --ctx-size 0 \
    --parallel "$PARALLEL" \
    --threads "$THREADS" \
    --flash-attn on \
    --no-kv-offload \
    --no-op-offload \
    --no-mmap \
    --mlock
```

- å¸®åŠ©

```bash
./llama-server.sh -h
```

```bash
ä½¿ç”¨æ–¹æ³•: ./llama-server.sh [é€‰é¡¹]
é€‰é¡¹:
  -l, --list                åˆ—å‡ºæ‰€æœ‰ GGUF æ¨¡å‹ï¼ˆæ’é™¤ /models/llama.cpp ç›®å½•ï¼‰
  -m, --model MODEL_PATH    æŒ‡å®šæ¨¡å‹è·¯å¾„
  -H, --host HOST           æŒ‡å®šä¸»æœºåœ°å€ (é»˜è®¤: 0.0.0.0)
  -p, --port PORT           æŒ‡å®šç«¯å£ (é»˜è®¤: 8000)
  -P, --parallel PARALLEL   å¹¶è¡Œæ•° (é»˜è®¤: 14)
  -t, --threads THREADS     çº¿ç¨‹æ•° (é»˜è®¤: 14)
  -h, --help                æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
```

- åˆ—å‡º GGUF æ¨¡å‹

```bash
./llama-server.sh -l
```

```bash
æ‰¾åˆ°çš„ GGUF æ¨¡å‹æ–‡ä»¶ï¼š
================================================
/models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf
/models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00002-of-00003.gguf
/models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00003-of-00003.gguf
/models/Qwen/Qwen3-30B-A3B-GGUF/Qwen3-30B-A3B-Q5_K_M.gguf
/models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q4_K_M.gguf
/models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_0.gguf
/models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_K_M.gguf
/models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf
/models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q8_0.gguf
/models/Qwen/Qwen3-Embedding-4B-GGUF/Qwen3-Embedding-4B-f16.gguf
/models/Qwen/Qwen3-Embedding-4B-GGUF/Qwen3-Embedding-4B-Q4_K_M.gguf
/models/Qwen/Qwen3-Embedding-4B-GGUF/Qwen3-Embedding-4B-Q5_0.gguf
/models/Qwen/Qwen3-Embedding-4B-GGUF/Qwen3-Embedding-4B-Q5_K_M.gguf
/models/Qwen/Qwen3-Embedding-4B-GGUF/Qwen3-Embedding-4B-Q6_K.gguf
/models/Qwen/Qwen3-Embedding-4B-GGUF/Qwen3-Embedding-4B-Q8_0.gguf
/models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf
/models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf
/models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf
/models/yairpatch/Qwen3-VL-30B-A3B-Instruct-GGUF/Qwen3-VL-30B-A3B-Instruct-Q5_K_M.gguf
================================================
æ€»è®¡: 19 ä¸ªæ¨¡å‹æ–‡ä»¶
```


## æ€§èƒ½æµ‹è¯•

### æ¨¡å‹æ€§èƒ½åˆ†æ

#### æ¨¡å‹è§„æ ¼å¯¹æ¯”

![](/images/2025/Jetson/llama-server/models.png)

#### PP512 æ€§èƒ½å¯¹æ¯”

![](/images/2025/Jetson/llama-server/pp512-performance.png)

#### TG128 æ€§èƒ½å¯¹æ¯”

![](/images/2025/Jetson/llama-server/tg128-performance.png)

#### æ¨¡å‹å¤§å° vs PP512 æ€§èƒ½åˆ†æ

![](/images/2025/Jetson/llama-server/model-size_pp512-performance.png)

#### æ¨¡å‹å¤§å° vs TG128 æ€§èƒ½åˆ†æ

![](/images/2025/Jetson/llama-server/model-size_tg128-performance.png)

### gpt-oss-120b-GGUF

- **MXFP4**

```bash
./build/bin/llama-bench -m /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf
```

| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | CUDA       |  99 |           pp512 |        533.06 Â± 7.20 |
| gpt-oss 120B MXFP4 MoE         |  59.02 GiB |   116.83 B | CUDA       |  99 |           tg128 |         39.95 Â± 0.03 |

### Qwen3-Coder-30B-A3B-Instruct-GGUF

- **Q4_K_M**

```bash
./build/bin/llama-bench -m /models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf
```

| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3moe 30B.A3B Q4_K - Medium |  17.28 GiB |    30.53 B | CUDA       |  99 |           pp512 |        959.77 Â± 7.22 |
| qwen3moe 30B.A3B Q4_K - Medium |  17.28 GiB |    30.53 B | CUDA       |  99 |           tg128 |         52.95 Â± 0.08 |

- **Q5_K_M**

```bash
./build/bin/llama-bench -m /models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf
```

| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3moe 30B.A3B Q5_K - Medium |  20.23 GiB |    30.53 B | CUDA       |  99 |           pp512 |        946.36 Â± 4.90 |
| qwen3moe 30B.A3B Q5_K - Medium |  20.23 GiB |    30.53 B | CUDA       |  99 |           tg128 |         54.55 Â± 0.08 |

- **Q6_K**

```bash
./build/bin/llama-bench -m /models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf
```

| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3moe 30B.A3B Q6_K          |  23.36 GiB |    30.53 B | CUDA       |  99 |           pp512 |        863.80 Â± 5.21 |
| qwen3moe 30B.A3B Q6_K          |  23.36 GiB |    30.53 B | CUDA       |  99 |           tg128 |         47.76 Â± 0.05 |

### Qwen3-8B-GGUF

- **Q4_K_M**

```bash
./build/bin/llama-bench -m /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q4_K_M.gguf
```

| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3 8B Q4_K - Medium         |   4.68 GiB |     8.19 B | CUDA       |  99 |           pp512 |       1256.90 Â± 0.40 |
| qwen3 8B Q4_K - Medium         |   4.68 GiB |     8.19 B | CUDA       |  99 |           tg128 |         40.07 Â± 0.02 |

- **Q5_0**

```bash
./build/bin/llama-bench -m /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_0.gguf
```

| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3 8B Q5_0                  |   5.32 GiB |     8.19 B | CUDA       |  99 |           pp512 |       1353.25 Â± 0.66 |
| qwen3 8B Q5_0                  |   5.32 GiB |     8.19 B | CUDA       |  99 |           tg128 |         39.22 Â± 0.02 |

- **Q5_K_M**

```bash
./build/bin/llama-bench -m /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q5_K_M.gguf
```

| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3 8B Q5_K - Medium         |   5.44 GiB |     8.19 B | CUDA       |  99 |           pp512 |       1239.98 Â± 0.97 |
| qwen3 8B Q5_K - Medium         |   5.44 GiB |     8.19 B | CUDA       |  99 |           tg128 |         36.13 Â± 0.02 |

- **Q6_K**

```bash
./build/bin/llama-bench -m /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q6_K.gguf
```

| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3 8B Q6_K                  |   6.26 GiB |     8.19 B | CUDA       |  99 |           pp512 |       1121.56 Â± 0.30 |
| qwen3 8B Q6_K                  |   6.26 GiB |     8.19 B | CUDA       |  99 |           tg128 |         31.35 Â± 0.97 |

- **Q8_0**

```bash
./build/bin/llama-bench -m /models/Qwen/Qwen3-8B-GGUF/Qwen3-8B-Q8_0.gguf
```

| model                          |       size |     params | backend    | ngl |            test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: |
| qwen3 8B Q8_0                  |   8.11 GiB |     8.19 B | CUDA       |  99 |           pp512 |       1413.13 Â± 0.80 |
| qwen3 8B Q8_0                  |   8.11 GiB |     8.19 B | CUDA       |  99 |           tg128 |         28.29 Â± 0.25 |


## å‚è€ƒèµ„æ–™
- [ğŸ“Œ GPT OSS](https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html)
- [ggml-org æ¨¡å‹åº“](https://www.modelscope.cn/organization/ggml-org?tab=model)
- [Unsloth AI æ¨¡å‹åº“](https://huggingface.co/unsloth/models)
- [vLLM X Ascend - Performance Benchmark](https://vllm-ascend.readthedocs.io/zh-cn/v0.9.2rc1/developer_guide/performance/performance_benchmark.html)
