---
layout: single
title:  "DeepSeek-V3 Technical Report"
date:   2025-01-23 10:00:00 +0800
categories: DeepSeek-V3 arXiv
tags: [DeepSeek-V3, MoE, LLM]
---

- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437)

## Abstractï¼ˆæ‘˜è¦ï¼‰

We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total
parameters with 37B activated for each token. To achieve efficient inference and cost-effective
training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-
tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers
an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training
objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and
high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to
fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms
other open-source models and achieves performance comparable to leading closed-source
models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours
for its full training. In addition, its training process is remarkably stable. Throughout the entire
training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.
The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.

æˆ‘ä»¬æå‡ºäº† DeepSeek-V3ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è¯­è¨€æ¨¡å‹ï¼Œæ€»å‚æ•°ä¸º 671Bï¼Œæ¯ä¸ªæ ‡è®°æ¿€æ´»äº† 37Bã€‚ä¸ºäº†å®ç°é«˜æ•ˆçš„æ¨ç†å’Œç»æµé«˜æ•ˆçš„è®­ç»ƒï¼ŒDeepSeek-V3 é‡‡ç”¨äº†å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰å’Œ DeepSeekMoE æ¶æ„ï¼Œè¿™åœ¨ DeepSeek-V2 ä¸­å¾—åˆ°äº†å½»åº•éªŒè¯ã€‚æ­¤å¤–ï¼ŒDeepSeek-V3 é¦–åˆ›äº†ä¸€ç§æ— è¾…åŠ©æŸå¤±çš„è´Ÿè½½å¹³è¡¡ç­–ç•¥ï¼Œå¹¶ä¸ºæ›´å¼ºçš„æ€§èƒ½è®¾ç½®äº†å¤šæ ‡è®°é¢„æµ‹è®­ç»ƒç›®æ ‡ã€‚æˆ‘ä»¬åœ¨ 14.8Tï¼ˆ14800 äº¿ï¼‰å¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„æ ‡è®°ä¸Šé¢„è®­ç»ƒ DeepSeek-V3ï¼Œç„¶åè¿›è¡Œç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä»¥å……åˆ†å‘æŒ¥å…¶èƒ½åŠ›ã€‚å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼ŒDeepSeek-V3 èƒœè¿‡å…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¹¶å®ç°äº†ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚å°½ç®¡æ€§èƒ½å‡ºè‰²ï¼ŒDeepSeek-V3 ä»…éœ€è¦ 2.788M H800 GPU å°æ—¶è¿›è¡Œå®Œæ•´è®­ç»ƒã€‚æ­¤å¤–ï¼Œå…¶è®­ç»ƒè¿‡ç¨‹éå¸¸ç¨³å®šã€‚åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰é‡åˆ°ä»»ä½•ä¸å¯æ¢å¤çš„æŸå¤±å³°å€¼ï¼Œä¹Ÿæ²¡æœ‰æ‰§è¡Œä»»ä½•å›æ»šã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨ https://github.com/deepseek-ai/DeepSeek-V3 ä¸Šæ‰¾åˆ°ã€‚

![](/images/2025/DeepSeekV3/Figure1.png)

`å›¾1 DeepSeek-V3 åŠå…¶å¯¹æ‰‹çš„åŸºå‡†æ€§èƒ½ã€‚`


## 1. Introductionï¼ˆä»‹ç»ï¼‰

In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and
evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap to-
wards Artificial General Intelligence (AGI). Beyond closed-source models, open-source models,
including DeepSeek series (DeepSeek-AI, 2024a,b,c; Guo et al., 2024), LLaMA series (AI@Meta,
2024a,b; Touvron et al., 2023a,b), Qwen series (Qwen, 2023, 2024a,b), and Mistral series (Jiang
et al., 2023; Mistral, 2024), are also making significant strides, endeavoring to close the gap with
their closed-source counterparts. To further push the boundaries of open-source model capa-
bilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE)
model with 671B parameters, of which 37B are activated for each token.

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»å†äº†å¿«é€Ÿè¿­ä»£å’Œæ¼”è¿›ï¼ˆAnthropicï¼Œ2024ï¼›Googleï¼Œ2024ï¼›OpenAIï¼Œ2024aï¼‰ï¼Œé€æ¸ç¼©å°äº†äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„å·®è·ã€‚é™¤äº†é—­æºæ¨¡å‹å¤–ï¼Œå¼€æºæ¨¡å‹ï¼ŒåŒ…æ‹¬ DeepSeek ç³»åˆ—ï¼ˆDeepSeek-AIï¼Œ2024aï¼Œbï¼Œcï¼›Guo ç­‰ï¼Œ2024ï¼‰ï¼ŒLLaMA ç³»åˆ—ï¼ˆAI@Metaï¼Œ2024aï¼Œbï¼›Touvron ç­‰ï¼Œ2023aï¼Œbï¼‰ï¼ŒQwen ç³»åˆ—ï¼ˆQwenï¼Œ2023ï¼Œ2024aï¼Œbï¼‰å’Œ Mistral ç³»åˆ—ï¼ˆJiang ç­‰ï¼Œ2023ï¼›Mistralï¼Œ2024ï¼‰ï¼Œä¹Ÿåœ¨å–å¾—é‡å¤§è¿›å±•ï¼ŒåŠªåŠ›ç¼©å°ä¸é—­æºå¯¹æ‰‹çš„å·®è·ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ¨åŠ¨å¼€æºæ¨¡å‹èƒ½åŠ›çš„è¾¹ç•Œï¼Œæˆ‘ä»¬æ‰©å¤§äº†æ¨¡å‹è§„æ¨¡ï¼Œå¹¶å¼•å…¥äº† DeepSeek-V3ï¼Œä¸€ä¸ªå¤§å‹ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¨¡å‹ï¼Œå…·æœ‰ 671B å‚æ•°ï¼Œå…¶ä¸­æ¯ä¸ªæ ‡è®°æ¿€æ´»äº† 37Bã€‚

With a forward-looking perspective, we consistently strive for strong model performance
and economical costs. Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head
Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai
et al., 2024) for cost-effective training. These two architectures have been validated in DeepSeek-
V2 (DeepSeek-AI, 2024c), demonstrating their capability to maintain robust model performance
while achieving efficient training and inference. Beyond the basic architecture, we implement
two additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pi-
oneers an auxiliary-loss-free strategy (Wang et al., 2024a) for load balancing, with the aim of
minimizing the adverse impact on model performance that arises from the effort to encourage
load balancing. Secondly, DeepSeek-V3 employs a multi-token prediction training objective,
which we have observed to enhance the overall performance on evaluation benchmarks.

ä»å‰ç»æ€§çš„è§’åº¦å‡ºå‘ï¼Œæˆ‘ä»¬å§‹ç»ˆè‡´åŠ›äºå¼ºå¤§çš„æ¨¡å‹æ€§èƒ½å’Œç»æµæˆæœ¬ã€‚å› æ­¤ï¼Œåœ¨æ¶æ„æ–¹é¢ï¼ŒDeepSeek-V3 ä»ç„¶é‡‡ç”¨å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰ï¼ˆDeepSeek-AIï¼Œ2024cï¼‰è¿›è¡Œé«˜æ•ˆæ¨ç†å’Œ DeepSeekMoEï¼ˆDai ç­‰ï¼Œ2024ï¼‰è¿›è¡Œç»æµé«˜æ•ˆçš„è®­ç»ƒã€‚è¿™ä¸¤ç§æ¶æ„å·²åœ¨ DeepSeek-V2ï¼ˆDeepSeek-AIï¼Œ2024cï¼‰ä¸­å¾—åˆ°éªŒè¯ï¼Œè¯æ˜äº†å®ƒä»¬åœ¨ä¿æŒå¼ºå¤§æ¨¡å‹æ€§èƒ½çš„åŒæ—¶å®ç°é«˜æ•ˆè®­ç»ƒå’Œæ¨ç†çš„èƒ½åŠ›ã€‚é™¤äº†åŸºæœ¬æ¶æ„å¤–ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸¤ç§é¢å¤–ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚é¦–å…ˆï¼ŒDeepSeek-V3 é¦–åˆ›äº†ä¸€ç§æ— è¾…åŠ©æŸå¤±çš„è´Ÿè½½å¹³è¡¡ç­–ç•¥ï¼ˆWang ç­‰ï¼Œ2024aï¼‰ï¼Œæ—¨åœ¨æœ€å¤§é™åº¦åœ°å‡å°‘ç”±äºé¼“åŠ±è´Ÿè½½å¹³è¡¡è€Œäº§ç”Ÿçš„å¯¹æ¨¡å‹æ€§èƒ½çš„ä¸åˆ©å½±å“ã€‚å…¶æ¬¡ï¼ŒDeepSeek-V3 ä½¿ç”¨å¤šæ ‡è®°é¢„æµ‹è®­ç»ƒç›®æ ‡ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™ç§æ–¹æ³•å¯ä»¥æé«˜è¯„ä¼°åŸºå‡†ä¸Šçš„æ•´ä½“æ€§èƒ½ã€‚

In order to achieve efficient training, we support the FP8 mixed precision training and
implement comprehensive optimizations for the training framework. Low-precision training
has emerged as a promising solution for efficient training (Dettmers et al., 2022; Kalamkar et al.,
2019; Narang et al., 2017; Peng et al., 2023b), its evolution being closely tied to advancements in
hardware capabilities (Luo et al., 2024; Micikevicius et al., 2022; Rouhani et al., 2023a). In this
work, we introduce an FP8 mixed precision training framework and, for the first time, validate
its effectiveness on an extremely large-scale model. Through the support for FP8 computation
and storage, we achieve both accelerated training and reduced GPU memory usage. As for
the training framework, we design the DualPipe algorithm for efficient pipeline parallelism,
which has fewer pipeline bubbles and hides most of the communication during training through
computation-communication overlap. This overlap ensures that, as the model further scales up,
as long as we maintain a constant computation-to-communication ratio, we can still employ
fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead.
In addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize
InfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory
footprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism.
Combining these efforts, we achieve high training efficiency.

ä¸ºäº†å®ç°é«˜æ•ˆè®­ç»ƒï¼Œæˆ‘ä»¬æ”¯æŒ FP8 æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¹¶ä¸ºè®­ç»ƒæ¡†æ¶å®æ–½å…¨é¢ä¼˜åŒ–ã€‚ä½ç²¾åº¦è®­ç»ƒå·²ç»æˆä¸ºé«˜æ•ˆè®­ç»ƒçš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼ˆDettmers ç­‰ï¼Œ2022ï¼›Kalamkar ç­‰ï¼Œ2019ï¼›Narang ç­‰ï¼Œ2017ï¼›Peng ç­‰ï¼Œ2023bï¼‰ï¼Œå…¶å‘å±•ä¸ç¡¬ä»¶èƒ½åŠ›çš„è¿›æ­¥å¯†åˆ‡ç›¸å…³ï¼ˆLuo ç­‰ï¼Œ2024ï¼›Micikevicius ç­‰ï¼Œ2022ï¼›Rouhani ç­‰ï¼Œ2023aï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº† FP8 æ··åˆç²¾åº¦è®­ç»ƒæ¡†æ¶ï¼Œå¹¶é¦–æ¬¡éªŒè¯äº†å…¶åœ¨æå¤§è§„æ¨¡æ¨¡å‹ä¸Šçš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡æ”¯æŒ FP8 è®¡ç®—å’Œå­˜å‚¨ï¼Œæˆ‘ä»¬æ—¢å®ç°äº†åŠ é€Ÿè®­ç»ƒï¼Œåˆå‡å°‘äº† GPU å†…å­˜ä½¿ç”¨ã€‚è‡³äºè®­ç»ƒæ¡†æ¶ï¼Œæˆ‘ä»¬è®¾è®¡äº† DualPipe ç®—æ³•ï¼Œç”¨äºé«˜æ•ˆçš„ç®¡é“å¹¶è¡Œï¼Œå®ƒå…·æœ‰è¾ƒå°‘çš„ç®¡é“æ°”æ³¡ï¼Œå¹¶é€šè¿‡è®¡ç®—-é€šä¿¡é‡å åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšè—äº†å¤§éƒ¨åˆ†é€šä¿¡ã€‚è¿™ç§é‡å ç¡®ä¿äº†ï¼Œéšç€æ¨¡å‹çš„è¿›ä¸€æ­¥æ‰©å¤§ï¼Œåªè¦æˆ‘ä»¬ä¿æŒæ’å®šçš„è®¡ç®—-é€šä¿¡æ¯”ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥åœ¨èŠ‚ç‚¹ä¹‹é—´ä½¿ç”¨ç»†ç²’åº¦ä¸“å®¶ï¼ŒåŒæ—¶å®ç°æ¥è¿‘é›¶çš„å…¨äº’è¿é€šä¿¡å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†é«˜æ•ˆçš„è·¨èŠ‚ç‚¹å…¨äº’è¿é€šä¿¡å†…æ ¸ï¼Œä»¥å……åˆ†åˆ©ç”¨ InfiniBandï¼ˆIBï¼‰å’Œ NVLink å¸¦å®½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç²¾å¿ƒä¼˜åŒ–äº†å†…å­˜å ç”¨ï¼Œä½¿å¾—å¯ä»¥åœ¨ä¸ä½¿ç”¨æ˜‚è´µçš„å¼ é‡å¹¶è¡Œçš„æƒ…å†µä¸‹è®­ç»ƒ DeepSeek-V3ã€‚é€šè¿‡ç»“åˆè¿™äº›åŠªåŠ›ï¼Œæˆ‘ä»¬å®ç°äº†è¾ƒé«˜çš„è®­ç»ƒæ•ˆç‡ã€‚

During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The
pre-training process is remarkably stable. Throughout the entire training process, we did not
encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage
context length extension for DeepSeek-V3. In the first stage, the maximum context length is
extended to 32K, and in the second stage, it is further extended to 128K. Following this, we
conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)
on the base model of DeepSeek-V3, to align it with human preferences and further unlock its
potential. During the post-training stage, we distill the reasoning capability from the DeepSeek-
R1 series of models, and meanwhile carefully maintain the balance between model accuracy
and generation length.

åœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬åœ¨ 14.8T é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ ‡è®°ä¸Šè®­ç»ƒ DeepSeek-V3ã€‚é¢„è®­ç»ƒè¿‡ç¨‹éå¸¸ç¨³å®šã€‚åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰é‡åˆ°ä»»ä½•ä¸å¯æ¢å¤çš„æŸå¤±å³°å€¼ï¼Œä¹Ÿæ²¡æœ‰æ‰§è¡Œä»»ä½•å›æ»šã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸º DeepSeek-V3 è¿›è¡Œäº†ä¸¤é˜¶æ®µçš„ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•åˆ° 32Kï¼Œåœ¨ç¬¬äºŒé˜¶æ®µï¼Œè¿›ä¸€æ­¥æ‰©å±•åˆ° 128Kã€‚éšåï¼Œæˆ‘ä»¬å¯¹ DeepSeek-V3 çš„åŸºç¡€æ¨¡å‹è¿›è¡Œäº†åè®­ç»ƒï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥ä½¿å…¶ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ï¼Œå¹¶è¿›ä¸€æ­¥é‡Šæ”¾å…¶æ½œåŠ›ã€‚åœ¨åè®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬ä» DeepSeek-R1 ç³»åˆ—æ¨¡å‹ä¸­æå–æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä»”ç»†ä¿æŒæ¨¡å‹å‡†ç¡®æ€§å’Œç”Ÿæˆé•¿åº¦ä¹‹é—´çš„å¹³è¡¡ã€‚

![](/images/2025/DeepSeekV3/Table1.png)

`è¡¨ 1 DeepSeek-V3 çš„è®­ç»ƒæˆæœ¬ï¼Œå‡è®¾ H800 çš„ç§Ÿé‡‘ä»·æ ¼ä¸ºæ¯ GPU å°æ—¶ 2 ç¾å…ƒã€‚`

We evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical
training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the
strongest open-source base model currently available, especially in code and math. Its chat
version also outperforms other open-source models and achieves performance comparable to
leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard
and open-ended benchmarks.

æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—å…¨é¢çš„åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº† DeepSeek-V3ã€‚å°½ç®¡å…¶è®­ç»ƒæˆæœ¬ç»æµï¼Œä½†å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼ŒDeepSeek-V3-Base å·²æˆä¸ºç›®å‰æœ€å¼ºå¤§çš„å¼€æºåŸºç¡€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç å’Œæ•°å­¦æ–¹é¢ã€‚å…¶èŠå¤©ç‰ˆæœ¬ä¹Ÿèƒœè¿‡å…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—æ ‡å‡†å’Œå¼€æ”¾æ€§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹ï¼ˆåŒ…æ‹¬ GPT-4o å’Œ Claude-3.5-Sonnetï¼‰ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in
Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware.
During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K
H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-
training stage is completed in less than two months and costs 2664K GPU hours. Combined
with 119K GPU hours for the context length extension and 5K GPU hours for post-training,
DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of
the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that
the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs
associated with prior research and ablation experiments on architectures, algorithms, or data.

æœ€åï¼Œæˆ‘ä»¬å†æ¬¡å¼ºè°ƒ DeepSeek-V3 çš„ç»æµè®­ç»ƒæˆæœ¬ï¼Œæ€»ç»“åœ¨è¡¨ 1 ä¸­ï¼Œé€šè¿‡æˆ‘ä»¬ä¼˜åŒ–çš„ç®—æ³•ã€æ¡†æ¶å’Œç¡¬ä»¶çš„ååŒè®¾è®¡å®ç°ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¯ä¸‡äº¿æ ‡è®°ä¸Šè®­ç»ƒ DeepSeek-V3 ä»…éœ€è¦ 180K H800 GPU å°æ—¶ï¼Œå³åœ¨æˆ‘ä»¬çš„ 2048 ä¸ª H800 GPU é›†ç¾¤ä¸Šä¸ºæœŸ 3.7 å¤©ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒé˜¶æ®µåœ¨ä¸åˆ°ä¸¤ä¸ªæœˆå†…å®Œæˆï¼Œæˆæœ¬ä¸º 2664K GPU å°æ—¶ã€‚åŠ ä¸Š 119K GPU å°æ—¶çš„ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•å’Œ 5K GPU å°æ—¶çš„åè®­ç»ƒï¼ŒDeepSeek-V3 çš„å®Œæ•´è®­ç»ƒä»…éœ€ 2.788M GPU å°æ—¶ã€‚å‡è®¾ H800 GPU çš„ç§Ÿé‡‘ä»·æ ¼ä¸ºæ¯ GPU å°æ—¶ 2 ç¾å…ƒï¼Œæˆ‘ä»¬çš„æ€»è®­ç»ƒæˆæœ¬ä»…ä¸º 557.6 ä¸‡ç¾å…ƒã€‚è¯·æ³¨æ„ï¼Œä¸Šè¿°æˆæœ¬ä»…åŒ…æ‹¬ DeepSeek-V3 çš„å®˜æ–¹è®­ç»ƒï¼Œä¸åŒ…æ‹¬ä¸å…ˆå‰ç ”ç©¶å’Œæ¶ˆèå®éªŒç›¸å…³çš„æˆæœ¬ï¼Œè¿™äº›å®éªŒæ¶‰åŠæ¶æ„ã€ç®—æ³•æˆ–æ•°æ®ã€‚

Our main contribution includes:

æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼š

**Architecture: Innovative Load Balancing Strategy and Training Objectiveï¼ˆæ¶æ„ï¼šåˆ›æ–°çš„è´Ÿè½½å¹³è¡¡ç­–ç•¥å’Œè®­ç»ƒç›®æ ‡ï¼‰**

- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.

- åœ¨ DeepSeek-V2 çš„é«˜æ•ˆæ¶æ„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é¦–åˆ›äº†ä¸€ç§æ— è¾…åŠ©æŸå¤±çš„è´Ÿè½½å¹³è¡¡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æœ€å¤§é™åº¦åœ°å‡å°‘äº†ç”±äºé¼“åŠ±è´Ÿè½½å¹³è¡¡è€Œäº§ç”Ÿçš„æ€§èƒ½é™çº§ã€‚

- We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration.

- æˆ‘ä»¬ç ”ç©¶äº†å¤šæ ‡è®°é¢„æµ‹ï¼ˆMTPï¼‰ç›®æ ‡ï¼Œå¹¶è¯æ˜å®ƒå¯¹æ¨¡å‹æ€§èƒ½æœ‰ç›Šã€‚å®ƒè¿˜å¯ä»¥ç”¨äºæ¨æµ‹è§£ç ä»¥åŠ é€Ÿæ¨ç†ã€‚

**Pre-Training: Towards Ultimate Training Efficiencyï¼ˆé¢„è®­ç»ƒï¼šè¿ˆå‘ç»ˆæè®­ç»ƒæ•ˆç‡ï¼‰**

- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.

- æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ª FP8 æ··åˆç²¾åº¦è®­ç»ƒæ¡†æ¶ï¼Œå¹¶é¦–æ¬¡éªŒè¯äº† FP8 è®­ç»ƒåœ¨æå¤§è§„æ¨¡æ¨¡å‹ä¸Šçš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚

- Through the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computation-communication overlap. This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.

- é€šè¿‡ç®—æ³•ã€æ¡†æ¶å’Œç¡¬ä»¶çš„ååŒè®¾è®¡ï¼Œæˆ‘ä»¬å…‹æœäº†è·¨èŠ‚ç‚¹ MoE è®­ç»ƒä¸­çš„é€šä¿¡ç“¶é¢ˆï¼Œå®ç°äº†æ¥è¿‘å…¨è®¡ç®—-é€šä¿¡é‡å ã€‚è¿™æ˜¾è‘—æé«˜äº†æˆ‘ä»¬çš„è®­ç»ƒæ•ˆç‡ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿè¿›ä¸€æ­¥æ‰©å¤§æ¨¡å‹è§„æ¨¡è€Œæ— éœ€é¢å¤–å¼€é”€ã€‚

- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.

- åœ¨ä»… 2.664M H800 GPU å°æ—¶çš„ç»æµæˆæœ¬ä¸‹ï¼Œæˆ‘ä»¬åœ¨ 14.8T æ ‡è®°ä¸Šå®Œæˆäº† DeepSeek-V3 çš„é¢„è®­ç»ƒï¼Œç”Ÿæˆäº†ç›®å‰æœ€å¼ºå¤§çš„å¼€æºåŸºç¡€æ¨¡å‹ã€‚é¢„è®­ç»ƒåçš„è®­ç»ƒé˜¶æ®µä»…éœ€è¦ 0.1M GPU å°æ—¶ã€‚

**Post-Training: Knowledge Distillation from DeepSeek-R1ï¼ˆåè®­ç»ƒï¼šä» DeepSeek-R1 è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼‰**

- We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain control over the output style and length of DeepSeek-V3.

- æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ›æ–°æ–¹æ³•ï¼Œå°†é•¿æ€ç»´é“¾ (CoT) æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ DeepSeek R1 ç³»åˆ—æ¨¡å‹ä¹‹ä¸€ï¼‰ä¸­çš„æ¨ç†èƒ½åŠ›æç‚¼åˆ°æ ‡å‡† LLMï¼ˆå°¤å…¶æ˜¯ DeepSeek-V3ï¼‰ä¸­ã€‚æˆ‘ä»¬çš„æµç¨‹å°† R1 çš„éªŒè¯å’Œåæ€æ¨¡å¼å·§å¦™åœ°èå…¥åˆ° DeepSeek-V3 ä¸­ï¼Œå¹¶æ˜¾è‘—æé«˜äº†å…¶æ¨ç†æ€§èƒ½ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜ä¿æŒå¯¹ DeepSeek-V3 çš„è¾“å‡ºæ ·å¼å’Œé•¿åº¦çš„æ§åˆ¶ã€‚

**Summary of Core Evaluation Resultsï¼ˆæ ¸å¿ƒè¯„ä¼°ç»“æœæ€»ç»“ï¼‰**

- **Knowledge**: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA, DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9 on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3 demonstrates superior performance among open-source models on both SimpleQA and Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese SimpleQA), highlighting its strength in Chinese factual knowledge.

- **çŸ¥è¯†**ï¼šï¼ˆ1ï¼‰åœ¨æ•™è‚²åŸºå‡†æµ‹è¯•ï¼ˆå¦‚ MMLUã€MMLU-Pro å’Œ GPQAï¼‰ä¸Šï¼ŒDeepSeek-V3 èƒœè¿‡æ‰€æœ‰å…¶ä»–å¼€æºæ¨¡å‹ï¼Œåœ¨ MMLU ä¸Šè¾¾åˆ° 88.5ï¼Œåœ¨ MMLU-Pro ä¸Šè¾¾åˆ° 75.9ï¼Œåœ¨ GPQA ä¸Šè¾¾åˆ° 59.1ã€‚å…¶æ€§èƒ½ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹ï¼ˆå¦‚ GPT-4o å’Œ Claude-Sonnet-3.5ï¼‰ç›¸åª²ç¾ï¼Œç¼©å°äº†åœ¨æ­¤é¢†åŸŸå¼€æºå’Œé—­æºæ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚ ï¼ˆ2ï¼‰å¯¹äºäº‹å®æ€§åŸºå‡†æµ‹è¯•ï¼ŒDeepSeek-V3 åœ¨ SimpleQA å’Œ Chinese SimpleQA ä¸Šè¡¨ç°å‡ºè‰²ã€‚è™½ç„¶åœ¨è‹±æ–‡äº‹å®çŸ¥è¯†ï¼ˆSimpleQAï¼‰ä¸Šè½åäº GPT-4o å’Œ Claude-Sonnet-3.5ï¼Œä½†åœ¨ä¸­æ–‡äº‹å®çŸ¥è¯†ï¼ˆChinese SimpleQAï¼‰ä¸Šè¶…è¿‡äº†è¿™äº›æ¨¡å‹ï¼Œçªæ˜¾äº†å…¶åœ¨ä¸­æ–‡äº‹å®çŸ¥è¯†æ–¹é¢çš„ä¼˜åŠ¿ã€‚

- **Code, Math, and Reasoning**: (1) DeepSeek-V3 achieves state-of-the-art performance on math-related benchmarks among all non-long-CoT open-source and closed-source models. Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500, demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks, DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks, such as LiveCodeBench, solidifying its position as the leading model in this domain. For engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5, it still outpaces all other models by a significant margin, demonstrating its competitiveness across diverse technical benchmarks.

- **ä»£ç ã€æ•°å­¦å’Œæ¨ç†**ï¼šï¼ˆ1ï¼‰DeepSeek-V3 åœ¨æ‰€æœ‰éé•¿ CoT å¼€æºå’Œé—­æºæ¨¡å‹ä¸­çš„æ•°å­¦ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒç”šè‡³åœ¨ç‰¹å®šåŸºå‡†æµ‹è¯•ï¼ˆå¦‚ MATH-500ï¼‰ä¸Šèƒœè¿‡äº† o1-previewï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚ ï¼ˆ2ï¼‰åœ¨ä¸ç¼–ç ç›¸å…³çš„ä»»åŠ¡ä¸Šï¼ŒDeepSeek-V3 æˆä¸ºç¼–ç ç«èµ›åŸºå‡†æµ‹è¯•ï¼ˆå¦‚ LiveCodeBenchï¼‰ä¸­è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼Œå·©å›ºäº†å…¶åœ¨è¯¥é¢†åŸŸçš„é¢†å…ˆåœ°ä½ã€‚å¯¹äºä¸å·¥ç¨‹ç›¸å…³çš„ä»»åŠ¡ï¼Œå°½ç®¡ DeepSeek-V3 çš„è¡¨ç°ç•¥ä½äº Claude-Sonnet-3.5ï¼Œä½†ä»ç„¶åœ¨å„ç§æŠ€æœ¯åŸºå‡†æµ‹è¯•ä¸­é¢†å…ˆå…¶ä»–æ¨¡å‹å¾ˆå¤§å¹…åº¦ï¼Œå±•ç¤ºäº†å…¶åœ¨å„ç§æŠ€æœ¯åŸºå‡†æµ‹è¯•ä¸­çš„ç«äº‰åŠ›ã€‚

In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3
model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing
our compute clusters, the training framework, the support for FP8 training, the inference
deployment strategy, and our suggestions on future hardware design. Next, we describe our
pre-training process, including the construction of training data, hyper-parameter settings, long-
context extension techniques, the associated evaluations, as well as some discussions (Section 4).
Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),
Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,
we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential
directions for future research (Section 6).

åœ¨æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬é¦–å…ˆè¯¦ç»†ä»‹ç»äº† DeepSeek-V3 æ¨¡å‹æ¶æ„ï¼ˆç¬¬ 2 èŠ‚ï¼‰ã€‚éšåï¼Œæˆ‘ä»¬ä»‹ç»äº†æˆ‘ä»¬çš„åŸºç¡€è®¾æ–½ï¼ŒåŒ…æ‹¬è®¡ç®—é›†ç¾¤ã€è®­ç»ƒæ¡†æ¶ã€å¯¹ FP8 è®­ç»ƒçš„æ”¯æŒã€æ¨ç†éƒ¨ç½²ç­–ç•¥ä»¥åŠå¯¹æœªæ¥ç¡¬ä»¶è®¾è®¡çš„å»ºè®®ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æè¿°äº†æˆ‘ä»¬çš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®çš„æ„å»ºã€è¶…å‚æ•°è®¾ç½®ã€é•¿ä¸Šä¸‹æ–‡æ‰©å±•æŠ€æœ¯ã€ç›¸å…³è¯„ä¼°ä»¥åŠä¸€äº›è®¨è®ºï¼ˆç¬¬ 4 èŠ‚ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¨è®ºäº†æˆ‘ä»¬åœ¨åè®­ç»ƒä¸­çš„åŠªåŠ›ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€ç›¸åº”çš„è¯„ä¼°å’Œè®¨è®ºï¼ˆç¬¬ 5 èŠ‚ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†è¿™é¡¹å·¥ä½œï¼Œè®¨è®ºäº† DeepSeek-V3 çš„ç°æœ‰å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ï¼ˆç¬¬ 6 èŠ‚ï¼‰ã€‚


## 2. Architectureï¼ˆæ¶æ„ï¼‰

We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-
tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)
for economical training. Then, we present a Multi-Token Prediction (MTP) training objective,
which we have observed to enhance the overall performance on evaluation benchmarks. For
other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeek-
V2 (DeepSeek-AI, 2024c).

æˆ‘ä»¬é¦–å…ˆä»‹ç» DeepSeek-V3 çš„åŸºæœ¬æ¶æ„ï¼Œå…¶ç‰¹ç‚¹æ˜¯å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰ï¼ˆDeepSeek-AIï¼Œ2024cï¼‰ç”¨äºé«˜æ•ˆæ¨ç†å’Œ DeepSeekMoEï¼ˆDai ç­‰ï¼Œ2024ï¼‰ç”¨äºç»æµè®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šæ ‡è®°é¢„æµ‹ï¼ˆMTPï¼‰è®­ç»ƒç›®æ ‡ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™ç§æ–¹æ³•å¯ä»¥æé«˜è¯„ä¼°åŸºå‡†ä¸Šçš„æ•´ä½“æ€§èƒ½ã€‚å¯¹äºæœªæ˜ç¡®æåŠçš„å…¶ä»–ç»†èŠ‚ï¼ŒDeepSeek-V3 éµå¾ª DeepSeek-V2ï¼ˆDeepSeek-AIï¼Œ2024cï¼‰çš„è®¾ç½®ã€‚

### 2.1. Basic Architectureï¼ˆåŸºæœ¬æ¶æ„ï¼‰

The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017)
framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA
and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with
DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing
strategy (Wang et al., 2024a) for DeepSeekMoE to mitigate the performance degradation induced
by the effort to ensure load balance. Figure 2 illustrates the basic architecture of DeepSeek-V3,
and we will briefly review the details of MLA and DeepSeekMoE in this section.

DeepSeek-V3 çš„åŸºæœ¬æ¶æ„ä»ç„¶åœ¨ Transformerï¼ˆVaswani ç­‰ï¼Œ2017ï¼‰æ¡†æ¶å†…ã€‚ä¸ºäº†å®ç°é«˜æ•ˆçš„æ¨ç†å’Œç»æµé«˜æ•ˆçš„è®­ç»ƒï¼ŒDeepSeek-V3 ä¹Ÿé‡‡ç”¨äº† MLA å’Œ DeepSeekMoEï¼Œè¿™ä¸¤ç§æ¶æ„åœ¨ DeepSeek-V2 ä¸­å¾—åˆ°äº†å½»åº•éªŒè¯ã€‚ä¸ DeepSeek-V2 ç›¸æ¯”ï¼Œä¸€ä¸ªä¾‹å¤–æ˜¯æˆ‘ä»¬è¿˜ä¸º DeepSeekMoE å¼•å…¥äº†ä¸€ç§æ— è¾…åŠ©æŸå¤±çš„è´Ÿè½½å¹³è¡¡ç­–ç•¥ï¼ˆWang ç­‰ï¼Œ2024aï¼‰ï¼Œä»¥å‡è½»ç”±äºåŠªåŠ›ç¡®ä¿è´Ÿè½½å¹³è¡¡è€Œå¼•èµ·çš„æ€§èƒ½é™çº§ã€‚å›¾ 2 å±•ç¤ºäº† DeepSeek-V3 çš„åŸºæœ¬æ¶æ„ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬èŠ‚ç®€è¦å›é¡¾ MLA å’Œ DeepSeekMoE çš„ç»†èŠ‚ã€‚

![](/images/2025/DeepSeekV3/Figure2.png)

`å›¾2 DeepSeek-V3 çš„åŸºæœ¬æ¶æ„ç¤ºæ„å›¾ã€‚ç»§æ‰¿ DeepSeek-V2ï¼Œæˆ‘ä»¬é‡‡ç”¨ MLA å’Œ DeepSeekMoE è¿›è¡Œé«˜æ•ˆæ¨ç†å’Œç»æµè®­ç»ƒã€‚`

#### 2.1.1. Multi-Head Latent Attentionï¼ˆå¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼‰

For attention, DeepSeek-V3 adopts the MLA architecture. Let ğ‘‘ denote the embedding dimen-
sion, ğ‘›â„ denote the number of attention heads, ğ‘‘â„ denote the dimension per head, and hğ‘¡ âˆˆRğ‘‘
denote the attention input for the ğ‘¡-th token at a given attention layer. The core of MLA is the
low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during
inference:

å¯¹äºæ³¨æ„åŠ›ï¼ŒDeepSeek-V3 é‡‡ç”¨äº† MLA æ¶æ„ã€‚è®¾ ğ‘‘ è¡¨ç¤ºåµŒå…¥ç»´åº¦ï¼Œğ‘›â„ è¡¨ç¤ºæ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œğ‘‘â„ è¡¨ç¤ºæ¯ä¸ªå¤´çš„ç»´åº¦ï¼Œhğ‘¡ âˆˆRğ‘‘ è¡¨ç¤ºç»™å®šæ³¨æ„åŠ›å±‚ä¸­ç¬¬ ğ‘¡ ä¸ªæ ‡è®°çš„æ³¨æ„åŠ›è¾“å…¥ã€‚MLA çš„æ ¸å¿ƒæ˜¯å¯¹æ³¨æ„åŠ›é”®å’Œå€¼è¿›è¡Œä½ç§©è”åˆå‹ç¼©ï¼Œä»¥å‡å°‘æ¨ç†æœŸé—´çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ï¼š

![](/images/2025/DeepSeekV3/Formula1.png)

![](/images/2025/DeepSeekV3/Formula2.png)

â„Ã—ğ‘‘ is the matrix used to produce the decoupled key that carries Rotary Positional Embedding (RoPE) (Su et al., 2024); RoPE(Â·)denotes the operation that applies RoPE matrices; and [Â·;Â·]denotes concatenation. Note that for MLA, only the blue-boxed vectors  need to be cached during generation, which results in significantly reduced KV cache while maintaining performance comparable to standard Multi-Head Attention (MHA) (Vaswani et al., 2017).

â„Ã—ğ‘‘ æ˜¯ç”¨äºç”Ÿæˆæºå¸¦æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ï¼ˆSu ç­‰ï¼Œ2024ï¼‰çš„è§£è€¦é”®çš„çŸ©é˜µï¼›RoPE(Â·)è¡¨ç¤ºåº”ç”¨ RoPE çŸ©é˜µçš„æ“ä½œï¼›[Â·;Â·]è¡¨ç¤ºè¿æ¥ã€‚è¯·æ³¨æ„ï¼Œå¯¹äº MLAï¼Œåªéœ€è¦åœ¨ç”ŸæˆæœŸé—´ç¼“å­˜è“è‰²æ¡†ä¸­çš„å‘é‡ï¼Œè¿™å¯¼è‡´ KV ç¼“å­˜æ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶ä¿æŒä¸æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰ï¼ˆVaswani ç­‰ï¼Œ2017ï¼‰ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

For the attention queries, we also perform a low-rank compression, which can reduce the activation memory during training:

å¯¹äºæ³¨æ„åŠ›æŸ¥è¯¢ï¼Œæˆ‘ä»¬è¿˜æ‰§è¡Œä½ç§©å‹ç¼©ï¼Œè¿™å¯ä»¥å‡å°‘è®­ç»ƒæœŸé—´çš„æ¿€æ´»å†…å­˜ï¼š

#### 2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancingï¼ˆå…·æœ‰æ— è¾…åŠ©æŸå¤±çš„è´Ÿè½½å¹³è¡¡çš„ DeepSeekMoEï¼‰

Basic Architecture of DeepSeekMoE. For Feed-Forward Networks (FFNs), DeepSeek-V3 employs the DeepSeekMoE architecture (Dai et al., 2024). Compared with traditional MoE architectures like GShard (Lepikhin et al., 2021), DeepSeekMoE uses finer-grained experts and isolates some experts as shared ones. Let uğ‘¡ denote the FFN input of the ğ‘¡-th token, we compute the FFN output hâ€²ğ‘¡ as follows:

DeepSeekMoE çš„åŸºæœ¬æ¶æ„ã€‚å¯¹äºå‰é¦ˆç½‘ç»œ (FFN)ï¼ŒDeepSeek-V3 é‡‡ç”¨ DeepSeekMoE æ¶æ„ (Dai et al., 2024)ã€‚ä¸ GShard (Lepikhin et al., 2021) ç­‰ä¼ ç»Ÿ MoE æ¶æ„ç›¸æ¯”ï¼ŒDeepSeekMoE ä½¿ç”¨æ›´ç»†ç²’åº¦çš„ä¸“å®¶ï¼Œå¹¶å°†ä¸€äº›ä¸“å®¶éš”ç¦»ä¸ºå…±äº«ä¸“å®¶ã€‚è®© uğ‘¡ è¡¨ç¤ºç¬¬ ğ‘¡ ä¸ª token çš„ FFN è¾“å…¥ï¼Œæˆ‘ä»¬æŒ‰å¦‚ä¸‹æ–¹å¼è®¡ç®— FFN è¾“å‡º hâ€²ğ‘¡ï¼š

![](/images/2025/DeepSeekV3/Formula3.png)

where ğ‘ğ‘  and ğ‘ğ‘Ÿ denote the numbers of shared experts and routed experts, respectively; FFN(ğ‘ )ğ‘– (Â·) and FFN(ğ‘Ÿ)ğ‘– (Â·)denote the ğ‘–-th shared expert and the ğ‘–-th routed expert, respectively; ğ¾ğ‘Ÿ denotes the number of activated routed experts; ğ‘”ğ‘–,ğ‘¡ is the gating value for the ğ‘–-th expert; ğ‘ ğ‘–,ğ‘¡ is the token-to-expert affinity; eğ‘– is the centroid vector of the ğ‘–-th routed expert; and Topk(Â·, ğ¾)denotes the set comprising ğ¾ highest scores among the affinity scores calculated for the ğ‘¡-th token and all routed experts. Slightly different from DeepSeek-V2, DeepSeek-V3 uses the sigmoid function to compute the affinity scores, and applies a normalization among all selected affinity scores to produce the gating values.

å…¶ä¸­ ğ‘ğ‘  å’Œ ğ‘ğ‘Ÿ åˆ†åˆ«è¡¨ç¤ºå…±äº«ä¸“å®¶å’Œè·¯ç”±ä¸“å®¶çš„æ•°é‡ï¼›FFN(ğ‘ )ğ‘– (Â·) å’Œ FFN(ğ‘Ÿ)ğ‘– (Â·) åˆ†åˆ«è¡¨ç¤ºç¬¬ ğ‘– ä¸ªå…±äº«ä¸“å®¶å’Œç¬¬ ğ‘– ä¸ªè·¯ç”±ä¸“å®¶ï¼›ğ¾ğ‘Ÿ è¡¨ç¤ºæ¿€æ´»çš„è·¯ç”±ä¸“å®¶æ•°é‡ï¼›ğ‘”ğ‘–,ğ‘¡ æ˜¯ç¬¬ ğ‘– ä¸ªä¸“å®¶çš„é—¨æ§å€¼ï¼›ğ‘ ğ‘–,ğ‘¡ æ˜¯ token åˆ°ä¸“å®¶çš„äº²å’ŒåŠ›ï¼›eğ‘– æ˜¯ç¬¬ ğ‘– ä¸ªè·¯ç”±ä¸“å®¶çš„è´¨å¿ƒå‘é‡ï¼›Topk(Â·, ğ¾) è¡¨ç¤ºè®¡ç®—ç¬¬ ğ‘¡ ä¸ª token å’Œæ‰€æœ‰è·¯ç”±ä¸“å®¶ä¹‹é—´çš„äº²å’ŒåŠ›å¾—åˆ†ä¸­çš„ ğ¾ ä¸ªæœ€é«˜åˆ†æ•°çš„é›†åˆã€‚ä¸ DeepSeek-V2 ç¨æœ‰ä¸åŒï¼ŒDeepSeek-V3 ä½¿ç”¨ sigmoid å‡½æ•°è®¡ç®—äº²å’ŒåŠ›å¾—åˆ†ï¼Œå¹¶åœ¨æ‰€æœ‰é€‰å®šçš„äº²å’ŒåŠ›å¾—åˆ†ä¹‹é—´åº”ç”¨å½’ä¸€åŒ–ä»¥ç”Ÿæˆé—¨æ§å€¼ã€‚

**Auxiliary-Loss-Free Load Balancing.** For MoE models, an unbalanced expert load will lead to routing collapse (Shazeer et al., 2017) and diminish computational efficiency in scenarios with expert parallelism. Conventional solutions usually rely on the auxiliary loss (Fedus et al., 2021; Lepikhin et al., 2021) to avoid unbalanced load. However, too large an auxiliary loss will impair the model performance (Wang et al., 2024a). To achieve a better trade-off between load balance and model performance, we pioneer an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) to ensure load balance. To be specific, we introduce a bias term ğ‘ğ‘– for each expert and add it to the corresponding affinity scores ğ‘ ğ‘–,ğ‘¡ to determine the top-K routing:

**æ— è¾…åŠ©æŸå¤±çš„è´Ÿè½½å¹³è¡¡ã€‚** å¯¹äº MoE æ¨¡å‹ï¼Œä¸å¹³è¡¡çš„ä¸“å®¶è´Ÿè½½ä¼šå¯¼è‡´è·¯ç”±å´©æºƒï¼ˆShazeer ç­‰ï¼Œ2017ï¼‰ï¼Œå¹¶åœ¨å…·æœ‰ä¸“å®¶å¹¶è¡Œæ€§çš„åœºæ™¯ä¸­é™ä½è®¡ç®—æ•ˆç‡ã€‚ä¼ ç»Ÿè§£å†³æ–¹æ¡ˆé€šå¸¸ä¾èµ–è¾…åŠ©æŸå¤±ï¼ˆFedus ç­‰ï¼Œ2021ï¼›Lepikhin ç­‰ï¼Œ2021ï¼‰æ¥é¿å…ä¸å¹³è¡¡çš„è´Ÿè½½ã€‚ç„¶è€Œï¼Œè¿‡å¤§çš„è¾…åŠ©æŸå¤±ä¼šæŸå®³æ¨¡å‹æ€§èƒ½ï¼ˆWang ç­‰ï¼Œ2024aï¼‰ã€‚ä¸ºäº†åœ¨è´Ÿè½½å¹³è¡¡å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´å–å¾—æ›´å¥½çš„æŠ˜è¡·ï¼Œæˆ‘ä»¬é¦–åˆ›äº†ä¸€ç§æ— è¾…åŠ©æŸå¤±çš„è´Ÿè½½å¹³è¡¡ç­–ç•¥ï¼ˆWang ç­‰ï¼Œ2024aï¼‰æ¥ç¡®ä¿è´Ÿè½½å¹³è¡¡ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªä¸“å®¶å¼•å…¥ä¸€ä¸ªåç½®é¡¹ ğ‘ğ‘–ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°ç›¸åº”çš„äº²å’ŒåŠ›å¾—åˆ† ğ‘ ğ‘–,ğ‘¡ ä¸­ï¼Œä»¥ç¡®å®šå‰ K ä¸ªè·¯ç”±ï¼š

![](/images/2025/DeepSeekV3/Formula4.png)

Note that the bias term is only used for routing. The gating value, which will be multiplied with the FFN output, is still derived from the original affinity score ğ‘ ğ‘–,ğ‘¡. During training, we keep monitoring the expert load on the whole batch of each training step. At the end of each step, we will decrease the bias term by ğ›¾if its corresponding expert is overloaded, and increase it by ğ›¾if its corresponding expert is underloaded, where ğ›¾is a hyper-parameter called bias update speed. Through the dynamic adjustment, DeepSeek-V3 keeps balanced expert load during training, and achieves better performance than models that encourage load balance through pure auxiliary losses.

è¯·æ³¨æ„ï¼Œåç½®é¡¹ä»…ç”¨äºè·¯ç”±ã€‚é—¨æ§å€¼ï¼Œå°†ä¸ FFN è¾“å‡ºç›¸ä¹˜ï¼Œä»ç„¶æ˜¯ä»åŸå§‹äº²å’ŒåŠ›å¾—åˆ† ğ‘ ğ‘–,ğ‘¡ ä¸­æ´¾ç”Ÿçš„ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬ä¼šæŒç»­ç›‘æ§æ¯ä¸ªè®­ç»ƒæ­¥éª¤çš„æ•´ä¸ªæ‰¹æ¬¡çš„ä¸“å®¶è´Ÿè½½ã€‚åœ¨æ¯ä¸ªæ­¥éª¤ç»“æŸæ—¶ï¼Œå¦‚æœç›¸åº”çš„ä¸“å®¶è´Ÿè½½è¿‡é‡ï¼Œæˆ‘ä»¬å°†å‡å°åç½®é¡¹ ğ›¾ï¼Œå¦‚æœç›¸åº”çš„ä¸“å®¶è´Ÿè½½ä¸è¶³ï¼Œåˆ™å¢åŠ å®ƒï¼Œå…¶ä¸­ ğ›¾ æ˜¯ç§°ä¸ºåç½®æ›´æ–°é€Ÿåº¦çš„è¶…å‚æ•°ã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´ï¼ŒDeepSeek-V3 åœ¨è®­ç»ƒæœŸé—´ä¿æŒå¹³è¡¡çš„ä¸“å®¶è´Ÿè½½ï¼Œå¹¶å®ç°äº†æ¯”çº¯è¾…åŠ©æŸå¤±é¼“åŠ±è´Ÿè½½å¹³è¡¡çš„æ¨¡å‹æ›´å¥½çš„æ€§èƒ½ã€‚

**Complementary Sequence-Wise Auxiliary Loss.** Although DeepSeek-V3 mainly relies on the auxiliary-loss-free strategy for load balance, to prevent extreme imbalance within any single sequence, we also employ a complementary sequence-wise balance loss:

**è¡¥å……åºåˆ—çº§è¾…åŠ©æŸå¤±ã€‚** å°½ç®¡ DeepSeek-V3 ä¸»è¦ä¾èµ–äºæ— è¾…åŠ©æŸå¤±ç­–ç•¥æ¥å®ç°è´Ÿè½½å¹³è¡¡ï¼Œä¸ºäº†é˜²æ­¢ä»»ä½•å•ä¸ªåºåˆ—å†…çš„æç«¯ä¸å¹³è¡¡ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ä¸ªè¡¥å……çš„åºåˆ—çº§å¹³è¡¡æŸå¤±ï¼š

![](/images/2025/DeepSeekV3/Formula5.png)

where the balance factor ğ›¼ is a hyper-parameter, which will be assigned an extremely small value for DeepSeek-V3; 1(Â·)denotes the indicator function; and ğ‘‡ denotes the number of tokens in a sequence. The sequence-wise balance loss encourages the expert load on each sequence to be balanced.

å…¶ä¸­å¹³è¡¡å› å­ ğ›¼ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå¯¹äº DeepSeek-V3ï¼Œå°†è¢«èµ‹äºˆä¸€ä¸ªæå°çš„å€¼ï¼›1(Â·) è¡¨ç¤ºæŒ‡ç¤ºå‡½æ•°ï¼›ğ‘‡ è¡¨ç¤ºåºåˆ—ä¸­çš„æ ‡è®°æ•°é‡ã€‚åºåˆ—çº§å¹³è¡¡æŸå¤±é¼“åŠ±æ¯ä¸ªåºåˆ—ä¸Šçš„ä¸“å®¶è´Ÿè½½ä¿æŒå¹³è¡¡ã€‚

![](/images/2025/DeepSeekV3/Figure3.png)

`å›¾3 æˆ‘ä»¬çš„å¤šæ ‡è®°é¢„æµ‹ï¼ˆMTPï¼‰å®ç°ç¤ºæ„å›¾ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ªæ·±åº¦ä¿ç•™æ¯ä¸ªæ ‡è®°çš„é¢„æµ‹çš„å®Œæ•´å› æœé“¾ã€‚`

**Node-Limited Routing.** Like the device-limited routing used by DeepSeek-V2, DeepSeek-V3 also uses a restricted routing mechanism to limit communication costs during training. In short, we ensure that each token will be sent to at most ğ‘€ nodes, which are selected according to the sum of the highest ğ¾ğ‘Ÿ ğ‘€ affinity scores of the experts distributed on each node. Under this constraint, our MoE training framework can nearly achieve full computation-communication overlap.

**èŠ‚ç‚¹é™åˆ¶è·¯ç”±ã€‚** ä¸ DeepSeek-V2 ä½¿ç”¨çš„è®¾å¤‡é™åˆ¶è·¯ç”±ç±»ä¼¼ï¼ŒDeepSeek-V3 ä¹Ÿä½¿ç”¨å—é™è·¯ç”±æœºåˆ¶æ¥é™åˆ¶è®­ç»ƒæœŸé—´çš„é€šä¿¡æˆæœ¬ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬ç¡®ä¿æ¯ä¸ªæ ‡è®°æœ€å¤šå‘é€åˆ° ğ‘€ ä¸ªèŠ‚ç‚¹ï¼Œè¿™äº›èŠ‚ç‚¹æ˜¯æ ¹æ®æ¯ä¸ªèŠ‚ç‚¹ä¸Šåˆ†å¸ƒçš„ä¸“å®¶çš„æœ€é«˜ ğ¾ğ‘Ÿ ğ‘€ ä¸ªäº²å’ŒåŠ›å¾—åˆ†ä¹‹å’Œé€‰æ‹©çš„ã€‚åœ¨è¿™ç§çº¦æŸä¸‹ï¼Œæˆ‘ä»¬çš„ MoE è®­ç»ƒæ¡†æ¶å‡ ä¹å¯ä»¥å®ç°å…¨è®¡ç®—-é€šä¿¡é‡å ã€‚

**No Token-Dropping.** Due to the effective load balancing strategy, DeepSeek-V3 keeps a good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training. In addition, we also implement specific deployment strategies to ensure inference load balance, so DeepSeek-V3 also does not drop tokens during inference.

**æ— æ ‡è®°ä¸¢å¼ƒã€‚** ç”±äºæœ‰æ•ˆçš„è´Ÿè½½å¹³è¡¡ç­–ç•¥ï¼ŒDeepSeek-V3 åœ¨æ•´ä¸ªè®­ç»ƒæœŸé—´ä¿æŒè‰¯å¥½çš„è´Ÿè½½å¹³è¡¡ã€‚å› æ­¤ï¼ŒDeepSeek-V3 åœ¨è®­ç»ƒæœŸé—´ä¸ä¼šä¸¢å¼ƒä»»ä½•æ ‡è®°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®æ–½äº†ç‰¹å®šçš„éƒ¨ç½²ç­–ç•¥æ¥ç¡®ä¿æ¨ç†è´Ÿè½½å¹³è¡¡ï¼Œå› æ­¤ DeepSeek-V3 åœ¨æ¨ç†æœŸé—´ä¹Ÿä¸ä¼šä¸¢å¼ƒæ ‡è®°ã€‚

### 2.2. Multi-Token Predictionï¼ˆå¤šæ ‡è®°é¢„æµ‹ï¼‰

Inspired by Gloeckle et al. (2024), we investigate and set a Multi-Token Prediction (MTP)
objective for DeepSeek-V3, which extends the prediction scope to multiple future tokens at each
position. On the one hand, an MTP objective densifies the training signals and may improve
data efficiency. On the other hand, MTP may enable the model to pre-plan its representations
for better prediction of future tokens. Figure 3 illustrates our implementation of MTP. Different
from Gloeckle et al. (2024), which parallelly predicts ğ· additional tokens using independent
output heads, we sequentially predict additional tokens and keep the complete causal chain at
each prediction depth. We introduce the details of our MTP implementation in this section.

å— Gloeckle ç­‰äººï¼ˆ2024ï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬ç ”ç©¶å¹¶ä¸º DeepSeek-V3 è®¾å®šäº†ä¸€ä¸ªå¤šæ ‡è®°é¢„æµ‹ï¼ˆMTPï¼‰ç›®æ ‡ï¼Œå°†é¢„æµ‹èŒƒå›´æ‰©å±•åˆ°æ¯ä¸ªä½ç½®çš„å¤šä¸ªæœªæ¥æ ‡è®°ã€‚ä¸€æ–¹é¢ï¼ŒMTP ç›®æ ‡ä½¿è®­ç»ƒä¿¡å·æ›´å¯†é›†ï¼Œå¯èƒ½æé«˜æ•°æ®æ•ˆç‡ã€‚å¦ä¸€æ–¹é¢ï¼ŒMTP å¯èƒ½ä½¿æ¨¡å‹èƒ½å¤Ÿé¢„å…ˆè§„åˆ’å…¶è¡¨ç¤ºï¼Œä»¥æ›´å¥½åœ°é¢„æµ‹æœªæ¥æ ‡è®°ã€‚å›¾ 3 å±•ç¤ºäº†æˆ‘ä»¬å¯¹ MTP çš„å®ç°ã€‚ä¸ Gloeckle ç­‰äººï¼ˆ2024ï¼‰ä¸åŒï¼Œåè€…ä½¿ç”¨ç‹¬ç«‹çš„è¾“å‡ºå¤´å¹¶è¡Œé¢„æµ‹ ğ· ä¸ªé¢å¤–æ ‡è®°ï¼Œæˆ‘ä»¬ä¾æ¬¡é¢„æµ‹é¢å¤–æ ‡è®°ï¼Œå¹¶åœ¨æ¯ä¸ªé¢„æµ‹æ·±åº¦ä¿ç•™å®Œæ•´çš„å› æœé“¾ã€‚æˆ‘ä»¬åœ¨æœ¬èŠ‚ä»‹ç»äº†æˆ‘ä»¬çš„ MTP å®ç°çš„ç»†èŠ‚ã€‚

![](/images/2025/DeepSeekV3/Formula6.png) 

![](/images/2025/DeepSeekV3/Formula7.png) 

![](/images/2025/DeepSeekV3/Formula8.png)

**MTP in Inference.** Our MTP strategy mainly aims to improve the performance of the main model, so during inference, we can directly discard the MTP modules and the main model can function independently and normally. Additionally, we can also repurpose these MTP modules for speculative decoding to further improve the generation latency.

**æ¨ç†ä¸­çš„ MTPã€‚** æˆ‘ä»¬çš„ MTP ç­–ç•¥ä¸»è¦æ—¨åœ¨æé«˜ä¸»æ¨¡å‹çš„æ€§èƒ½ï¼Œå› æ­¤åœ¨æ¨ç†æœŸé—´ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä¸¢å¼ƒ MTP æ¨¡å—ï¼Œä¸»æ¨¡å‹å¯ä»¥ç‹¬ç«‹æ­£å¸¸åœ°è¿è¡Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é‡æ–°åˆ©ç”¨è¿™äº› MTP æ¨¡å—è¿›è¡Œæ¨æµ‹è§£ç ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç”Ÿæˆå»¶è¿Ÿã€‚


## 3. Infrastructuresï¼ˆåŸºç¡€è®¾æ–½ï¼‰

### 3.1. Compute Clustersï¼ˆè®¡ç®—é›†ç¾¤ï¼‰

DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs. Each node in
the H800 cluster contains 8 GPUs connected by NVLink and NVSwitch within nodes. Across
different nodes, InfiniBand (IB) interconnects are utilized to facilitate communications.

DeepSeek-V3 åœ¨é…å¤‡ 2048 ä¸ª NVIDIA H800 GPU çš„é›†ç¾¤ä¸Šè¿›è¡Œè®­ç»ƒã€‚H800 é›†ç¾¤ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹åŒ…å« 8 ä¸ª GPUï¼ŒèŠ‚ç‚¹å†…é€šè¿‡ NVLink å’Œ NVSwitch è¿æ¥ã€‚åœ¨ä¸åŒèŠ‚ç‚¹ä¹‹é—´ï¼Œä½¿ç”¨ InfiniBandï¼ˆIBï¼‰äº’è¿æ¥ä¿ƒè¿›é€šä¿¡ã€‚

![](/images/2025/DeepSeekV3/Figure4.png)

å›¾4 ä¸€å¯¹å•ç‹¬çš„å‰å‘å’Œåå‘å—çš„é‡å ç­–ç•¥ï¼ˆå˜å‹å™¨å—çš„è¾¹ç•Œæœªå¯¹é½ï¼‰ã€‚æ©™è‰²è¡¨ç¤ºå‰å‘ï¼Œç»¿è‰²è¡¨ç¤ºâ€œè¾“å…¥çš„åå‘â€ï¼Œè“è‰²è¡¨ç¤ºâ€œæƒé‡çš„åå‘â€ï¼Œç´«è‰²è¡¨ç¤º PP é€šä¿¡ï¼Œçº¢è‰²è¡¨ç¤ºå±éšœã€‚å…¨äº’è¿å’Œ PP é€šä¿¡éƒ½å¯ä»¥å®Œå…¨éšè—ã€‚

### 3.2. Training Frameworkï¼ˆè®­ç»ƒæ¡†æ¶ï¼‰

The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and
lightweight training framework crafted by our engineers from the ground up. On the whole,
DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Paral-
lelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajb-
handari et al., 2020).

DeepSeek-V3 çš„è®­ç»ƒç”± HAI-LLM æ¡†æ¶æ”¯æŒï¼Œè¿™æ˜¯æˆ‘ä»¬çš„å·¥ç¨‹å¸ˆä»å¤´å¼€å§‹ç²¾å¿ƒæ‰“é€ çš„é«˜æ•ˆè½»é‡çº§è®­ç»ƒæ¡†æ¶ã€‚æ€»ä½“è€Œè¨€ï¼ŒDeepSeek-V3 åº”ç”¨ 16 è·¯ç®¡é“å¹¶è¡Œï¼ˆPPï¼‰ï¼ˆQi ç­‰ï¼Œ2023aï¼‰ï¼Œ64 è·¯ä¸“å®¶å¹¶è¡Œï¼ˆEPï¼‰ï¼ˆLepikhin ç­‰ï¼Œ2021ï¼‰è·¨ 8 ä¸ªèŠ‚ç‚¹ï¼Œä»¥åŠ ZeRO-1 æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰ï¼ˆRajbhandari ç­‰ï¼Œ2020ï¼‰ã€‚

In order to facilitate efficient training of DeepSeek-V3, we implement meticulous engineering optimizations. Firstly, we design the DualPipe algorithm for efficient pipeline parallelism. Compared with existing PP methods, DualPipe has fewer pipeline bubbles. More importantly, it overlaps the computation and communication phases across forward and backward processes, thereby addressing the challenge of heavy communication overhead introduced by cross-node expert parallelism. Secondly, we develop efficient cross-node all-to-all communication kernels to fully utilize IB and NVLink bandwidths and conserve Streaming Multiprocessors (SMs) dedicated to communication. Finally, we meticulously optimize the memory footprint during training, thereby enabling us to train DeepSeek-V3 without using costly Tensor Parallelism (TP).

ä¸ºäº†æé«˜ DeepSeek-V3 çš„è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç»†è‡´çš„å·¥ç¨‹ä¼˜åŒ–ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº† DualPipe ç®—æ³•ä»¥å®ç°é«˜æ•ˆçš„æµæ°´çº¿å¹¶è¡Œã€‚ä¸ç°æœ‰çš„ PP æ–¹æ³•ç›¸æ¯”ï¼ŒDualPipe å…·æœ‰æ›´å°‘çš„æµæ°´çº¿æ°”æ³¡ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒå°†è®¡ç®—å’Œé€šä¿¡é˜¶æ®µé‡å åœ¨å‰å‘å’Œåå‘è¿‡ç¨‹ä¸­ï¼Œä»è€Œè§£å†³äº†è·¨èŠ‚ç‚¹ä¸“å®¶å¹¶è¡Œå¼•å…¥çš„æ²‰é‡é€šä¿¡å¼€é”€çš„æŒ‘æˆ˜ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†é«˜æ•ˆçš„è·¨èŠ‚ç‚¹å…¨å¯¹å…¨é€šä¿¡å†…æ ¸ï¼Œä»¥å……åˆ†åˆ©ç”¨ IB å’Œ NVLink å¸¦å®½å¹¶èŠ‚çœä¸“ç”¨äºé€šä¿¡çš„æµå¤šå¤„ç†å™¨ (SM)ã€‚æœ€åï¼Œæˆ‘ä»¬ç²¾å¿ƒä¼˜åŒ–äº†è®­ç»ƒæœŸé—´çš„å†…å­˜å ç”¨ï¼Œä»è€Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨æ˜‚è´µçš„å¼ é‡å¹¶è¡Œ (TP) çš„æƒ…å†µä¸‹è®­ç»ƒ DeepSeek-V3ã€‚

#### 3.2.1. DualPipe and Computation-Communication Overlapï¼ˆDualPipe å’Œè®¡ç®—-é€šä¿¡é‡å ï¼‰

For DeepSeek-V3, the communication overhead introduced by cross-node expert parallelism
results in an inefficient computation-to-communication ratio of approximately 1:1. To tackle this
challenge, we design an innovative pipeline parallelism algorithm called DualPipe, which not
only accelerates model training by effectively overlapping forward and backward computation-
communication phases, but also reduces the pipeline bubbles.

å¯¹äº DeepSeek-V3ï¼Œè·¨èŠ‚ç‚¹ä¸“å®¶å¹¶è¡Œå¼•å…¥çš„é€šä¿¡å¼€é”€å¯¼è‡´äº†çº¦ 1:1 çš„ä½æ•ˆè®¡ç®—-é€šä¿¡æ¯”ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ›æ–°çš„æµæ°´çº¿å¹¶è¡Œç®—æ³• DualPipeï¼Œå®ƒä¸ä»…é€šè¿‡æœ‰æ•ˆåœ°é‡å å‰å‘å’Œåå‘è®¡ç®—-é€šä¿¡é˜¶æ®µåŠ é€Ÿæ¨¡å‹è®­ç»ƒï¼Œè¿˜å‡å°‘äº†æµæ°´çº¿æ°”æ³¡ã€‚

The key idea of DualPipe is to overlap the computation and communication within a pair of
individual forward and backward chunks. To be specific, we divide each chunk into four compo-
nents: attention, all-to-all dispatch, MLP, and all-to-all combine. Specially, for
a backward chunk, both attention and MLP are further split into two parts, backward for
input and backward for weights, like in ZeroBubble (Qi et al., 2023b). In addition, we
have a PP communication component. As illustrated in Figure 4, for a pair of forward and
backward chunks, we rearrange these components and manually adjust the ratio of GPU SMs
dedicated to communication versus computation. In this overlapping strategy, we can ensure
that both all-to-all and PP communication can be fully hidden during execution. Given the
efficient overlapping strategy, the full DualPipe scheduling is illustrated in Figure 5. It employs
a bidirectional pipeline scheduling, which feeds micro-batches from both ends of the pipeline
simultaneously and a significant portion of communications can be fully overlapped. This
overlap also ensures that, as the model further scales up, as long as we maintain a constant
computation-to-communication ratio, we can still employ fine-grained experts across nodes
while achieving a near-zero all-to-all communication overhead.

DualPipe çš„å…³é”®æ€æƒ³æ˜¯åœ¨ä¸€å¯¹å•ç‹¬çš„å‰å‘å’Œåå‘å—å†…é‡å è®¡ç®—å’Œé€šä¿¡ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå—åˆ†ä¸ºå››ä¸ªç»„ä»¶ï¼šæ³¨æ„åŠ›ã€å…¨å¯¹å…¨è°ƒåº¦ã€MLP å’Œå…¨å¯¹å…¨ç»„åˆã€‚ç‰¹åˆ«æ˜¯ï¼Œå¯¹äºåå‘å—ï¼Œæ³¨æ„åŠ›å’Œ MLP è¿›ä¸€æ­¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œè¾“å…¥çš„åå‘å’Œæƒé‡çš„åå‘ï¼Œå°±åƒ ZeroBubbleï¼ˆQi ç­‰ï¼Œ2023bï¼‰ä¸€æ ·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ª PP é€šä¿¡ç»„ä»¶ã€‚å¦‚å›¾ 4 æ‰€ç¤ºï¼Œå¯¹äºä¸€å¯¹å‰å‘å’Œåå‘å—ï¼Œæˆ‘ä»¬é‡æ–°æ’åˆ—è¿™äº›ç»„ä»¶ï¼Œå¹¶æ‰‹åŠ¨è°ƒæ•´ä¸“ç”¨äºé€šä¿¡ä¸è®¡ç®—çš„ GPU SM çš„æ¯”ä¾‹ã€‚åœ¨è¿™ç§é‡å ç­–ç•¥ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿å…¨å¯¹å…¨å’Œ PP é€šä¿¡åœ¨æ‰§è¡ŒæœŸé—´éƒ½å¯ä»¥å®Œå…¨éšè—ã€‚é‰´äºé«˜æ•ˆçš„é‡å ç­–ç•¥ï¼Œå®Œæ•´çš„ DualPipe è°ƒåº¦å¦‚å›¾ 5 æ‰€ç¤ºã€‚å®ƒé‡‡ç”¨åŒå‘æµæ°´çº¿è°ƒåº¦ï¼ŒåŒæ—¶ä»æµæ°´çº¿çš„ä¸¤ç«¯æä¾›å¾®æ‰¹æ¬¡ï¼Œå¤§éƒ¨åˆ†é€šä¿¡å¯ä»¥å®Œå…¨é‡å ã€‚è¿™ç§é‡å è¿˜ç¡®ä¿ï¼Œéšç€æ¨¡å‹çš„è¿›ä¸€æ­¥æ‰©å±•ï¼Œåªè¦æˆ‘ä»¬ä¿æŒæ’å®šçš„è®¡ç®—-é€šä¿¡æ¯”ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥åœ¨èŠ‚ç‚¹ä¹‹é—´ä½¿ç”¨ç»†ç²’åº¦ä¸“å®¶ï¼ŒåŒæ—¶å®ç°æ¥è¿‘é›¶çš„å…¨å¯¹å…¨é€šä¿¡å¼€é”€ã€‚

![](/images/2025/DeepSeekV3/Figure5.png)

`å›¾5 ç¤ºä¾‹ DualPipe è°ƒåº¦ï¼Œ8 ä¸ª PP æ’åå’Œä¸¤ä¸ªæ–¹å‘çš„ 20 ä¸ªå¾®æ‰¹æ¬¡ã€‚åå‘æ–¹å‘çš„å¾®æ‰¹æ¬¡ä¸æ­£å‘æ–¹å‘çš„å¾®æ‰¹æ¬¡å¯¹ç§°ï¼Œå› æ­¤æˆ‘ä»¬ä¸ºäº†ç®€åŒ–è¯´æ˜çœç•¥äº†å®ƒä»¬çš„æ‰¹æ¬¡ IDã€‚ç”±å…±äº«é»‘è‰²è¾¹æ¡†åŒ…å›´çš„ä¸¤ä¸ªå•å…ƒæ ¼å…·æœ‰ç›¸äº’é‡å çš„è®¡ç®—å’Œé€šä¿¡ã€‚`

![](/images/2025/DeepSeekV3/Table2.png)

`è¡¨2 ä¸åŒæµæ°´çº¿å¹¶è¡Œæ–¹æ³•ä¹‹é—´çš„æµæ°´çº¿æ°”æ³¡å’Œå†…å­˜ä½¿ç”¨æƒ…å†µæ¯”è¾ƒã€‚ğ¹ è¡¨ç¤ºå‰å‘å—çš„æ‰§è¡Œæ—¶é—´ï¼Œğµ è¡¨ç¤ºå®Œæ•´åå‘å—çš„æ‰§è¡Œæ—¶é—´ï¼Œğ‘Š è¡¨ç¤ºâ€œæƒé‡åå‘â€å—çš„æ‰§è¡Œæ—¶é—´ï¼Œğ¹&ğµ è¡¨ç¤ºä¸¤ä¸ªç›¸äº’é‡å çš„å‰å‘å’Œåå‘å—çš„æ‰§è¡Œæ—¶é—´ã€‚`

In addition, even in more general scenarios without a heavy communication burden, Du-
alPipe still exhibits efficiency advantages. In Table 2, we summarize the pipeline bubbles and
memory usage across different PP methods. As shown in the table, compared with ZB1P (Qi
et al., 2023b) and 1F1B (Harlap et al., 2018), DualPipe significantly reduces the pipeline bubbles
while only increasing the peak activation memory by 1
ğ‘ƒğ‘ƒ times. Although DualPipe requires
keeping two copies of the model parameters, this does not significantly increase the memory
consumption since we use a large EP size during training. Compared with Chimera (Li and
Hoefler, 2021), DualPipe only requires that the pipeline stages and micro-batches be divisible by
2, without requiring micro-batches to be divisible by pipeline stages. In addition, for DualPipe,
neither the bubbles nor activation memory will increase as the number of micro-batches grows.

æ­¤å¤–ï¼Œå³ä½¿åœ¨æ²¡æœ‰æ²‰é‡é€šä¿¡è´Ÿæ‹…çš„æ›´ä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒDualPipe ä»ç„¶è¡¨ç°å‡ºæ•ˆç‡ä¼˜åŠ¿ã€‚å¦‚è¡¨ 2 æ‰€ç¤ºï¼Œæˆ‘ä»¬æ€»ç»“äº†ä¸åŒ PP æ–¹æ³•ä¹‹é—´çš„æµæ°´çº¿æ°”æ³¡å’Œå†…å­˜ä½¿ç”¨æƒ…å†µã€‚å¦‚è¡¨æ‰€ç¤ºï¼Œä¸ ZB1Pï¼ˆQi ç­‰ï¼Œ2023bï¼‰å’Œ 1F1Bï¼ˆHarlap ç­‰ï¼Œ2018ï¼‰ç›¸æ¯”ï¼ŒDualPipe æ˜¾è‘—å‡å°‘äº†æµæ°´çº¿æ°”æ³¡ï¼ŒåŒæ—¶åªå¢åŠ äº†å³°å€¼æ¿€æ´»å†…å­˜çš„ 1 ğ‘ƒğ‘ƒ å€ã€‚å°½ç®¡ DualPipe éœ€è¦ä¿ç•™ä¸¤ä»½æ¨¡å‹å‚æ•°çš„å‰¯æœ¬ï¼Œä½†ç”±äºæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨äº†å¤§çš„ EP å¤§å°ï¼Œè¿™å¹¶ä¸ä¼šæ˜¾è‘—å¢åŠ å†…å­˜æ¶ˆè€—ã€‚ä¸ Chimeraï¼ˆLi å’Œ Hoeflerï¼Œ2021ï¼‰ç›¸æ¯”ï¼ŒDualPipe åªéœ€è¦æµæ°´çº¿é˜¶æ®µå’Œå¾®æ‰¹æ¬¡å¯è¢« 2 æ•´é™¤ï¼Œè€Œä¸éœ€è¦å¾®æ‰¹æ¬¡å¯è¢«æµæ°´çº¿é˜¶æ®µæ•´é™¤ã€‚æ­¤å¤–ï¼Œå¯¹äº DualPipeï¼Œéšç€å¾®æ‰¹æ¬¡æ•°é‡çš„å¢åŠ ï¼Œæ°”æ³¡å’Œæ¿€æ´»å†…å­˜éƒ½ä¸ä¼šå¢åŠ ã€‚

#### 3.2.2. Efficient Implementation of Cross-Node All-to-All Communicationï¼ˆè·¨èŠ‚ç‚¹å…¨å¯¹å…¨é€šä¿¡çš„é«˜æ•ˆå®ç°ï¼‰

In order to ensure sufficient computational performance for DualPipe, we customize efficient
cross-node all-to-all communication kernels (including dispatching and combining) to conserve
the number of SMs dedicated to communication. The implementation of the kernels is co-
designed with the MoE gating algorithm and the network topology of our cluster. To be specific,
in our cluster, cross-node GPUs are fully interconnected with IB, and intra-node communications
are handled via NVLink. NVLink offers a bandwidth of 160 GB/s, roughly 3.2 times that of IB
(50 GB/s). To effectively leverage the different bandwidths of IB and NVLink, we limit each
token to be dispatched to at most 4 nodes, thereby reducing IB traffic. For each token, when its
routing decision is made, it will first be transmitted via IB to the GPUs with the same in-node
index on its target nodes. Once it reaches the target nodes, we will endeavor to ensure that it is
instantaneously forwarded via NVLink to specific GPUs that host their target experts, without
being blocked by subsequently arriving tokens. In this way, communications via IB and NVLink
are fully overlapped, and each token can efficiently select an average of 3.2 experts per node
without incurring additional overhead from NVLink. This implies that, although DeepSeek-V3
selects only 8 routed experts in practice, it can scale up this number to a maximum of 13 experts
(4 nodes Ã—3.2 experts/node) while preserving the same communication cost. Overall, under
such a communication strategy, only 20 SMs are sufficient to fully utilize the bandwidths of IB
and NVLink.

ä¸ºäº†ç¡®ä¿ DualPipe çš„å……è¶³è®¡ç®—æ€§èƒ½ï¼Œæˆ‘ä»¬å®šåˆ¶äº†é«˜æ•ˆçš„è·¨èŠ‚ç‚¹å…¨å¯¹å…¨é€šä¿¡å†…æ ¸ï¼ˆåŒ…æ‹¬è°ƒåº¦å’Œç»„åˆï¼‰ï¼Œä»¥èŠ‚çœä¸“ç”¨äºé€šä¿¡çš„ SM æ•°é‡ã€‚å†…æ ¸çš„å®ç°ä¸ MoE é—¨æ§ç®—æ³•å’Œæˆ‘ä»¬é›†ç¾¤çš„ç½‘ç»œæ‹“æ‰‘å…±åŒè®¾è®¡ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨æˆ‘ä»¬çš„é›†ç¾¤ä¸­ï¼Œè·¨èŠ‚ç‚¹ GPU é€šè¿‡ IB å®Œå…¨äº’è¿ï¼ŒèŠ‚ç‚¹å†…é€šä¿¡é€šè¿‡ NVLink å¤„ç†ã€‚NVLink æä¾›çš„å¸¦å®½ä¸º 160 GB/sï¼Œå¤§çº¦æ˜¯ IBï¼ˆ50 GB/sï¼‰çš„ 3.2 å€ã€‚ä¸ºäº†æœ‰æ•ˆåˆ©ç”¨ IB å’Œ NVLink çš„ä¸åŒå¸¦å®½ï¼Œæˆ‘ä»¬é™åˆ¶æ¯ä¸ªæ ‡è®°æœ€å¤šåˆ†å‘åˆ° 4 ä¸ªèŠ‚ç‚¹ï¼Œä»è€Œå‡å°‘ IB æµé‡ã€‚å¯¹äºæ¯ä¸ªæ ‡è®°ï¼Œå½“å…¶è·¯ç”±å†³ç­–ç¡®å®šåï¼Œå®ƒå°†é¦–å…ˆé€šè¿‡ IB ä¼ è¾“åˆ°ç›®æ ‡èŠ‚ç‚¹ä¸Šå…·æœ‰ç›¸åŒèŠ‚ç‚¹ç´¢å¼•çš„ GPUã€‚ä¸€æ—¦åˆ°è¾¾ç›®æ ‡èŠ‚ç‚¹ï¼Œæˆ‘ä»¬å°†åŠªåŠ›ç¡®ä¿å®ƒç«‹å³é€šè¿‡ NVLink è½¬å‘åˆ°æ‰˜ç®¡å…¶ç›®æ ‡ä¸“å®¶çš„ç‰¹å®š GPUï¼Œè€Œä¸ä¼šè¢«éšååˆ°è¾¾çš„æ ‡è®°é˜»å¡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œé€šè¿‡ IB å’Œ NVLink çš„é€šä¿¡å®Œå…¨é‡å ï¼Œæ¯ä¸ªæ ‡è®°å¯ä»¥åœ¨ä¸å¢åŠ  NVLink çš„é¢å¤–å¼€é”€çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°é€‰æ‹©æ¯ä¸ªèŠ‚ç‚¹çš„å¹³å‡ 3.2 ä¸ªä¸“å®¶ã€‚è¿™æ„å‘³ç€ï¼Œå°½ç®¡ DeepSeek-V3 åœ¨å®è·µä¸­ä»…é€‰æ‹©äº† 8 ä¸ªè·¯ç”±ä¸“å®¶ï¼Œä½†å®ƒå¯ä»¥å°†è¿™ä¸ªæ•°å­—æ‰©å±•åˆ°æœ€å¤š 13 ä¸ªä¸“å®¶ï¼ˆ4 ä¸ªèŠ‚ç‚¹ Ã—3.2 ä¸ªä¸“å®¶/èŠ‚ç‚¹ï¼‰ï¼ŒåŒæ—¶ä¿æŒç›¸åŒçš„é€šä¿¡æˆæœ¬ã€‚æ€»çš„æ¥è¯´ï¼Œåœ¨è¿™ç§é€šä¿¡ç­–ç•¥ä¸‹ï¼Œåªéœ€è¦ 20 ä¸ª SM å°±è¶³ä»¥å……åˆ†åˆ©ç”¨ IB å’Œ NVLink çš„å¸¦å®½ã€‚

In detail, we employ the warp specialization technique (Bauer et al., 2014) and partition
20 SMs into 10 communication channels. During the dispatching process, (1) IB sending, (2)
IB-to-NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The
number of warps allocated to each communication task is dynamically adjusted according to the
actual workload across all SMs. Similarly, during the combining process, (1) NVLink sending,
(2) NVLink-to-IB forwarding and accumulation, and (3) IB receiving and accumulation are also
handled by dynamically adjusted warps. In addition, both dispatching and combining kernels
overlap with the computation stream, so we also consider their impact on other SM computation
kernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and
auto-tune the communication chunk size, which significantly reduces the use of the L2 cache
and the interference to other SMs.

å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é‡‡ç”¨äº† warp ä¸“ç”¨åŒ–æŠ€æœ¯ï¼ˆBauer ç­‰ï¼Œ2014ï¼‰ï¼Œå°† 20 ä¸ª SM åˆ’åˆ†ä¸º 10 ä¸ªé€šä¿¡é€šé“ã€‚åœ¨è°ƒåº¦è¿‡ç¨‹ä¸­ï¼ŒIB å‘é€ã€IB åˆ° NVLink è½¬å‘å’Œ NVLink æ¥æ”¶ç”±å„è‡ªçš„ warp å¤„ç†ã€‚æ ¹æ®æ‰€æœ‰ SM çš„å®é™…å·¥ä½œè´Ÿè½½ï¼Œä¸ºæ¯ä¸ªé€šä¿¡ä»»åŠ¡åˆ†é…çš„ warp æ•°é‡æ˜¯åŠ¨æ€è°ƒæ•´çš„ã€‚ç±»ä¼¼åœ°ï¼Œåœ¨ç»„åˆè¿‡ç¨‹ä¸­ï¼ŒNVLink å‘é€ã€NVLink åˆ° IB è½¬å‘å’Œç´¯ç§¯ï¼Œä»¥åŠ IB æ¥æ”¶å’Œç´¯ç§¯ä¹Ÿç”±åŠ¨æ€è°ƒæ•´çš„ warp å¤„ç†ã€‚æ­¤å¤–ï¼Œè°ƒåº¦å’Œç»„åˆå†…æ ¸éƒ½ä¸è®¡ç®—æµé‡å ï¼Œå› æ­¤æˆ‘ä»¬è¿˜è€ƒè™‘å®ƒä»¬å¯¹å…¶ä»– SM è®¡ç®—å†…æ ¸çš„å½±å“ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ä½¿ç”¨å®šåˆ¶çš„ PTXï¼ˆå¹¶è¡Œçº¿ç¨‹æ‰§è¡Œï¼‰æŒ‡ä»¤ï¼Œå¹¶è‡ªåŠ¨è°ƒæ•´é€šä¿¡å—å¤§å°ï¼Œè¿™æ˜¾è‘—å‡å°‘äº† L2 ç¼“å­˜çš„ä½¿ç”¨å’Œå¯¹å…¶ä»– SM çš„å¹²æ‰°ã€‚

#### 3.2.3. Extremely Memory Saving with Minimal Overheadï¼ˆæå¤§èŠ‚çœå†…å­˜ï¼Œå¼€é”€æœ€å°ï¼‰

In order to reduce the memory footprint during training, we employ the following techniques.

ä¸ºäº†å‡å°‘è®­ç»ƒæœŸé—´çš„å†…å­˜å ç”¨ï¼Œæˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹æŠ€æœ¯ã€‚

**Recomputation of RMSNorm and MLA Up-Projection.** We recompute all RMSNorm op-
erations and MLA up-projections during back-propagation, thereby eliminating the need to
persistently store their output activations. With a minor overhead, this strategy significantly
reduces memory requirements for storing activations.

**RMSNorm å’Œ MLA ä¸ŠæŠ•å½±çš„é‡æ–°è®¡ç®—ã€‚** æˆ‘ä»¬åœ¨åå‘ä¼ æ’­æœŸé—´é‡æ–°è®¡ç®—æ‰€æœ‰ RMSNorm æ“ä½œå’Œ MLA ä¸ŠæŠ•å½±ï¼Œä»è€Œæ¶ˆé™¤äº†éœ€è¦æŒä¹…å­˜å‚¨å®ƒä»¬çš„è¾“å‡ºæ¿€æ´»çš„éœ€æ±‚ã€‚é€šè¿‡å°çš„å¼€é”€ï¼Œè¿™ç§ç­–ç•¥æ˜¾è‘—å‡å°‘äº†å­˜å‚¨æ¿€æ´»æ‰€éœ€çš„å†…å­˜ã€‚

**Exponential Moving Average in CPU.** During training, we preserve the Exponential Mov-
ing Average (EMA) of the model parameters for early estimation of the model performance
after learning rate decay. The EMA parameters are stored in CPU memory and are updated
asynchronously after each training step. This method allows us to maintain EMA parameters
without incurring additional memory or time overhead.

**CPU ä¸­çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ã€‚** åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬ä¿ç•™æ¨¡å‹å‚æ•°çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ (EMA)ï¼Œä»¥ä¾¿åœ¨å­¦ä¹ ç‡è¡°å‡åå¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œæ—©æœŸä¼°è®¡ã€‚EMA å‚æ•°å­˜å‚¨åœ¨ CPU å†…å­˜ä¸­ï¼Œå¹¶åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤åå¼‚æ­¥æ›´æ–°ã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸äº§ç”Ÿé¢å¤–å†…å­˜æˆ–æ—¶é—´å¼€é”€çš„æƒ…å†µä¸‹ç»´æŠ¤ EMA å‚æ•°ã€‚

**Shared Embedding and Output Head for Multi-Token Prediction.** With the DualPipe strategy,
we deploy the shallowest layers (including the embedding layer) and deepest layers (including
the output head) of the model on the same PP rank. This arrangement enables the physical
sharing of parameters and gradients, of the shared embedding and output head, between the
MTP module and the main model. This physical sharing mechanism further enhances our
memory efficiency.

**å¤šæ ‡è®°é¢„æµ‹çš„å…±äº«åµŒå…¥å’Œè¾“å‡ºå¤´ã€‚** å€ŸåŠ© DualPipe ç­–ç•¥ï¼Œæˆ‘ä»¬å°†æ¨¡å‹çš„æœ€æµ…å±‚ï¼ˆåŒ…æ‹¬åµŒå…¥å±‚ï¼‰å’Œæœ€æ·±å±‚ï¼ˆåŒ…æ‹¬è¾“å‡ºå¤´ï¼‰éƒ¨ç½²åœ¨åŒä¸€ä¸ª PP æ’åä¸Šã€‚è¿™ç§å®‰æ’ä½¿å¾— MTP æ¨¡å—å’Œä¸»æ¨¡å‹ä¹‹é—´çš„å…±äº«åµŒå…¥å’Œè¾“å‡ºå¤´çš„å‚æ•°å’Œæ¢¯åº¦å¯ä»¥ç‰©ç†å…±äº«ã€‚è¿™ç§ç‰©ç†å…±äº«æœºåˆ¶è¿›ä¸€æ­¥æé«˜äº†æˆ‘ä»¬çš„å†…å­˜æ•ˆç‡ã€‚

### 3.3. FP8 Trainingï¼ˆFP8 è®­ç»ƒï¼‰

### 3.4. Inference and Deploymentï¼ˆæ¨ç†å’Œéƒ¨ç½²ï¼‰

We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected
using NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously
ensure both the Service-Level Objective (SLO) for online services and high throughput, we
employ the following deployment strategy that separates the prefilling and decoding stages.

æˆ‘ä»¬åœ¨ H800 é›†ç¾¤ä¸Šéƒ¨ç½² DeepSeek-V3ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹å†…çš„ GPU ä½¿ç”¨ NVLink è¿›è¡Œäº’è¿ï¼Œæ•´ä¸ªé›†ç¾¤ä¸­çš„æ‰€æœ‰ GPU é€šè¿‡ IB å®Œå…¨äº’è¿ã€‚ä¸ºäº†åŒæ—¶ç¡®ä¿åœ¨çº¿æœåŠ¡çš„æœåŠ¡çº§ç›®æ ‡ (SLO) å’Œé«˜ååé‡ï¼Œæˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹éƒ¨ç½²ç­–ç•¥ï¼Œå°†é¢„å¡«å……å’Œè§£ç é˜¶æ®µåˆ†å¼€ã€‚

#### 3.4.1. Prefillingï¼ˆé¢„å¡«å……ï¼‰

The minimum deployment unit of the prefilling stage consists of 4 nodes with 32 GPUs. The
attention part employs 4-way Tensor Parallelism (TP4) with Sequence Parallelism (SP), com-
bined with 8-way Data Parallelism (DP8). Its small TP size of 4 limits the overhead of TP
communication. For the MoE part, we use 32-way Expert Parallelism (EP32), which ensures that
each expert processes a sufficiently large batch size, thereby enhancing computational efficiency.
For the MoE all-to-all communication, we use the same method as in training: first transferring
tokens across nodes via IB, and then forwarding among the intra-node GPUs via NVLink. In
particular, we use 1-way Tensor Parallelism for the dense MLPs in shallow layers to save TP
communication.

é¢„å¡«å……é˜¶æ®µçš„æœ€å°éƒ¨ç½²å•å…ƒç”± 4 ä¸ªèŠ‚ç‚¹å’Œ 32 ä¸ª GPU ç»„æˆã€‚æ³¨æ„åŠ›éƒ¨åˆ†é‡‡ç”¨ 4 è·¯å¼ é‡å¹¶è¡Œ (TP4) ä¸åºåˆ—å¹¶è¡Œ (SP) ç»“åˆï¼Œç»“åˆ 8 è·¯æ•°æ®å¹¶è¡Œ (DP8)ã€‚å…¶å°çš„ TP å¤§å°ä¸º 4ï¼Œé™åˆ¶äº† TP é€šä¿¡çš„å¼€é”€ã€‚å¯¹äº MoE éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä½¿ç”¨ 32 è·¯ä¸“å®¶å¹¶è¡Œ (EP32)ï¼Œç¡®ä¿æ¯ä¸ªä¸“å®¶å¤„ç†è¶³å¤Ÿå¤§çš„æ‰¹é‡å¤§å°ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡ã€‚å¯¹äº MoE å…¨å¯¹å…¨é€šä¿¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æ–¹æ³•ï¼šé¦–å…ˆé€šè¿‡ IB åœ¨èŠ‚ç‚¹ä¹‹é—´ä¼ è¾“æ ‡è®°ï¼Œç„¶åé€šè¿‡ NVLink åœ¨èŠ‚ç‚¹å†… GPU ä¹‹é—´è½¬å‘ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åœ¨æµ…å±‚ä¸­çš„å¯†é›† MLP ä¸­ä½¿ç”¨ 1 è·¯å¼ é‡å¹¶è¡Œæ¥èŠ‚çœ TP é€šä¿¡ã€‚

To achieve load balancing among different experts in the MoE part, we need to ensure that
each GPU processes approximately the same number of tokens. To this end, we introduce a
deployment strategy of redundant experts, which duplicates high-load experts and deploys them
redundantly. The high-load experts are detected based on statistics collected during the online
deployment and are adjusted periodically (e.g., every 10 minutes). After determining the set
of redundant experts, we carefully rearrange experts among GPUs within a node based on the
observed loads, striving to balance the load across GPUs as much as possible without increasing
the cross-node all-to-all communication overhead. For the deployment of DeepSeek-V3, we set
32 redundant experts for the prefilling stage. For each GPU, besides the original 8 experts it
hosts, it will also host one additional redundant expert.

ä¸ºäº†åœ¨ MoE éƒ¨åˆ†çš„ä¸åŒä¸“å®¶ä¹‹é—´å®ç°è´Ÿè½½å¹³è¡¡ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æ¯ä¸ª GPU å¤„ç†å¤§çº¦ç›¸åŒæ•°é‡çš„æ ‡è®°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å†—ä½™ä¸“å®¶çš„éƒ¨ç½²ç­–ç•¥ï¼Œå¤åˆ¶é«˜è´Ÿè½½ä¸“å®¶å¹¶å°†å…¶å†—ä½™éƒ¨ç½²ã€‚é«˜è´Ÿè½½ä¸“å®¶æ˜¯åŸºäºåœ¨çº¿éƒ¨ç½²æœŸé—´æ”¶é›†çš„ç»Ÿè®¡æ•°æ®æ£€æµ‹çš„ï¼Œå¹¶å®šæœŸè°ƒæ•´ï¼ˆä¾‹å¦‚ï¼Œæ¯ 10 åˆ†é’Ÿï¼‰ã€‚ç¡®å®šå†—ä½™ä¸“å®¶é›†ä¹‹åï¼Œæˆ‘ä»¬æ ¹æ®è§‚å¯Ÿåˆ°çš„è´Ÿè½½åœ¨èŠ‚ç‚¹å†…çš„ GPU ä¹‹é—´ä»”ç»†é‡æ–°æ’åˆ—ä¸“å®¶ï¼ŒåŠªåŠ›åœ¨ä¸å¢åŠ è·¨èŠ‚ç‚¹å…¨å¯¹å…¨é€šä¿¡å¼€é”€çš„æƒ…å†µä¸‹å°½å¯èƒ½å¹³è¡¡ GPU ä¹‹é—´çš„è´Ÿè½½ã€‚å¯¹äº DeepSeek-V3 çš„éƒ¨ç½²ï¼Œæˆ‘ä»¬ä¸ºé¢„å¡«å……é˜¶æ®µè®¾ç½®äº† 32 ä¸ªå†—ä½™ä¸“å®¶ã€‚å¯¹äºæ¯ä¸ª GPUï¼Œé™¤äº†å®ƒåŸæ¥æ‰˜ç®¡çš„ 8 ä¸ªä¸“å®¶ä¹‹å¤–ï¼Œå®ƒè¿˜å°†æ‰˜ç®¡ä¸€ä¸ªé¢å¤–çš„å†—ä½™ä¸“å®¶ã€‚

Furthermore, in the prefilling stage, to improve the throughput and hide the overhead of
all-to-all and TP communication, we simultaneously process two micro-batches with similar
computational workloads, overlapping the attention and MoE of one micro-batch with the
dispatch and combine of another.

æ­¤å¤–ï¼Œåœ¨é¢„å¡«å……é˜¶æ®µï¼Œä¸ºäº†æé«˜ååé‡å¹¶éšè—å…¨å¯¹å…¨å’Œ TP é€šä¿¡çš„å¼€é”€ï¼Œæˆ‘ä»¬åŒæ—¶å¤„ç†ä¸¤ä¸ªå…·æœ‰ç›¸ä¼¼è®¡ç®—å·¥ä½œè´Ÿè½½çš„å¾®æ‰¹æ¬¡ï¼Œå°†ä¸€ä¸ªå¾®æ‰¹æ¬¡çš„æ³¨æ„åŠ›å’Œ MoE ä¸å¦ä¸€ä¸ªçš„è°ƒåº¦å’Œç»„åˆé‡å ã€‚

Finally, we are exploring a dynamic redundancy strategy for experts, where each GPU hosts
more experts (e.g., 16 experts), but only 9 will be activated during each inference step. Before
the all-to-all operation at each layer begins, we compute the globally optimal routing scheme
on the fly. Given the substantial computation involved in the prefilling stage, the overhead of
computing this routing scheme is almost negligible.

æœ€åï¼Œæˆ‘ä»¬æ­£åœ¨æ¢ç´¢ä¸€ç§ä¸“å®¶çš„åŠ¨æ€å†—ä½™ç­–ç•¥ï¼Œå…¶ä¸­æ¯ä¸ª GPU æ‰˜ç®¡æ›´å¤šçš„ä¸“å®¶ï¼ˆä¾‹å¦‚ï¼Œ16 ä¸ªä¸“å®¶ï¼‰ï¼Œä½†æ¯ä¸ªæ¨ç†æ­¥éª¤åªä¼šæ¿€æ´» 9 ä¸ªã€‚åœ¨æ¯ä¸ªå±‚çš„å…¨å¯¹å…¨æ“ä½œå¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬ä¼šåŠ¨æ€è®¡ç®—å…¨å±€æœ€ä¼˜è·¯ç”±æ–¹æ¡ˆã€‚é‰´äºé¢„å¡«å……é˜¶æ®µæ¶‰åŠçš„å¤§é‡è®¡ç®—ï¼Œè®¡ç®—è¿™ç§è·¯ç”±æ–¹æ¡ˆçš„å¼€é”€å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚

#### 3.4.2. Decodingï¼ˆè§£ç ï¼‰

During decoding, we treat the shared expert as a routed one. From this perspective, each token
will select 9 experts during routing, where the shared expert is regarded as a heavy-load one
that will always be selected. The minimum deployment unit of the decoding stage consists
of 40 nodes with 320 GPUs. The attention part employs TP4 with SP, combined with DP80,
while the MoE part uses EP320. For the MoE part, each GPU hosts only one expert, and 64 GPUs
are responsible for hosting redundant experts and shared experts. All-to-all communication
of the dispatch and combine parts is performed via direct point-to-point transfers over IB to
achieve low latency. Additionally, we leverage the IBGDA (NVIDIA, 2022) technology to further
minimize latency and enhance communication efficiency.

åœ¨è§£ç æœŸé—´ï¼Œæˆ‘ä»¬å°†å…±äº«ä¸“å®¶è§†ä¸ºè·¯ç”±ä¸“å®¶ã€‚ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œæ¯ä¸ªæ ‡è®°åœ¨è·¯ç”±è¿‡ç¨‹ä¸­å°†é€‰æ‹© 9 ä¸ªä¸“å®¶ï¼Œå…¶ä¸­å…±äº«ä¸“å®¶è¢«è§†ä¸ºä¸€ä¸ªå§‹ç»ˆä¼šè¢«é€‰æ‹©çš„é«˜è´Ÿè½½ä¸“å®¶ã€‚è§£ç é˜¶æ®µçš„æœ€å°éƒ¨ç½²å•å…ƒç”± 40 ä¸ªèŠ‚ç‚¹å’Œ 320 ä¸ª GPU ç»„æˆã€‚æ³¨æ„åŠ›éƒ¨åˆ†é‡‡ç”¨ TP4 ä¸ SP ç»“åˆï¼Œç»“åˆ DP80ï¼Œè€Œ MoE éƒ¨åˆ†ä½¿ç”¨ EP320ã€‚å¯¹äº MoE éƒ¨åˆ†ï¼Œæ¯ä¸ª GPU ä»…æ‰˜ç®¡ä¸€ä¸ªä¸“å®¶ï¼Œ64 ä¸ª GPU è´Ÿè´£æ‰˜ç®¡å†—ä½™ä¸“å®¶å’Œå…±äº«ä¸“å®¶ã€‚è°ƒåº¦å’Œç»„åˆéƒ¨åˆ†çš„å…¨å¯¹å…¨é€šä¿¡é€šè¿‡ IB ä¸Šçš„ç›´æ¥ç‚¹å¯¹ç‚¹ä¼ è¾“è¿›è¡Œï¼Œä»¥å®ç°ä½å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨ IBGDAï¼ˆNVIDIAï¼Œ2022ï¼‰æŠ€æœ¯è¿›ä¸€æ­¥é™ä½å»¶è¿Ÿå¹¶æé«˜é€šä¿¡æ•ˆç‡ã€‚

Similar to prefilling, we periodically determine the set of redundant experts in a certain
interval, based on the statistical expert load from our online service. However, we do not need
to rearrange experts since each GPU only hosts one expert. We are also exploring the dynamic
redundancy strategy for decoding. However, this requires more careful optimization of the
algorithm that computes the globally optimal routing scheme and the fusion with the dispatch
kernel to reduce overhead.

ä¸é¢„å¡«å……ç±»ä¼¼ï¼Œæˆ‘ä»¬æ ¹æ®åœ¨çº¿æœåŠ¡çš„ç»Ÿè®¡ä¸“å®¶è´Ÿè½½ï¼Œå‘¨æœŸæ€§åœ°åœ¨ä¸€å®šé—´éš”å†…ç¡®å®šå†—ä½™ä¸“å®¶é›†ã€‚ä½†æ˜¯ï¼Œç”±äºæ¯ä¸ª GPU ä»…æ‰˜ç®¡ä¸€ä¸ªä¸“å®¶ï¼Œæˆ‘ä»¬æ— éœ€é‡æ–°æ’åˆ—ä¸“å®¶ã€‚æˆ‘ä»¬è¿˜åœ¨æ¢ç´¢è§£ç çš„åŠ¨æ€å†—ä½™ç­–ç•¥ã€‚ä½†æ˜¯ï¼Œè¿™éœ€è¦æ›´ä»”ç»†åœ°ä¼˜åŒ–è®¡ç®—å…¨å±€æœ€ä¼˜è·¯ç”±æ–¹æ¡ˆçš„ç®—æ³•ï¼Œå¹¶ä¸è°ƒåº¦å†…æ ¸èåˆä»¥å‡å°‘å¼€é”€ã€‚

Additionally, to enhance throughput and hide the overhead of all-to-all communication,
we are also exploring processing two micro-batches with similar computational workloads
simultaneously in the decoding stage. Unlike prefilling, attention consumes a larger portion
of time in the decoding stage. Therefore, we overlap the attention of one micro-batch with
the dispatch+MoE+combine of another. In the decoding stage, the batch size per expert
is relatively small (usually within 256 tokens), and the bottleneck is memory access rather
than computation. Since the MoE part only needs to load the parameters of one expert, the
memory access overhead is minimal, so using fewer SMs will not significantly affect the overall
performance. Therefore, to avoid impacting the computation speed of the attention part, we
can allocate only a small portion of SMs to dispatch+MoE+combine.

æ­¤å¤–ï¼Œä¸ºäº†æé«˜ååé‡å¹¶éšè—å…¨å¯¹å…¨é€šä¿¡çš„å¼€é”€ï¼Œæˆ‘ä»¬è¿˜åœ¨è§£ç é˜¶æ®µæ¢ç´¢åŒæ—¶å¤„ç†ä¸¤ä¸ªå…·æœ‰ç›¸ä¼¼è®¡ç®—å·¥ä½œè´Ÿè½½çš„å¾®æ‰¹æ¬¡ã€‚ä¸é¢„å¡«å……ä¸åŒï¼Œæ³¨æ„åŠ›åœ¨è§£ç é˜¶æ®µå ç”¨äº†æ›´å¤§çš„æ—¶é—´æ¯”ä¾‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªå¾®æ‰¹æ¬¡çš„æ³¨æ„åŠ›ä¸å¦ä¸€ä¸ªçš„è°ƒåº¦+MoE+ç»„åˆé‡å ã€‚åœ¨è§£ç é˜¶æ®µï¼Œæ¯ä¸ªä¸“å®¶çš„æ‰¹é‡å¤§å°ç›¸å¯¹è¾ƒå°ï¼ˆé€šå¸¸åœ¨ 256 ä¸ªæ ‡è®°å†…ï¼‰ï¼Œç“¶é¢ˆæ˜¯å†…å­˜è®¿é—®è€Œä¸æ˜¯è®¡ç®—ã€‚ç”±äº MoE éƒ¨åˆ†åªéœ€è¦åŠ è½½ä¸€ä¸ªä¸“å®¶çš„å‚æ•°ï¼Œå†…å­˜è®¿é—®å¼€é”€å¾ˆå°ï¼Œå› æ­¤ä½¿ç”¨æ›´å°‘çš„ SM ä¸ä¼šæ˜¾è‘—å½±å“æ•´ä½“æ€§èƒ½ã€‚å› æ­¤ï¼Œä¸ºäº†é¿å…å½±å“æ³¨æ„åŠ›éƒ¨åˆ†çš„è®¡ç®—é€Ÿåº¦ï¼Œæˆ‘ä»¬å¯ä»¥å°†åªåˆ†é…ä¸€å°éƒ¨åˆ† SM ç»™è°ƒåº¦+MoE+ç»„åˆã€‚

### 3.5. Suggestions on Hardware Designï¼ˆç¡¬ä»¶è®¾è®¡å»ºè®®ï¼‰


## 4. Pre-Trainingï¼ˆé¢„è®­ç»ƒï¼‰


## 5. Post-Trainingï¼ˆåè®­ç»ƒï¼‰


## 6. Conclusion, Limitations, and Future Directionsï¼ˆç»“è®ºã€å±€é™æ€§å’Œæœªæ¥æ–¹å‘ï¼‰

In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total pa-
rameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and
DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing
and sets a multi-token prediction training objective for stronger performance. The training of
DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering op-
timizations. The post-training also makes a success in distilling the reasoning capability from the
DeepSeek-R1 series of models. Comprehensive evaluations demonstrate that DeepSeek-V3 has
emerged as the strongest open-source model currently available, and achieves performance com-
parable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. Despite its strong
performance, it also maintains economical training costs. It requires only 2.788M H800 GPU
hours for its full training, including pre-training, context length extension, and post-training.

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† DeepSeek-V3ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰ 671B æ€»å‚æ•°å’Œ 37B æ¿€æ´»å‚æ•°çš„å¤§å‹ MoE è¯­è¨€æ¨¡å‹ï¼Œè®­ç»ƒäº† 14.8T ä¸ªæ ‡è®°ã€‚é™¤äº† MLA å’Œ DeepSeekMoE æ¶æ„ä¹‹å¤–ï¼Œå®ƒè¿˜ä¸ºè´Ÿè½½å¹³è¡¡å¼€åˆ›äº†ä¸€ç§æ— è¾…åŠ©æŸå¤±ç­–ç•¥ï¼Œå¹¶ä¸ºæ›´å¼ºçš„æ€§èƒ½è®¾å®šäº†å¤šæ ‡è®°é¢„æµ‹è®­ç»ƒç›®æ ‡ã€‚DeepSeek-V3 çš„è®­ç»ƒå…·æœ‰æˆæœ¬æ•ˆç›Šï¼Œå› ä¸ºå®ƒæ”¯æŒ FP8 è®­ç»ƒå’Œç»†è‡´çš„å·¥ç¨‹ä¼˜åŒ–ã€‚åè®­ç»ƒè¿˜æˆåŠŸåœ°ä» DeepSeek-R1 ç³»åˆ—æ¨¡å‹ä¸­æå–äº†æ¨ç†èƒ½åŠ›ã€‚å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼ŒDeepSeek-V3 å·²æˆä¸ºç›®å‰æœ€å¼ºå¤§çš„å¼€æºæ¨¡å‹ï¼Œå¹¶å®ç°äº†ä¸é¢†å…ˆçš„é—­æºæ¨¡å‹ï¼ˆå¦‚ GPT-4o å’Œ Claude-3.5-Sonnetï¼‰å¯æ¯”çš„æ€§èƒ½ã€‚å°½ç®¡æ€§èƒ½å¼ºåŠ²ï¼Œä½†å®ƒä¹Ÿä¿æŒäº†ç»æµçš„è®­ç»ƒæˆæœ¬ã€‚å®ƒçš„å…¨é¢è®­ç»ƒä»…éœ€è¦ 2.788M H800 GPU å°æ—¶ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•å’Œåè®­ç»ƒã€‚

While acknowledging its strong performance and cost-effectiveness, we also recognize that
DeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient
inference, the recommended deployment unit for DeepSeek-V3 is relatively large, which might
pose a burden for small-sized teams. Secondly, although our deployment strategy for DeepSeek-
V3 has achieved an end-to-end generation speed of more than two times that of DeepSeek-V2,
there still remains potential for further enhancement. Fortunately, these limitations are expected
to be naturally addressed with the development of more advanced hardware.

å°½ç®¡æ‰¿è®¤å…¶å¼ºå¤§çš„æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šï¼Œæˆ‘ä»¬ä¹Ÿè®¤è¯†åˆ° DeepSeek-V3 å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éƒ¨ç½²æ–¹é¢ã€‚é¦–å…ˆï¼Œä¸ºäº†ç¡®ä¿é«˜æ•ˆæ¨ç†ï¼ŒDeepSeek-V3 çš„æ¨èéƒ¨ç½²å•å…ƒç›¸å¯¹è¾ƒå¤§ï¼Œè¿™å¯èƒ½å¯¹å°å‹å›¢é˜Ÿæ„æˆè´Ÿæ‹…ã€‚å…¶æ¬¡ï¼Œå°½ç®¡æˆ‘ä»¬ä¸º DeepSeek-V3 çš„éƒ¨ç½²ç­–ç•¥å®ç°äº†æ¯” DeepSeek-V2 å¿«ä¸¤å€ä»¥ä¸Šçš„ç«¯åˆ°ç«¯ç”Ÿæˆé€Ÿåº¦ï¼Œä½†ä»æœ‰è¿›ä¸€æ­¥æå‡çš„æ½œåŠ›ã€‚å¹¸è¿çš„æ˜¯ï¼Œéšç€æ›´å…ˆè¿›ç¡¬ä»¶çš„å‘å±•ï¼Œè¿™äº›å±€é™æ€§æœ‰æœ›å¾—åˆ°è‡ªç„¶è§£å†³ã€‚

DeepSeek consistently adheres to the route of open-source models with longtermism, aiming
to steadily approach the ultimate goal of AGI (Artificial General Intelligence). In the future, we
plan to strategically invest in research across the following directions.

DeepSeek å§‹ç»ˆåšæŒå¼€æºæ¨¡å‹çš„é•¿æœŸä¸»ä¹‰è·¯çº¿ï¼Œæ—¨åœ¨ç¨³æ­¥æ¥è¿‘ AGIï¼ˆäººå·¥é€šç”¨æ™ºèƒ½ï¼‰çš„æœ€ç»ˆç›®æ ‡ã€‚åœ¨æœªæ¥ï¼Œæˆ‘ä»¬è®¡åˆ’åœ¨ä»¥ä¸‹æ–¹å‘ä¸Šè¿›è¡Œæˆ˜ç•¥æ€§æŠ•èµ„ç ”ç©¶ã€‚

- We will consistently study and refine our model architectures, aiming to further improve both the training and inference efficiency, striving to approach efficient support for infinite context length. Additionally, we will try to break through the architectural limitations of Transformer, thereby pushing the boundaries of its modeling capabilities.

- æˆ‘ä»¬å°†æŒç»­ç ”ç©¶å’Œå®Œå–„æˆ‘ä»¬çš„æ¨¡å‹æ¶æ„ï¼Œæ—¨åœ¨è¿›ä¸€æ­¥æé«˜è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼ŒåŠªåŠ›å®ç°å¯¹æ— é™ä¸Šä¸‹æ–‡é•¿åº¦çš„é«˜æ•ˆæ”¯æŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å°è¯•çªç ´ Transformer çš„æ¶æ„é™åˆ¶ï¼Œä»è€Œæ¨åŠ¨å…¶å»ºæ¨¡èƒ½åŠ›çš„è¾¹ç•Œã€‚

- We will continuously iterate on the quantity and quality of our training data, and explore the incorporation of additional training signal sources, aiming to drive data scaling across a more comprehensive range of dimensions.

- æˆ‘ä»¬å°†æŒç»­è¿­ä»£æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®çš„æ•°é‡å’Œè´¨é‡ï¼Œå¹¶æ¢ç´¢æ•´åˆé¢å¤–çš„è®­ç»ƒä¿¡å·æºï¼Œæ—¨åœ¨æ¨åŠ¨è·¨æ›´å…¨é¢ç»´åº¦çš„æ•°æ®æ‰©å±•ã€‚

- We will consistently explore and iterate on the deep thinking capabilities of our models, aiming to enhance their intelligence and problem-solving abilities by expanding their reasoning length and depth.

- æˆ‘ä»¬å°†æŒç»­æ¢ç´¢å’Œè¿­ä»£æˆ‘ä»¬æ¨¡å‹çš„æ·±åº¦æ€è€ƒèƒ½åŠ›ï¼Œæ—¨åœ¨é€šè¿‡æ‰©å±•å…¶æ¨ç†é•¿åº¦å’Œæ·±åº¦æ¥å¢å¼ºå…¶æ™ºèƒ½å’Œè§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚

- We will explore more comprehensive and multi-dimensional model evaluation methods to prevent the tendency towards optimizing a fixed set of benchmarks during research, which may create a misleading impression of the model capabilities and affect our foundational assessment.

- æˆ‘ä»¬å°†æ¢ç´¢æ›´å…¨é¢å’Œå¤šç»´çš„æ¨¡å‹è¯„ä¼°æ–¹æ³•ï¼Œä»¥é˜²æ­¢åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­ä¼˜åŒ–å›ºå®šä¸€ç»„åŸºå‡†çš„å€¾å‘ï¼Œè¿™å¯èƒ½ä¼šç»™äººä¸€ç§è¯¯å¯¼æ€§çš„æ¨¡å‹èƒ½åŠ›å°è±¡ï¼Œå¹¶å½±å“æˆ‘ä»¬çš„åŸºç¡€è¯„ä¼°ã€‚
