---
layout: single
title:  "åä¸º Atlas 800I A2 å¤§æ¨¡å‹éƒ¨ç½²å®æˆ˜ï¼ˆå…­ï¼‰ï¼švLLM éƒ¨ç½² LLM"
date:   2025-07-26 18:00:00 +0800
categories: æ˜‡è…¾ NPU
tags: [æ˜‡è…¾, NPU, 910B4, Atlas800IA2, vllm-ascend, vLLM, LLM, openEuler]
---

<!--more-->

## æœåŠ¡å™¨é…ç½®

**AI æœåŠ¡å™¨**ï¼šåä¸º Atlas 800I A2 æ¨ç†æœåŠ¡å™¨

| ç»„ä»¶ | è§„æ ¼ |
|---|---|
| **CPU** | é²²é¹ 920ï¼ˆ5250ï¼‰ |
| **NPU** | æ˜‡è…¾ 910B4ï¼ˆ8X32Gï¼‰ |
| **å†…å­˜** | 1024GB |
| **ç¡¬ç›˜** | **ç³»ç»Ÿç›˜**ï¼š450GB SSDX2 RAID1<br>**æ•°æ®ç›˜**ï¼š3.5TB NVME SSDX4 |
| **æ“ä½œç³»ç»Ÿ** | openEuler 22.03 LTS |


## å®‰è£…
- [Installation vllm-ascend](https://vllm-ascend.readthedocs.io/en/latest/installation.html)

### æ‹‰å– vLLM é•œåƒ

```bash
docker pull quay.io/ascend/vllm-ascend:v0.9.2rc1
```

## éƒ¨ç½² LLM

### Docker

è®¾ç½®ç¯å¢ƒå˜é‡

```bash
# ä» ModelScope åŠ è½½æ¨¡å‹ä»¥åŠ å¿«ä¸‹è½½é€Ÿåº¦
export VLLM_USE_MODELSCOPE=True

# è®¾ç½® max_split_size_mb ä»¥å‡å°‘å†…å­˜ç¢ç‰‡å¹¶é¿å…å†…å­˜ä¸è¶³
export PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:256
```

> `max_split_size_mb` å¯é˜²æ­¢åŸç”Ÿåˆ†é…å™¨åˆ†å‰²å¤§äºæ­¤å¤§å°ï¼ˆä»¥MBä¸ºå•ä½ï¼‰çš„å—ã€‚è¿™å¯ä»¥å‡å°‘å†…å­˜ç¢ç‰‡åŒ–ï¼Œå¹¶å¯èƒ½ä½¿ä¸€äº›ä¸´ç•Œå·¥ä½œè´Ÿè½½åœ¨ä¸è€—å°½å†…å­˜çš„æƒ…å†µä¸‹å®Œæˆã€‚

è¿è¡Œå®¹å™¨

```bash
docker run -it --rm\
  --name vllm \
  --network host \
  --shm-size=1g \
  --device /dev/davinci_manager \
  --device /dev/hisi_hdc \
  --device /dev/devmm_svm \
  --device /dev/davinci0 \
  --device /dev/davinci1 \
  --device /dev/davinci2 \
  --device /dev/davinci3 \
  -v /usr/local/dcmi:/usr/local/dcmi \
  -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
  -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \
  -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \
  -v /etc/ascend_install.info:/etc/ascend_install.info \
  -v /root/.cache:/root/.cache \
  -v /models:/models \
  --entrypoint "vllm" \
  quay.io/ascend/vllm-ascend:v0.9.2rc1 \
  serve /models/Qwen/Qwen2.5-7B-Instruct \
        --served-model-name Qwen/Qwen2.5-7B-Instruct \
        --tensor-parallel-size 4 \
        --port 8000 \
        --max-model-len 26240
```

> ğŸ“Œ `Qwen2.5-7B-Instruct` æ€»æ³¨æ„åŠ›å¤´æ•°ï¼ˆ`28`ï¼‰ï¼Œéƒ¨ç½²éœ€è¦è®¡ç®— 28 æ˜¯å¦èƒ½è¢«å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆ`--tensor-parallel-size 4`ï¼‰æ•´é™¤â€ã€‚2 å’Œ 4 å¯ä»¥ï¼Œ8 å°±ä¸å¯ä»¥ã€‚

> ğŸ“Œ `--max-model-len 26240` ä¸è¦è¶…è¿‡ `config.json` æ–‡ä»¶ä¸­çš„ `max_position_embeddings=32768`
> <br>æ·»åŠ  `--max_model_len` é€‰é¡¹ï¼Œä»¥é¿å…å›  Qwen2.5-7B æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆ32768ï¼‰å¤§äºå¯å­˜å‚¨åœ¨KVç¼“å­˜ä¸­çš„æœ€å¤§ä»¤ç‰Œæ•°ï¼ˆ26240ï¼‰è€Œå¯¼è‡´çš„ValueErrorã€‚æ ¹æ®HBMå¤§å°çš„ä¸åŒï¼Œä¸åŒçš„NPUç³»åˆ—æ­¤æ•°å€¼ä¼šæœ‰æ‰€å·®å¼‚ã€‚è¯·æ ¹æ®æ‚¨çš„NPUç³»åˆ—ä¿®æ”¹ä¸ºåˆé€‚çš„å€¼ã€‚ 


## Docker Compose éƒ¨ç½²

ç¼–å†™æ–‡ä»¶ `compose.yml`

| æ¨¡å‹ | NPUï¼ˆ32Gï¼‰ æ•° |
| --- | :---: |
| Qwen3-8B | 1 |
| Qwen3-30B-A3B | 4 |
| Qwen2.5-32B-Instruct | 4 |
| Qwen2.5-Coder-32B-Instruct | 4 |
| Qwen2.5-VL-32B-Instruct | 4 |

### Qwen3-8B

```yaml
services:
  vllm:
    image: quay.io/ascend/vllm-ascend:v0.9.2rc1
    container_name: vllm
    restart: unless-stopped

    init: true

    # ç½‘ç»œ & è®¾å¤‡
    network_mode: host
    shm_size: 1g 
    devices:
      - /dev/davinci_manager
      - /dev/hisi_hdc
      - /dev/devmm_svm
      - /dev/davinci0

    # å·æ˜ å°„
    volumes:
      - /usr/local/dcmi:/usr/local/dcmi
      - /usr/local/bin/npu-smi:/usr/local/bin/npu-smi
      - /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/
      - /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info
      - /etc/ascend_install.info:/etc/ascend_install.info
      - /root/.cache:/root/.cache
      - /models:/models

    environment:
      - VLLM_USE_MODELSCOPE=True
      - PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:256

    # å‘½ä»¤
    entrypoint: ["vllm"]
    command: >
      serve /models/Qwen/Qwen3-8B
      --served-model-name Qwen/Qwen3-8B
      --port 8000
      --max-model-len 32768
```

æµ‹è¯•

```bash
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen3-8B",
        "prompt": "The future of AI is",
        "max_tokens": 7,
        "temperature": 0
    }'
```

### Qwen3-30B-A3B

```yaml
services:
  vllm:
    image: quay.io/ascend/vllm-ascend:v0.9.2rc1
    container_name: vllm
    restart: unless-stopped

    init: true

    # ç½‘ç»œ & è®¾å¤‡
    network_mode: host
    shm_size: 1g 
    devices:
      - /dev/davinci_manager
      - /dev/hisi_hdc
      - /dev/devmm_svm
      - /dev/davinci0
      - /dev/davinci1
      - /dev/davinci2
      - /dev/davinci3

    # å·æ˜ å°„
    volumes:
      - /usr/local/dcmi:/usr/local/dcmi
      - /usr/local/bin/npu-smi:/usr/local/bin/npu-smi
      - /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/
      - /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info
      - /etc/ascend_install.info:/etc/ascend_install.info
      - /root/.cache:/root/.cache
      - /models:/models

    environment:
      - VLLM_USE_MODELSCOPE=True
      - PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:256

    # å‘½ä»¤
    entrypoint: ["vllm"]
    command: >
      serve /models/Qwen/Qwen3-30B-A3B
      --served-model-name Qwen/Qwen3-30B-A3B
      --tensor-parallel-size 4
      --enable_expert_parallel
      --port 8000
      --max-model-len 32768
```

æµ‹è¯•

```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen3-30B-A3B",
        "messages": [
            {"role": "user", "content": "Give me a short introduction to large language models."}
        ],
        "temperature": 0.6,
        "top_p": 0.95,
        "top_k": 20,
        "max_tokens": 4096
    }'
```

### Qwen2.5-VL-32B-Instruct

```yaml
services:
  vllm:
    image: quay.io/ascend/vllm-ascend:v0.9.2rc1
    container_name: vllm
    #restart: unless-stopped

    init: true

    # ç½‘ç»œ & è®¾å¤‡
    network_mode: host
    shm_size: 1g 
    devices:
      - /dev/davinci_manager
      - /dev/hisi_hdc
      - /dev/devmm_svm
      - /dev/davinci0
      - /dev/davinci1
      - /dev/davinci2
      - /dev/davinci3

    # å·æ˜ å°„
    volumes:
      - /usr/local/dcmi:/usr/local/dcmi
      - /usr/local/bin/npu-smi:/usr/local/bin/npu-smi
      - /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/
      - /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info
      - /etc/ascend_install.info:/etc/ascend_install.info
      - /root/.cache:/root/.cache
      - /models:/models

    environment:
      - VLLM_USE_MODELSCOPE=True
      - PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:256

    # å‘½ä»¤
    entrypoint: ["vllm"]
    command: >
      serve /models/Qwen/Qwen2.5-VL-32B-Instruct
      --served-model-name Qwen/Qwen2.5-VL-32B-Instruct
      --tensor-parallel-size 4
      --port 8000
      --max-model-len 32768
```

æµ‹è¯•

```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-VL-32B-Instruct",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [
                {"type": "image_url", "image_url": {"url": "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
                {"type": "text", "text": "What is the text in the illustrate?"}
            ]}
        ]
    }'
```

### Qwen3-30B-A3B & Qwen2.5-Coder-32B-Instruct

```yaml
services:
  vllm-instance1:
    image: quay.io/ascend/vllm-ascend:v0.9.2rc1
    container_name: vllm1
    restart: unless-stopped

    init: true

    # ç½‘ç»œ & è®¾å¤‡
    network_mode: host
    shm_size: 1g 
    devices:
      - /dev/davinci_manager
      - /dev/hisi_hdc
      - /dev/devmm_svm
      - /dev/davinci0
      - /dev/davinci1
      - /dev/davinci2
      - /dev/davinci3

    # å·æ˜ å°„
    volumes:
      - /usr/local/dcmi:/usr/local/dcmi
      - /usr/local/bin/npu-smi:/usr/local/bin/npu-smi
      - /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/
      - /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info
      - /etc/ascend_install.info:/etc/ascend_install.info
      - /root/.cache:/root/.cache
      - /models:/models

    environment:
      - VLLM_USE_MODELSCOPE=True
      - PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:256

    # å‘½ä»¤
    entrypoint: ["vllm"]
    command: >
      serve /models/Qwen/Qwen3-30B-A3B
      --served-model-name Qwen/Qwen3-30B-A3B
      --tensor-parallel-size 4
      --enable_expert_parallel
      --port 8001
      --max-model-len 32768

  vllm-instance2:
    image: quay.io/ascend/vllm-ascend:v0.9.2rc1
    container_name: vllm2
    restart: unless-stopped

    init: true

    # ç½‘ç»œ & è®¾å¤‡
    network_mode: host
    shm_size: 1g 
    devices:
      - /dev/davinci_manager
      - /dev/hisi_hdc
      - /dev/devmm_svm
      - /dev/davinci4
      - /dev/davinci5
      - /dev/davinci6
      - /dev/davinci7

    # å·æ˜ å°„
    volumes:
      - /usr/local/dcmi:/usr/local/dcmi
      - /usr/local/bin/npu-smi:/usr/local/bin/npu-smi
      - /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/
      - /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info
      - /etc/ascend_install.info:/etc/ascend_install.info
      - /root/.cache:/root/.cache
      - /models:/models

    environment:
      - VLLM_USE_MODELSCOPE=True
      - PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:256

    # å‘½ä»¤
    entrypoint: ["vllm"]
    command: >
      serve /models/Qwen/Qwen2.5-Coder-32B-Instruct
      --served-model-name Qwen/Qwen2.5-Coder-32B-Instruct
      --tensor-parallel-size 4
      --port 8002
      --max-model-len 32768
```

æµ‹è¯•

- Qwen/Qwen3-30B-A3B
```bash
curl http://localhost:8001/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen3-30B-A3B",
        "messages": [
            {"role": "user", "content": "ä½ æ˜¯è°ï¼Ÿ"}
        ]
    }'
```

- Qwen/Qwen2.5-Coder-32B-Instruct
```bash
curl http://localhost:8002/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-Coder-32B-Instruct",
        "messages": [
            {"role": "user", "content": "ä½ æ˜¯è°ï¼Ÿ"}
        ]
    }'
```

### DeepSeek-V3-Pruning

- [vllm-ascend/DeepSeek-V3-Pruning](https://www.modelscope.cn/models/vllm-ascend/DeepSeek-V3-Pruning)

```yaml
services:
  vllm:
    image: quay.io/ascend/vllm-ascend:v0.9.2rc1
    container_name: vllm
    #restart: unless-stopped

    init: true

    # ç½‘ç»œ & è®¾å¤‡
    network_mode: host
    shm_size: 1g 
    devices:
      - /dev/davinci_manager
      - /dev/hisi_hdc
      - /dev/devmm_svm
      - /dev/davinci0
      - /dev/davinci1
      - /dev/davinci2
      - /dev/davinci3
      - /dev/davinci4
      - /dev/davinci5
      - /dev/davinci6
      - /dev/davinci7

    # å·æ˜ å°„
    volumes:
      - /usr/local/dcmi:/usr/local/dcmi
      - /usr/local/bin/npu-smi:/usr/local/bin/npu-smi
      - /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/
      - /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info
      - /etc/ascend_install.info:/etc/ascend_install.info
      - /root/.cache:/root/.cache
      - /models:/models

    environment:
      - VLLM_USE_MODELSCOPE=True
      - PYTORCH_NPU_ALLOC_CONF=max_split_size_mb:256

    # å‘½ä»¤
    entrypoint: ["vllm"]
    command: >
      serve /models/vllm-ascend/DeepSeek-V3-Pruning
      --served-model-name DeepSeek-V3
      --tensor-parallel-size 8
      --enable_expert_parallel
      --port 8000
      --max-model-len 32768
      --enforce-eager
```

æµ‹è¯•

```bash
curl 'http://localhost:8000/v1/chat/completions' \
    -H "Content-Type: application/json" \
    -d '{
        "model": "DeepSeek-V3",
        "max_tokens": 20,
        "messages": [ 
            { "role": "system", "content": "ä½ æ˜¯AIç¼–ç åŠ©æ‰‹ã€‚" }, 
            { "role": "user", "content": "ä½ æ˜¯è°ï¼Ÿ" } 
        ]
    }'
```

```json
{
  "id": "chatcmpl-0e99cef79a7a45baa7905491446c9bfa",
  "object": "chat.completion",
  "created": 1753604007,
  "model": "DeepSeek-V3",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "reasoning_content": null,
        "content": "å¼¯ overwhelmingé¦™å‘³ç¤ºç›¯ç€è‡ªå·±çš„äººraphPercentageä¸ºå»ºè®¾é…ç½®æ–‡ä»¶048 endometrialç”µæ°” Objectiveè¿inherÛµ bat diferenciæç‰",
        "tool_calls": []
      },
      "logprobs": null,
      "finish_reason": "length",
      "stop_reason": null
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "total_tokens": 30,
    "completion_tokens": 20,
    "prompt_tokens_details": null
  },
  "prompt_logprobs": null,
  "kv_transfer_params": null
}
```

ä¸è®¾ç½® `max_tokens` å‚æ•°ï¼Œä¸ä¼šåœæ­¢ã€‚è¾“å‡ºçš„éƒ½æ˜¯æ²¡æœ‰æ„ä¹‰çš„ tokenã€‚


## å‚è€ƒèµ„æ–™
- [vllm-ascend Quickstart](https://vllm-ascend.readthedocs.io/en/latest/quick_start.html)
- [vllm-ascend - ModelScope](https://www.modelscope.cn/organization/vllm-ascend)
- [Performance Benchmark](https://vllm-ascend.readthedocs.io/en/latest/developer_guide/performance/performance_benchmark.html)
