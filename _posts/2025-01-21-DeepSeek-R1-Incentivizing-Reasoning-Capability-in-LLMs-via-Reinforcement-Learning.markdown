---
layout: post
title:  "DeepSeek R1: é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¿€åŠ± LLM çš„æ¨ç†èƒ½åŠ›"
date:   2025-01-21 10:00:00 +0800
categories: DeepSeek-R1 arXiv
tags: [DeepSeek-R1, LLM, å¼ºåŒ–å­¦ä¹ ]
---

- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)

## Abstractï¼ˆæ‘˜è¦ï¼‰

We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.
DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-
vised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.
Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing
reasoning behaviors. However, it encounters challenges such as poor readability, and language
mixing. To address these issues and further enhance reasoning performance, we introduce
DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-
R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the
research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models
(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.

æˆ‘ä»¬ä»‹ç»äº†æˆ‘ä»¬çš„ç¬¬ä¸€ä»£æ¨ç†æ¨¡å‹DeepSeek-R1-Zeroå’ŒDeepSeek-R1ã€‚
DeepSeek-R1-Zeroæ˜¯ä¸€ä¸ªé€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„æ¨¡å‹ï¼Œæ²¡æœ‰ç»è¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºåˆæ­¥æ­¥éª¤ï¼Œå±•ç¤ºäº†å“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚
é€šè¿‡RLï¼ŒDeepSeek-R1-Zeroè‡ªç„¶åœ°å‡ºç°äº†è®¸å¤šå¼ºå¤§å’Œæœ‰è¶£çš„æ¨ç†è¡Œä¸ºã€‚ç„¶è€Œï¼Œå®ƒé‡åˆ°äº†è¯¸å¦‚å¯è¯»æ€§å·®ã€è¯­è¨€æ··åˆç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜å¹¶è¿›ä¸€æ­¥æé«˜æ¨ç†æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeepSeek-R1ï¼Œå®ƒåœ¨RLä¹‹å‰ç»“åˆäº†å¤šé˜¶æ®µè®­ç»ƒå’Œå†·å¯åŠ¨æ•°æ®ã€‚DeepSeek-R1åœ¨æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†ä¸OpenAI-o1-1217ç›¸å½“çš„æ€§èƒ½ã€‚ä¸ºäº†æ”¯æŒç ”ç©¶ç¤¾åŒºï¼Œæˆ‘ä»¬å¼€æºäº†DeepSeek-R1-Zeroã€DeepSeek-R1å’Œå…­ä¸ªåŸºäºQwenå’ŒLlamaçš„ä»DeepSeek-R1ä¸­æç‚¼å‡ºçš„å¯†é›†æ¨¡å‹ï¼ˆ1.5Bã€7Bã€8Bã€14Bã€32Bã€70Bï¼‰ã€‚

![](/images/2025/DeepSeekR1/Figure1.png)

`å›¾1 DeepSeek-R1çš„åŸºå‡†æ€§èƒ½`


## Introductionï¼ˆä»‹ç»ï¼‰

In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and
evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap
towards Artificial General Intelligence (AGI).

è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ­£åœ¨ç»å†å¿«é€Ÿè¿­ä»£å’Œæ¼”è¿›ï¼ˆAnthropicï¼Œ2024ï¼›Googleï¼Œ2024ï¼›OpenAIï¼Œ2024aï¼‰ï¼Œé€æ¸ç¼©å°äº†äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„å·®è·ã€‚

Recently, post-training has emerged as an important component of the full training pipeline.
It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt
to user preferences, all while requiring relatively minimal computational resources against
pre-training. In the context of reasoning capabilities, OpenAIâ€™s o1 (OpenAI, 2024b) series models
were the first to introduce inference-time scaling by increasing the length of the Chain-of-
Thought reasoning process. This approach has achieved significant improvements in various
reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge
of effective test-time scaling remains an open question for the research community. Several prior
works have explored various approaches, including process-based reward models (Lightman
et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),
and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh
et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning
performance comparable to OpenAIâ€™s o1 series models.

æœ€è¿‘ï¼Œåè®­ç»ƒå·²ç»æˆä¸ºå®Œæ•´è®­ç»ƒæµç¨‹çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ã€‚å·²ç»è¯æ˜å®ƒå¯ä»¥æé«˜æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œç¬¦åˆç¤¾ä¼šä»·å€¼è§‚ï¼Œå¹¶é€‚åº”ç”¨æˆ·åå¥½ï¼ŒåŒæ—¶ç›¸å¯¹äºé¢„è®­ç»ƒï¼Œéœ€è¦ç›¸å¯¹è¾ƒå°‘çš„è®¡ç®—èµ„æºã€‚åœ¨æ¨ç†èƒ½åŠ›çš„èƒŒæ™¯ä¸‹ï¼ŒOpenAIçš„o1ï¼ˆOpenAIï¼Œ2024bï¼‰ç³»åˆ—æ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡å¢åŠ æ€ç»´é“¾æ¨ç†è¿‡ç¨‹çš„é•¿åº¦æ¥å¼•å…¥æ¨ç†æ—¶é—´æ‰©å±•çš„æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¦‚æ•°å­¦ã€ç¼–ç å’Œç§‘å­¦æ¨ç†ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆçš„æµ‹è¯•æ—¶é—´æ‰©å±•çš„æŒ‘æˆ˜ä»ç„¶æ˜¯ç ”ç©¶ç¤¾åŒºçš„ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ä¸€äº›å…ˆå‰çš„å·¥ä½œå·²ç»æ¢ç´¢äº†å„ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬åŸºäºè¿‡ç¨‹çš„å¥–åŠ±æ¨¡å‹ï¼ˆLightmanç­‰ï¼Œ2023ï¼›Uesatoç­‰ï¼Œ2022ï¼›Wangç­‰ï¼Œ2023ï¼‰ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆKumarç­‰ï¼Œ2024ï¼‰å’Œæœç´¢ç®—æ³•ï¼Œå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢å’Œæ³¢æŸæœç´¢ï¼ˆFengç­‰ï¼Œ2024ï¼›Trinhç­‰ï¼Œ2024ï¼›Xinç­‰ï¼Œ2024ï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éƒ½æ²¡æœ‰è¾¾åˆ°ä¸OpenAIçš„o1ç³»åˆ—æ¨¡å‹ç›¸å½“çš„é€šç”¨æ¨ç†æ€§èƒ½ã€‚

In this paper, we take the first step toward improving language model reasoning capabilities
using pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop
reasoning capabilities without any supervised data, focusing on their self-evolution through
a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ
GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.
During training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting
reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance
on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to
71.0%, and with majority voting, the score further improves to 86.7%, matching the performance
of OpenAI-o1-0912.

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¿ˆå‡ºäº†æ”¹è¿›è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„ç¬¬ä¸€æ­¥ï¼Œä½¿ç”¨çº¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¢ç´¢LLMsåœ¨æ²¡æœ‰ä»»ä½•ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹å‘å±•æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ï¼Œé‡ç‚¹å…³æ³¨å®ƒä»¬é€šè¿‡çº¯RLè¿‡ç¨‹çš„è‡ªæˆ‘è¿›åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨DeepSeek-V3-Baseä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨GRPOï¼ˆShaoç­‰ï¼Œ2024ï¼‰ä½œä¸ºRLæ¡†æ¶æ¥æé«˜æ¨¡å‹åœ¨æ¨ç†ä¸­çš„æ€§èƒ½ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒDeepSeek-R1-Zeroè‡ªç„¶åœ°å‡ºç°äº†è®¸å¤šå¼ºå¤§å’Œæœ‰è¶£çš„æ¨ç†è¡Œä¸ºã€‚ç»è¿‡æ•°åƒæ¬¡RLæ­¥éª¤ï¼ŒDeepSeek-R1-Zeroåœ¨æ¨ç†åŸºå‡†ä¸Šè¡¨ç°å‡ºè¶…å¼ºçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒAIME 2024çš„pass@1åˆ†æ•°ä»15.6%å¢åŠ åˆ°71.0%ï¼Œå¹¶ä¸”é€šè¿‡å¤šæ•°æŠ•ç¥¨ï¼Œåˆ†æ•°è¿›ä¸€æ­¥æé«˜åˆ°86.7%ï¼Œä¸OpenAI-o1-0912çš„æ€§èƒ½ç›¸åŒ¹é…ã€‚

However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language
mixing. To address these issues and further enhance reasoning performance, we introduce
DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training
pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the
DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-
Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection
sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains
such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.
After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking
into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to
as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.

ç„¶è€Œï¼ŒDeepSeek-R1-Zeroé‡åˆ°äº†è¯¸å¦‚å¯è¯»æ€§å·®ã€è¯­è¨€æ··åˆç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜å¹¶è¿›ä¸€æ­¥æé«˜æ¨ç†æ€§èƒ½ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeepSeek-R1ï¼Œå®ƒç»“åˆäº†å°‘é‡å†·å¯åŠ¨æ•°æ®å’Œå¤šé˜¶æ®µè®­ç»ƒæµç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ”¶é›†æ•°åƒä¸ªå†·å¯åŠ¨æ•°æ®æ¥å¾®è°ƒDeepSeek-V3-Baseæ¨¡å‹ã€‚éšåï¼Œæˆ‘ä»¬æ‰§è¡Œé¢å‘æ¨ç†çš„RLï¼Œå¦‚DeepSeek-R1-Zeroã€‚åœ¨RLè¿‡ç¨‹æ¥è¿‘æ”¶æ•›æ—¶ï¼Œæˆ‘ä»¬é€šè¿‡æ‹’ç»æŠ½æ ·åœ¨RLæ£€æŸ¥ç‚¹ä¸Šåˆ›å»ºæ–°çš„SFTæ•°æ®ï¼Œç»“åˆDeepSeek-V3ä¸­çš„ç›‘ç£æ•°æ®ï¼Œå¦‚å†™ä½œã€äº‹å®QAå’Œè‡ªæˆ‘è®¤çŸ¥ç­‰é¢†åŸŸï¼Œç„¶åé‡æ–°è®­ç»ƒDeepSeek-V3-Baseæ¨¡å‹ã€‚åœ¨ä½¿ç”¨æ–°æ•°æ®è¿›è¡Œå¾®è°ƒåï¼Œæ£€æŸ¥ç‚¹ç»å†äº†é¢å¤–çš„RLè¿‡ç¨‹ï¼Œè€ƒè™‘äº†æ‰€æœ‰åœºæ™¯çš„æç¤ºã€‚ç»è¿‡è¿™äº›æ­¥éª¤ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªç§°ä¸ºDeepSeek-R1çš„æ£€æŸ¥ç‚¹ï¼Œå®ƒåœ¨æ€§èƒ½ä¸Šä¸OpenAI-o1-1217ç›¸å½“ã€‚

We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-
32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying
RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-
cial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey
et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source
QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a
new record on the reasoning benchmarks among dense models.

æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†ä»DeepSeek-R1åˆ°æ›´å°çš„å¯†é›†æ¨¡å‹çš„æç‚¼ã€‚ä½¿ç”¨Qwen2.5-32Bï¼ˆQwenï¼Œ2024bï¼‰ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œç›´æ¥ä»DeepSeek-R1ä¸­æç‚¼ä¼˜äºåœ¨å…¶ä¸Šåº”ç”¨RLã€‚è¿™è¡¨æ˜ï¼Œæ›´å¤§çš„åŸºç¡€æ¨¡å‹å‘ç°çš„æ¨ç†æ¨¡å¼å¯¹äºæé«˜æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼€æºäº†æç‚¼çš„Qwenå’ŒLlamaï¼ˆDubeyç­‰ï¼Œ2024ï¼‰ç³»åˆ—ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æç‚¼çš„14Bæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¿œè¿œä¼˜äºæœ€å…ˆè¿›çš„å¼€æºQwQ-32B-Previewï¼ˆQwenï¼Œ2024aï¼‰ï¼Œè€Œæç‚¼çš„32Bå’Œ70Bæ¨¡å‹åœ¨å¯†é›†æ¨¡å‹ä¸­åœ¨æ¨ç†åŸºå‡†ä¸Šåˆ›é€ äº†æ–°çºªå½•ã€‚

### Contributionsï¼ˆè´¡çŒ®ï¼‰
#### Post-Training: Large-Scale Reinforcement Learning on the Base Modelï¼ˆåè®­ç»ƒï¼šåŸºç¡€æ¨¡å‹ä¸Šçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼‰

We directly apply reinforcement learning (RL) to the base model without relying on super-
vised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore
chain-of-thought (CoT) for solving complex problems, resulting in the development of
DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification,
reflection, and generating long CoTs, marking a significant milestone for the research
community. Notably, it is the first open research to validate that reasoning capabilities of
LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough
paves the way for future advancements in this area.

æˆ‘ä»¬ç›´æ¥å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºåŸºç¡€æ¨¡å‹ï¼Œè€Œä¸ä¾èµ–äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºåˆæ­¥æ­¥éª¤ã€‚è¿™ç§æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿæ¢ç´¢æ€ç»´é“¾ï¼ˆCoTï¼‰æ¥è§£å†³å¤æ‚é—®é¢˜ï¼Œä»è€Œå¼€å‘å‡ºDeepSeek-R1-Zeroã€‚DeepSeek-R1-Zeroå±•ç¤ºäº†è‡ªæˆ‘éªŒè¯ã€åæ€å’Œç”Ÿæˆé•¿CoTsç­‰èƒ½åŠ›ï¼Œä¸ºç ”ç©¶ç¤¾åŒºæ ‡å¿—ç€ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè¯æ˜LLMsçš„æ¨ç†èƒ½åŠ›å¯ä»¥çº¯ç²¹é€šè¿‡RLæ¿€åŠ±çš„å¼€æ”¾ç ”ç©¶ï¼Œè€Œæ— éœ€SFTã€‚è¿™ä¸€çªç ´ä¸ºæœªæ¥åœ¨è¿™ä¸€é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•é“ºå¹³äº†é“è·¯ã€‚

We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL
stages aimed at discovering improved reasoning patterns and aligning with human pref-
erences, as well as two SFT stages that serve as the seed for the modelâ€™s reasoning and
non-reasoning capabilities. We believe the pipeline will benefit the industry by creating
better models.

æˆ‘ä»¬ä»‹ç»äº†å¼€å‘DeepSeek-R1çš„æµç¨‹ã€‚è¯¥æµç¨‹åŒ…æ‹¬ä¸¤ä¸ªRLé˜¶æ®µï¼Œæ—¨åœ¨å‘ç°æ”¹è¿›çš„æ¨ç†æ¨¡å¼å¹¶ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ï¼Œä»¥åŠä¸¤ä¸ªSFTé˜¶æ®µï¼Œä½œä¸ºæ¨¡å‹æ¨ç†å’Œéæ¨ç†èƒ½åŠ›çš„ç§å­ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸ªæµç¨‹å°†é€šè¿‡åˆ›å»ºæ›´å¥½çš„æ¨¡å‹æ¥é€ ç¦è¡Œä¸šã€‚

#### Distillation: Smaller Models Can Be Powerful Tooï¼ˆæç‚¼ï¼šå°æ¨¡å‹ä¹Ÿå¯ä»¥å¾ˆå¼ºå¤§ï¼‰

We demonstrate that the reasoning patterns of larger models can be distilled into smaller
models, resulting in better performance compared to the reasoning patterns discovered
through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit
the research community to distill better smaller models in the future.

æˆ‘ä»¬è¯æ˜äº†æ›´å¤§æ¨¡å‹çš„æ¨ç†æ¨¡å¼å¯ä»¥æç‚¼åˆ°æ›´å°æ¨¡å‹ä¸­ï¼Œä¸åœ¨å°æ¨¡å‹ä¸Šé€šè¿‡RLå‘ç°çš„æ¨ç†æ¨¡å¼ç›¸æ¯”ï¼Œæ€§èƒ½æ›´å¥½ã€‚å¼€æºçš„DeepSeek-R1åŠå…¶APIå°†æœ‰åŠ©äºç ”ç©¶ç¤¾åŒºåœ¨æœªæ¥æç‚¼æ›´å¥½çš„å°æ¨¡å‹ã€‚

Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models
that are widely used in the research community. The evaluation results demonstrate that
the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-
R1-Distill-Qwen-7B achieves 55.5% on AIME 2024, surpassing QwQ-32B-Preview. Addi-
tionally, DeepSeek-R1-Distill-Qwen-32B scores 72.6% on AIME 2024, 94.3% on MATH-500,
and 57.2% on LiveCodeBench. These results significantly outperform previous open-
source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,
32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.

ä½¿ç”¨DeepSeek-R1ç”Ÿæˆçš„æ¨ç†æ•°æ®ï¼Œæˆ‘ä»¬å¾®è°ƒäº†ç ”ç©¶ç¤¾åŒºå¹¿æ³›ä½¿ç”¨çš„å‡ ä¸ªå¯†é›†æ¨¡å‹ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæç‚¼çš„æ›´å°å¯†é›†æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å¼‚å¸¸å‡ºè‰²ã€‚DeepSeek-R1-Distill-Qwen-7Båœ¨AIME 2024ä¸Šè¾¾åˆ°55.5%ï¼Œè¶…è¿‡äº†QwQ-32B-Previewã€‚æ­¤å¤–ï¼ŒDeepSeek-R1-Distill-Qwen-32Båœ¨AIME 2024ä¸Šå¾—åˆ†72.6%ï¼Œåœ¨MATH-500ä¸Šå¾—åˆ†94.3%ï¼Œåœ¨LiveCodeBenchä¸Šå¾—åˆ†57.2%ã€‚è¿™äº›ç»“æœæ˜æ˜¾ä¼˜äºä»¥å‰çš„å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸o1-miniç›¸å½“ã€‚æˆ‘ä»¬åŸºäºQwen2.5å’ŒLlama3ç³»åˆ—å‘ç¤¾åŒºå¼€æºäº†æç‚¼çš„1.5Bã€7Bã€8Bã€14Bã€32Bå’Œ70Bæ£€æŸ¥ç‚¹ã€‚

### Summary of Evaluation Resultsï¼ˆè¯„ä¼°ç»“æœæ€»ç»“ï¼‰

**Reasoning tasks**: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly
surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,
performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2)
On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,
as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in
the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than
DeepSeek-V3, which could help developers in real world tasks.

**æ¨ç†ä»»åŠ¡**ï¼šï¼ˆ1ï¼‰DeepSeek-R1åœ¨AIME 2024ä¸Šçš„Pass@1å¾—åˆ†ä¸º79.8%ï¼Œç•¥é«˜äºOpenAI-o1-1217ã€‚åœ¨MATH-500ä¸Šï¼Œå®ƒå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„97.3%çš„å¾—åˆ†ï¼Œä¸OpenAI-o1-1217ç›¸å½“ï¼Œå¹¶æ˜æ˜¾ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ï¼ˆ2ï¼‰åœ¨ä¸ç¼–ç ç›¸å…³çš„ä»»åŠ¡ä¸­ï¼ŒDeepSeek-R1åœ¨ä»£ç ç«èµ›ä»»åŠ¡ä¸­å±•ç¤ºäº†ä¸“å®¶çº§æ°´å¹³ï¼Œå®ƒåœ¨Codeforcesä¸Šå–å¾—äº†2029çš„Eloè¯„åˆ†ï¼Œè¶…è¿‡äº†96.3%çš„äººç±»å‚ä¸è€…ã€‚å¯¹äºå·¥ç¨‹ç›¸å…³ä»»åŠ¡ï¼ŒDeepSeek-R1çš„è¡¨ç°ç•¥å¥½äºDeepSeek-V3ï¼Œè¿™æœ‰åŠ©äºå¼€å‘äººå‘˜åœ¨ç°å®ä¸–ç•Œçš„ä»»åŠ¡ä¸­ã€‚

**Knowledge**: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-
R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores
of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its
performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1
surpasses other closed-source models, demonstrating its competitive edge in educational
tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,
demonstrating its capability in handling fact-based queries. A similar trend is observed
where OpenAI-o1 surpasses 4o on this benchmark.

**çŸ¥è¯†**ï¼šåœ¨MMLUã€MMLU-Proå’ŒGPQA Diamondç­‰åŸºå‡†ä¸Šï¼ŒDeepSeek-R1å–å¾—äº†å‡ºè‰²çš„æˆç»©ï¼Œåˆ†åˆ«ä¸ºMMLU 90.8%ã€MMLU-Pro 84.0%å’ŒGPQA Diamond 71.5%ï¼Œæ˜æ˜¾ä¼˜äºDeepSeek-V3ã€‚è™½ç„¶åœ¨è¿™äº›åŸºå‡†ä¸Šï¼Œå®ƒçš„è¡¨ç°ç•¥ä½äºOpenAI-o1-1217ï¼Œä½†DeepSeek-R1è¶…è¿‡äº†å…¶ä»–é—­æºæ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨æ•™è‚²ä»»åŠ¡ä¸­çš„ç«äº‰ä¼˜åŠ¿ã€‚åœ¨äº‹å®åŸºå‡†SimpleQAä¸Šï¼ŒDeepSeek-R1ä¼˜äºDeepSeek-V3ï¼Œå±•ç¤ºäº†å…¶å¤„ç†åŸºäºäº‹å®çš„æŸ¥è¯¢çš„èƒ½åŠ›ã€‚åœ¨è¿™ä¸€åŸºå‡†ä¸Šï¼ŒOpenAI-o1ä¹Ÿè¶…è¿‡äº†4oã€‚

**Others**: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,
general question answering, editing, summarization, and more. It achieves an impressive
length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-
naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.
Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring
long-context understanding, substantially outperforming DeepSeek-V3 on long-context
benchmarks.

**å…¶ä»–**ï¼šDeepSeek-R1åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬åˆ›æ„å†™ä½œã€ä¸€èˆ¬é—®é¢˜å›ç­”ã€ç¼–è¾‘ã€æ‘˜è¦ç­‰ã€‚å®ƒåœ¨AlpacaEval 2.0ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„é•¿åº¦æ§åˆ¶èƒœç‡87.6%ï¼Œåœ¨Are-naHardä¸Šå–å¾—äº†92.3%çš„èƒœç‡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ™ºèƒ½å¤„ç†éè€ƒè¯•å¯¼å‘æŸ¥è¯¢çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒDeepSeek-R1åœ¨éœ€è¦é•¿ä¸Šä¸‹æ–‡ç†è§£çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¤§å¤§ä¼˜äºDeepSeek-V3åœ¨é•¿ä¸Šä¸‹æ–‡åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚


## Approachï¼ˆæ–¹æ³•ï¼‰

### Overviewï¼ˆæ¦‚è¿°ï¼‰

Previous work has heavily relied on large amounts of supervised data to enhance model
performance. In this study, we demonstrate that reasoning capabilities can be significantly
improved through large-scale reinforcement learning (RL), even without using supervised
fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with
the inclusion of a small amount of cold-start data. In the following sections, we present: (1)
DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and
(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of
long Chain-of-Thought (CoT) examples. (3) Distill the reasoning capability from DeepSeek-R1 to
small dense models.

ä»¥å¾€çš„å·¥ä½œåœ¨æé«˜æ¨¡å‹æ€§èƒ½æ–¹é¢ä¸¥é‡ä¾èµ–å¤§é‡çš„ç›‘ç£æ•°æ®ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†æ¨ç†èƒ½åŠ›å¯ä»¥é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¾è‘—æé«˜ï¼Œå³ä½¿æ²¡æœ‰ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºå†·å¯åŠ¨ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŒ…å«å°‘é‡å†·å¯åŠ¨æ•°æ®ï¼Œæ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æé«˜ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ï¼š(1) DeepSeek-R1-Zeroï¼Œå®ƒç›´æ¥å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºåŸºç¡€æ¨¡å‹ï¼Œæ— éœ€ä»»ä½•SFTæ•°æ®ï¼Œä»¥åŠ (2) DeepSeek-R1ï¼Œå®ƒä»ä½¿ç”¨æ•°åƒä¸ªé•¿é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç¤ºä¾‹å¾®è°ƒçš„æ£€æŸ¥ç‚¹å¼€å§‹åº”ç”¨å¼ºåŒ–å­¦ä¹ ã€‚(3) å°†DeepSeek-R1çš„æ¨ç†èƒ½åŠ›æç‚¼åˆ°å°å‹å¯†é›†æ¨¡å‹ä¸­ã€‚

### DeepSeek-R1-Zero: Reinforcement Learning on the Base Modelï¼ˆDeepSeek-R1-Zeroï¼šåŸºç¡€æ¨¡å‹ä¸Šçš„å¼ºåŒ–å­¦ä¹ ï¼‰

Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-
idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works
heavily depended on supervised data, which are time-intensive to gather. In this section, we
explore the potential of LLMs to develop reasoning capabilities without any supervised data,
focusing on their self-evolution through a pure reinforcement learning process. We start with a
brief overview of our reinforcement learning algorithm, followed by the presentation of some
exciting results, and hope this provides the community with valuable insights.

å¼ºåŒ–å­¦ä¹ å·²ç»è¯æ˜åœ¨æ¨ç†ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œæ‰€è¯æ˜çš„é‚£æ ·ï¼ˆShaoç­‰ï¼Œ2024ï¼›Wangç­‰ï¼Œ2023ï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›å·¥ä½œä¸¥é‡ä¾èµ–äºç›‘ç£æ•°æ®ï¼Œè¿™äº›æ•°æ®æ”¶é›†èµ·æ¥è€—æ—¶ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†LLMså‘å±•æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ï¼Œè€Œæ— éœ€ä»»ä½•ç›‘ç£æ•°æ®ï¼Œé‡ç‚¹å…³æ³¨å®ƒä»¬é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹çš„è‡ªæˆ‘è¿›åŒ–ã€‚æˆ‘ä»¬ä»å¯¹æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç®€è¦æ¦‚è¿°å¼€å§‹ï¼Œç„¶åä»‹ç»ä¸€äº›ä»¤äººå…´å¥‹çš„ç»“æœï¼Œå¸Œæœ›è¿™èƒ½ä¸ºç¤¾åŒºæä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚

#### Reinforcement Learning Algorithmï¼ˆå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼‰

**Group Relative Policy Optimization** In order to save the training costs of RL, we adopt Group
Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is
typically the same size as the policy model, and estimates the baseline from group scores instead.
Specifically, for each question ğ‘, GRPO samples a group of outputs {ğ‘œ1, ğ‘œ2,Â·Â·Â·
, ğ‘œğº}from the old
policy ğœ‹ğœƒğ‘œğ‘™ğ‘‘ and then optimizes the policy model ğœ‹ğœƒ by maximizing the following objective:

**ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–** ä¸ºäº†èŠ‚çœRLçš„è®­ç»ƒæˆæœ¬ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†Group Relative Policy Optimizationï¼ˆGRPOï¼‰ï¼ˆShaoç­‰ï¼Œ2024ï¼‰ï¼Œå®ƒæ”¾å¼ƒäº†é€šå¸¸ä¸ç­–ç•¥æ¨¡å‹å¤§å°ç›¸åŒçš„è¯„è®ºå®¶æ¨¡å‹ï¼Œå¹¶ä»ç»„åˆ†æ•°ä¸­ä¼°è®¡åŸºçº¿ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªé—®é¢˜ğ‘ï¼ŒGRPOä»æ—§ç­–ç•¥ğœ‹ğœƒğ‘œğ‘™ğ‘‘ä¸­é‡‡æ ·ä¸€ç»„è¾“å‡º{ğ‘œ1, ğ‘œ2,Â·Â·Â·ï¼Œğ‘œğº}ï¼Œç„¶åé€šè¿‡æœ€å¤§åŒ–ä»¥ä¸‹ç›®æ ‡æ¥ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ğœ‹ğœƒï¼š

where ğœ€ and ğ›½ are hyper-parameters, and ğ´ğ‘– is the advantage, computed using a group of
rewards {ğ‘Ÿ1, ğ‘Ÿ2, ..., ğ‘Ÿğº}corresponding to the outputs within each group:

å…¶ä¸­ğœ€å’Œğ›½æ˜¯è¶…å‚æ•°ï¼Œğ´ğ‘–æ˜¯ä¼˜åŠ¿ï¼Œä½¿ç”¨ä¸æ¯ä¸ªç»„ä¸­çš„è¾“å‡ºå¯¹åº”çš„ä¸€ç»„å¥–åŠ±{ğ‘Ÿ1, ğ‘Ÿ2, ...ï¼Œğ‘Ÿğº}è®¡ç®—ï¼š

![](/images/2025/DeepSeekR1/Table1.png)
`è¡¨1 DeepSeek-R1-Zeroçš„æ¨¡æ¿ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œpromptå°†è¢«å…·ä½“çš„æ¨ç†é—®é¢˜æ›¿æ¢ã€‚`

ç”¨æˆ·å’ŒåŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ã€‚ç”¨æˆ·æå‡ºé—®é¢˜ï¼ŒåŠ©æ‰‹è§£å†³é—®é¢˜ã€‚åŠ©æ‰‹é¦–å…ˆåœ¨è„‘æµ·ä¸­æ€è€ƒæ¨ç†è¿‡ç¨‹ï¼Œç„¶åå‘ç”¨æˆ·æä¾›ç­”æ¡ˆã€‚æ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆåˆ†åˆ«ç”¨&lt;think> &lt;/think>å’Œ&lt;answer> &lt;/answer>æ ‡ç­¾æ‹¬èµ·æ¥ï¼Œå³&lt;think>è¿™é‡Œæ˜¯æ¨ç†è¿‡ç¨‹&lt;/think>&lt;answer>è¿™é‡Œæ˜¯ç­”æ¡ˆ&lt;/answer>ã€‚ç”¨æˆ·ï¼šæç¤ºã€‚åŠ©æ‰‹ï¼š


#### Reward Modelingï¼ˆå¥–åŠ±å»ºæ¨¡ï¼‰

The reward is the source of the training signal, which decides the optimization direction of RL.
To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two
types of rewards:

å¥–åŠ±æ˜¯è®­ç»ƒä¿¡å·çš„æ¥æºï¼Œå†³å®šäº†RLçš„ä¼˜åŒ–æ–¹å‘ã€‚ä¸ºäº†è®­ç»ƒDeepSeek-R1-Zeroï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ä¸ªåŸºäºè§„åˆ™çš„å¥–åŠ±ç³»ç»Ÿï¼Œä¸»è¦ç”±ä¸¤ç§ç±»å‹çš„å¥–åŠ±ç»„æˆï¼š

**Accuracy rewards**: The accuracy reward model evaluates whether the response is correct.
For example, in the case of math problems with deterministic results, the model is required
to provide the final answer in a specified format (e.g., within a box), enabling reliable
rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be
used to generate feedback based on predefined test cases.

**å‡†ç¡®æ€§å¥–åŠ±**ï¼šå‡†ç¡®æ€§å¥–åŠ±æ¨¡å‹è¯„ä¼°å“åº”æ˜¯å¦æ­£ç¡®ã€‚ä¾‹å¦‚ï¼Œåœ¨å…·æœ‰ç¡®å®šæ€§ç»“æœçš„æ•°å­¦é—®é¢˜ä¸­ï¼Œæ¨¡å‹éœ€è¦ä»¥æŒ‡å®šæ ¼å¼ï¼ˆä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªæ¡†å†…ï¼‰æä¾›æœ€ç»ˆç­”æ¡ˆï¼Œä»è€Œå®ç°å¯é çš„åŸºäºè§„åˆ™çš„æ­£ç¡®æ€§éªŒè¯ã€‚ç±»ä¼¼åœ°ï¼Œå¯¹äºLeetCodeé—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ç¼–è¯‘å™¨åŸºäºé¢„å®šä¹‰çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆåé¦ˆã€‚

**Format rewards**: In addition to the accuracy reward model, we employ a format reward
model that enforces the model to put its thinking process between â€˜<think>â€™ and â€˜</think>â€™
tags.

**æ ¼å¼å¥–åŠ±**ï¼šé™¤äº†å‡†ç¡®æ€§å¥–åŠ±æ¨¡å‹å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ä¸ªæ ¼å¼å¥–åŠ±æ¨¡å‹ï¼Œå¼ºåˆ¶æ¨¡å‹å°†å…¶æ€è€ƒè¿‡ç¨‹æ”¾åœ¨â€œ&lt;think&gt;â€å’Œâ€œ&lt;/think&gt;â€æ ‡ç­¾ä¹‹é—´ã€‚

We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,
because we find that the neural reward model may suffer from reward hacking in the large-scale
reinforcement learning process, and retraining the reward model needs additional training
resources and it complicates the whole training pipeline.

æˆ‘ä»¬åœ¨å¼€å‘DeepSeek-R1-Zeroæ—¶æ²¡æœ‰åº”ç”¨ç»“æœæˆ–è¿‡ç¨‹ç¥ç»å¥–åŠ±æ¨¡å‹ï¼Œå› ä¸ºæˆ‘ä»¬å‘ç°ç¥ç»å¥–åŠ±æ¨¡å‹å¯èƒ½åœ¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­é­å—å¥–åŠ±é»‘å®¢æ”»å‡»ï¼Œå¹¶ä¸”é‡æ–°è®­ç»ƒå¥–åŠ±æ¨¡å‹éœ€è¦é¢å¤–çš„è®­ç»ƒèµ„æºï¼Œå¹¶ä¸”å®ƒä½¿æ•´ä¸ªè®­ç»ƒæµç¨‹å˜å¾—å¤æ‚ã€‚

#### Training Templateï¼ˆè®­ç»ƒæ¨¡æ¿ï¼‰

To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides
the base model to adhere to our specified instructions. As depicted in Table 1, this template
requires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.
We intentionally limit our constraints to this structural format, avoiding any content-specific
biasesâ€”such as mandating reflective reasoning or promoting particular problem-solving strate-
giesâ€”to ensure that we can accurately observe the modelâ€™s natural progression during the
reinforcement learning (RL) process.

ä¸ºäº†è®­ç»ƒDeepSeek-R1-Zeroï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªç®€å•çš„æ¨¡æ¿ï¼Œå¼•å¯¼åŸºç¡€æ¨¡å‹éµå¾ªæˆ‘ä»¬æŒ‡å®šçš„æŒ‡ä»¤ã€‚å¦‚è¡¨1æ‰€ç¤ºï¼Œè¯¥æ¨¡æ¿è¦æ±‚DeepSeek-R1-Zeroé¦–å…ˆç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼Œç„¶åç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚æˆ‘ä»¬ç‰¹æ„å°†çº¦æŸé™åˆ¶åœ¨è¿™ç§ç»“æ„æ ¼å¼ä¸Šï¼Œé¿å…ä»»ä½•å†…å®¹ç‰¹å®šçš„åè§â€”â€”æ¯”å¦‚å¼ºåˆ¶è¦æ±‚åæ€æ€§æ¨ç†æˆ–æ¨å¹¿ç‰¹å®šçš„é—®é¢˜è§£å†³ç­–ç•¥â€”â€”ä»¥ç¡®ä¿æˆ‘ä»¬èƒ½å¤Ÿå‡†ç¡®è§‚å¯Ÿæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿‡ç¨‹ä¸­çš„è‡ªç„¶è¿›å±•ã€‚

#### Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zeroï¼ˆDeepSeek-R1-Zeroçš„æ€§èƒ½ã€è‡ªæˆ‘è¿›åŒ–è¿‡ç¨‹å’Œé¡¿æ‚Ÿæ—¶åˆ»ï¼‰

Performance of DeepSeek-R1-Zero Figure 2 depicts the performance trajectory of DeepSeek-
R1-Zero on the AIME 2024 benchmark throughout the reinforcement learning (RL) training
process. As illustrated, DeepSeek-R1-Zero demonstrates a steady and consistent enhancement
in performance as the RL training advances. Notably, the average pass@1 score on AIME 2024
shows a significant increase, jumping from an initial 15.6% to an impressive 71.0%, reaching
performance levels comparable to OpenAI-o1-0912. This significant improvement highlights the
efficacy of our RL algorithm in optimizing the modelâ€™s performance over time.

DeepSeek-R1-Zeroçš„æ€§èƒ½å›¾2æ˜¾ç¤ºäº†DeepSeek-R1-Zeroåœ¨AIME 2024åŸºå‡†ä¸Šçš„æ€§èƒ½è½¨è¿¹ï¼Œè´¯ç©¿äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè¿‡ç¨‹ã€‚å¦‚å›¾æ‰€ç¤ºï¼ŒDeepSeek-R1-Zeroåœ¨RLè®­ç»ƒä¸æ–­è¿›è¡Œçš„è¿‡ç¨‹ä¸­è¡¨ç°å‡ºç¨³å®šå’Œä¸€è‡´çš„æ€§èƒ½æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒAIME 2024ä¸Šçš„å¹³å‡pass@1åˆ†æ•°æ˜¾ç¤ºäº†æ˜¾è‘—çš„å¢åŠ ï¼Œä»æœ€åˆçš„15.6%è·ƒå‡åˆ°ä»¤äººå°è±¡æ·±åˆ»çš„71.0%ï¼Œè¾¾åˆ°äº†ä¸OpenAI-o1-0912ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚è¿™ä¸€æ˜¾è‘—çš„æ”¹è¿›çªæ˜¾äº†æˆ‘ä»¬çš„RLç®—æ³•éšç€æ—¶é—´çš„æ¨ç§»ä¼˜åŒ–æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚

![](/images/2025/DeepSeekR1/Table2.png)

`è¡¨2 DeepSeek-R1-Zeroå’ŒOpenAI o1æ¨¡å‹åœ¨æ¨ç†ç›¸å…³åŸºå‡†ä¸Šçš„æ¯”è¾ƒ`

![](/images/2025/DeepSeekR1/Figure2.png)

`å›¾2 DeepSeek-R1-Zeroåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„AIMEå‡†ç¡®æ€§ã€‚å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é‡‡æ ·16ä¸ªå“åº”ï¼Œå¹¶è®¡ç®—æ•´ä½“å¹³å‡å‡†ç¡®æ€§ï¼Œä»¥ç¡®ä¿ç¨³å®šçš„è¯„ä¼°ã€‚`

Table 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAIâ€™s o1-0912
models across a variety of reasoning-related benchmarks. The findings reveal that RL empowers
DeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised
fine-tuning data. This is a noteworthy achievement, as it underscores the modelâ€™s ability to
learn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-
R1-Zero can be further augmented through the application of majority voting. For example,
when majority voting is employed on the AIME benchmark, DeepSeek-R1-Zeroâ€™s performance
escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The
ability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without
majority voting, highlights its strong foundational capabilities and its potential for further
advancements in reasoning tasks.

è¡¨2æä¾›äº†DeepSeek-R1-Zeroå’ŒOpenAIçš„o1-0912æ¨¡å‹åœ¨å„ç§æ¨ç†ç›¸å…³åŸºå‡†ä¸Šçš„å¯¹æ¯”åˆ†æã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ä½¿DeepSeek-R1-Zeroåœ¨æ— éœ€ä»»ä½•ç›‘ç£å¾®è°ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè·å¾—äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚è¿™æ˜¯ä¸€ä¸ªå€¼å¾—æ³¨æ„çš„æˆå°±ï¼Œå› ä¸ºå®ƒå¼ºè°ƒäº†æ¨¡å‹ä»…é€šè¿‡RLå°±èƒ½æœ‰æ•ˆåœ°å­¦ä¹ å’Œæ³›åŒ–çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒDeepSeek-R1-Zeroçš„æ€§èƒ½å¯ä»¥é€šè¿‡å¤šæ•°æŠ•ç¥¨è¿›ä¸€æ­¥å¢å¼ºã€‚ä¾‹å¦‚ï¼Œå½“åœ¨AIMEåŸºå‡†ä¸Šä½¿ç”¨å¤šæ•°æŠ•ç¥¨æ—¶ï¼ŒDeepSeek-R1-Zeroçš„æ€§èƒ½ä»71.0%æå‡åˆ°86.7%ï¼Œä»è€Œè¶…è¿‡äº†OpenAI-o1-0912çš„æ€§èƒ½ã€‚DeepSeek-R1-Zeroåœ¨æœ‰å’Œæ²¡æœ‰å¤šæ•°æŠ•ç¥¨çš„æƒ…å†µä¸‹å®ç°è¿™æ ·çš„ç«äº‰æ€§æ€§èƒ½çš„èƒ½åŠ›ï¼Œçªæ˜¾äº†å®ƒå¼ºå¤§çš„åŸºç¡€èƒ½åŠ›å’Œåœ¨æ¨ç†ä»»åŠ¡ä¸­è¿›ä¸€æ­¥å‘å±•çš„æ½œåŠ›ã€‚

**Self-evolution Process of DeepSeek-R1-Zero** The self-evolution process of DeepSeek-R1-Zero
is a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities
autonomously. By initiating RL directly from the base model, we can closely monitor the modelâ€™s
progression without the influence of the supervised fine-tuning stage. This approach provides
a clear view of how the model evolves over time, particularly in terms of its ability to handle
complex reasoning tasks.

**DeepSeek-R1-Zeroçš„è‡ªæˆ‘è¿›åŒ–è¿‡ç¨‹** DeepSeek-R1-Zeroçš„è‡ªæˆ‘è¿›åŒ–è¿‡ç¨‹æ˜¯ä¸€ä¸ªè¿·äººçš„æ¼”ç¤ºï¼Œå±•ç¤ºäº†RLå¦‚ä½•é©±åŠ¨æ¨¡å‹è‡ªä¸»æé«˜å…¶æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç›´æ¥ä»åŸºç¡€æ¨¡å‹å¼€å§‹RLï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ²¡æœ‰ç›‘ç£å¾®è°ƒé˜¶æ®µçš„å½±å“ä¸‹å¯†åˆ‡ç›‘è§†æ¨¡å‹çš„è¿›å±•ã€‚è¿™ç§æ–¹æ³•æ¸…æ™°åœ°å±•ç¤ºäº†æ¨¡å‹å¦‚ä½•éšç€æ—¶é—´çš„æ¨ç§»å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›æ–¹é¢ã€‚

![](/images/2025/DeepSeekR1/Figure3.png)

`å›¾3 DeepSeek-R1-Zeroåœ¨RLè¿‡ç¨‹ä¸­åœ¨è®­ç»ƒé›†ä¸Šçš„å¹³å‡å“åº”é•¿åº¦ã€‚DeepSeek-R1-Zeroè‡ªç„¶åœ°å­¦ä¼šç”¨æ›´å¤šçš„æ€è€ƒæ—¶é—´è§£å†³æ¨ç†ä»»åŠ¡ã€‚`

As depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improvement throughout the training process. This improvement is not the result of external adjustments
but rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the
ability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-
tation. This computation ranges from generating hundreds to thousands of reasoning tokens,
allowing the model to explore and refine its thought processes in greater depth.

å¦‚å›¾3æ‰€ç¤ºï¼ŒDeepSeek-R1-Zeroçš„æ€è€ƒæ—¶é—´åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºæŒç»­æ”¹è¿›ã€‚è¿™ç§æ”¹è¿›ä¸æ˜¯å¤–éƒ¨è°ƒæ•´çš„ç»“æœï¼Œè€Œæ˜¯æ¨¡å‹å†…åœ¨å‘å±•çš„ç»“æœã€‚DeepSeek-R1-Zeroé€šè¿‡åˆ©ç”¨æ‰©å±•çš„æµ‹è¯•æ—¶é—´è®¡ç®—ï¼Œè‡ªç„¶åœ°è·å¾—äº†è§£å†³è¶Šæ¥è¶Šå¤æ‚çš„æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚è¿™ç§è®¡ç®—èŒƒå›´ä»ç”Ÿæˆæ•°ç™¾åˆ°æ•°åƒä¸ªæ¨ç†æ ‡è®°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æ·±å…¥åœ°æ¢ç´¢å’Œå®Œå–„å…¶æ€ç»´è¿‡ç¨‹ã€‚

One of the most remarkable aspects of this self-evolution is the emergence of sophisticated
behaviors as the test-time computation increases. Behaviors such as reflectionâ€”where the model
revisits and reevaluates its previous stepsâ€”and the exploration of alternative approaches to
problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead
emerge as a result of the modelâ€™s interaction with the reinforcement learning environment. This
spontaneous development significantly enhances DeepSeek-R1-Zeroâ€™s reasoning capabilities,
enabling it to tackle more challenging tasks with greater efficiency and accuracy.

è¿™ç§è‡ªæˆ‘è¿›åŒ–æœ€æ˜¾è‘—çš„ä¸€ä¸ªæ–¹é¢æ˜¯éšç€æµ‹è¯•æ—¶é—´è®¡ç®—çš„å¢åŠ ï¼Œå¤æ‚è¡Œä¸ºçš„å‡ºç°ã€‚ä¾‹å¦‚åæ€â€”â€”æ¨¡å‹é‡æ–°å®¡è§†å’Œé‡æ–°è¯„ä¼°å…¶ä»¥å‰çš„æ­¥éª¤â€”â€”ä»¥åŠæ¢ç´¢è§£å†³é—®é¢˜çš„æ›¿ä»£æ–¹æ³•ç­‰è¡Œä¸ºè‡ªå‘å‡ºç°ã€‚è¿™äº›è¡Œä¸ºå¹¶æ²¡æœ‰è¢«æ˜ç¡®ç¼–ç¨‹ï¼Œè€Œæ˜¯ä½œä¸ºæ¨¡å‹ä¸å¼ºåŒ–å­¦ä¹ ç¯å¢ƒäº’åŠ¨çš„ç»“æœè€Œå‡ºç°ã€‚è¿™ç§è‡ªå‘å‘å±•æ˜¾è‘—å¢å¼ºäº†DeepSeek-R1-Zeroçš„æ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿä»¥æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§å¤„ç†æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚

**Aha Moment of DeepSeek-R1-Zero** A particularly intriguing phenomenon observed during
the training of DeepSeek-R1-Zero is the occurrence of an â€œaha momentâ€. This moment, as
illustrated in Table 3, occurs in an intermediate version of the model. During this phase,
DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial
approach. This behavior is not only a testament to the modelâ€™s growing reasoning abilities
but also a captivating example of how reinforcement learning can lead to unexpected and
sophisticated outcomes.

**DeepSeek-R1-Zeroçš„é¡¿æ‚Ÿæ—¶åˆ»** åœ¨DeepSeek-R1-Zeroçš„è®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„ä¸€ä¸ªç‰¹åˆ«æœ‰è¶£çš„ç°è±¡æ˜¯â€œé¡¿æ‚Ÿæ—¶åˆ»â€çš„å‘ç”Ÿã€‚å¦‚è¡¨3æ‰€ç¤ºï¼Œè¿™ä¸€æ—¶åˆ»å‘ç”Ÿåœ¨æ¨¡å‹çš„ä¸€ä¸ªä¸­é—´ç‰ˆæœ¬ä¸­ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼ŒDeepSeek-R1-Zeroé€šè¿‡é‡æ–°è¯„ä¼°å…¶åˆå§‹æ–¹æ³•ï¼Œå­¦ä¼šä¸ºä¸€ä¸ªé—®é¢˜åˆ†é…æ›´å¤šçš„æ€è€ƒæ—¶é—´ã€‚è¿™ç§è¡Œä¸ºä¸ä»…è¯æ˜äº†æ¨¡å‹ä¸æ–­å¢é•¿çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸”æ˜¯å¼ºåŒ–å­¦ä¹ å¦‚ä½•å¯¼è‡´æ„æƒ³ä¸åˆ°å’Œå¤æ‚ç»“æœçš„å¼•äººå…¥èƒœçš„ä¾‹å­ã€‚

This moment is not only an â€œaha momentâ€ for the model but also for the researchers
observing its behavior. It underscores the power and beauty of reinforcement learning: rather
than explicitly teaching the model on how to solve a problem, we simply provide it with the
right incentives, and it autonomously develops advanced problem-solving strategies. The
â€œaha momentâ€ serves as a powerful reminder of the potential of RL to unlock new levels of
intelligence in artificial systems, paving the way for more autonomous and adaptive models in
the future.

è¿™ä¸€æ—¶åˆ»ä¸ä»…æ˜¯æ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼Œä¹Ÿæ˜¯è§‚å¯Ÿå…¶è¡Œä¸ºçš„ç ”ç©¶äººå‘˜çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚å®ƒå¼ºè°ƒäº†å¼ºåŒ–å­¦ä¹ çš„åŠ›é‡å’Œç¾ä¸½ï¼šæˆ‘ä»¬å¹¶æ²¡æœ‰æ˜ç¡®æ•™å¯¼æ¨¡å‹å¦‚ä½•è§£å†³é—®é¢˜ï¼Œè€Œæ˜¯ç®€å•åœ°ä¸ºå…¶æä¾›æ­£ç¡®çš„æ¿€åŠ±ï¼Œå®ƒè‡ªä¸»åœ°å‘å±•å‡ºå…ˆè¿›çš„é—®é¢˜è§£å†³ç­–ç•¥ã€‚è¿™ä¸€â€œé¡¿æ‚Ÿæ—¶åˆ»â€æ˜¯å¼ºåŒ–å­¦ä¹ æ½œåŠ›çš„æœ‰åŠ›æé†’ï¼Œå®ƒå¯ä»¥è§£é”äººå·¥ç³»ç»Ÿçš„æ–°æ™ºèƒ½æ°´å¹³ï¼Œä¸ºæœªæ¥æ›´è‡ªä¸»å’Œé€‚åº”æ€§çš„æ¨¡å‹é“ºå¹³é“è·¯ã€‚

![](/images/2025/DeepSeekR1/Table3.png)

```
é—®é¢˜ï¼šå¦‚æœ ğ‘ > 1ï¼Œåˆ™ âˆšï¸(ğ‘âˆ’âˆš(ğ‘+ğ‘¥)) = ğ‘¥ çš„å®æ•°è§£çš„å’Œç­‰äº
```

```
å›ç­”ï¼š<think>
è¦è§£æ–¹ç¨‹ âˆšï¸(ğ‘âˆ’âˆš(ğ‘+ğ‘¥)) = ğ‘¥ï¼Œè®©æˆ‘ä»¬ä»ä¸¤è¾¹å¹³æ–¹å¼€å§‹Â·Â·Â·
(âˆšï¸(ğ‘âˆ’âˆš(ğ‘+ğ‘¥)))^2 = ğ‘¥2 â‡’ ğ‘âˆ’âˆš(ğ‘+ğ‘¥) = ğ‘¥^2ã€‚
é‡æ–°æ’åˆ—ä»¥éš”ç¦»å†…éƒ¨å¹³æ–¹æ ¹é¡¹ï¼š
(ğ‘âˆ’ğ‘¥^2)^2 = ğ‘+ğ‘¥ = â‡’ ğ‘2âˆ’2ğ‘ğ‘¥^2+(ğ‘¥^2)^2 = ğ‘+ğ‘¥ â‡’ ğ‘¥4âˆ’2ğ‘ğ‘¥^2âˆ’ğ‘¥+(ğ‘^2-a) = 0
. . .
**ç­‰ç­‰ï¼Œç­‰ç­‰ã€‚ç­‰ç­‰ã€‚è¿™æ˜¯æˆ‘å¯ä»¥æ ‡è®°çš„ä¸€ä¸ªé¡¿æ‚Ÿæ—¶åˆ»ã€‚**
è®©æˆ‘ä»¬é€æ­¥é‡æ–°è¯„ä¼°è¿™ä¸€æ­¥éª¤ï¼Œä»¥ç¡®å®šæ˜¯å¦å¯ä»¥Â·Â·Â·
æˆ‘ä»¬ä»æ–¹ç¨‹å¼€å§‹ï¼š
âˆšï¸(ğ‘âˆ’âˆš(ğ‘+ğ‘¥)) = ğ‘¥
é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä¸¤è¾¹å¹³æ–¹ï¼š
ğ‘âˆ’âˆš(ğ‘+ğ‘¥) = ğ‘¥^2 â‡’ âˆš(ğ‘+ğ‘¥) = ğ‘âˆ’ğ‘¥^2
æ¥ä¸‹æ¥ï¼Œæˆ‘å¯ä»¥å†æ¬¡ä¸¤è¾¹å¹³æ–¹ï¼Œå¤„ç†æ–¹ç¨‹ï¼šÂ·Â·Â·
```

`è¡¨3 DeepSeek-R1-Zeroä¸­é—´ç‰ˆæœ¬çš„ä¸€ä¸ªæœ‰è¶£çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚æ¨¡å‹å­¦ä¼šé‡æ–°æ€è€ƒï¼Œä½¿ç”¨ä¸€ç§æ‹Ÿäººçš„è¯­æ°”ã€‚è¿™å¯¹æˆ‘ä»¬æ¥è¯´ä¹Ÿæ˜¯ä¸€ä¸ªé¡¿æ‚Ÿæ—¶åˆ»ï¼Œè®©æˆ‘ä»¬è§è¯äº†å¼ºåŒ–å­¦ä¹ çš„åŠ›é‡å’Œç¾ä¸½ã€‚`

Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning
capabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces
several issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,
and language mixing. To make reasoning processes more readable and share them with the
open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly
cold-start data.

**DeepSeek-R1-Zeroçš„ç¼ºç‚¹** å°½ç®¡DeepSeek-R1-Zeroè¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶è‡ªä¸»å‘å±•å‡ºæ„æƒ³ä¸åˆ°çš„å¼ºå¤§æ¨ç†è¡Œä¸ºï¼Œä½†å®ƒé¢ä¸´ä¸€äº›é—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒDeepSeek-R1-Zeroåœ¨é¢å¯¹è¯¸å¦‚å¯è¯»æ€§å·®ã€è¯­è¨€æ··åˆç­‰æŒ‘æˆ˜æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºäº†ä½¿æ¨ç†è¿‡ç¨‹æ›´æ˜“è¯»ï¼Œå¹¶ä¸å¼€æ”¾ç¤¾åŒºåˆ†äº«ï¼Œæˆ‘ä»¬æ¢ç´¢äº†DeepSeek-R1ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨äººç±»å‹å¥½çš„å†·å¯åŠ¨æ•°æ®çš„RLæ–¹æ³•ã€‚

### DeepSeek-R1: Reinforcement Learning with Cold Startï¼ˆDeepSeek-R1ï¼šå†·å¯åŠ¨çš„å¼ºåŒ–å­¦ä¹ ï¼‰

Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can
reasoning performance be further improved or convergence accelerated by incorporating a small
amount of high-quality data as a cold start? 2) How can we train a user-friendly model that
not only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong
general capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The
pipeline consists of four stages, outlined as follows.

å—DeepSeek-R1-Zeroä»¤äººé¼“èˆçš„ç»“æœå¯å‘ï¼Œè‡ªç„¶ä¼šå‡ºç°ä¸¤ä¸ªé—®é¢˜ï¼š1ï¼‰é€šè¿‡å°†å°‘é‡é«˜è´¨é‡æ•°æ®ä½œä¸ºå†·å¯åŠ¨ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ¨ç†æ€§èƒ½æˆ–åŠ é€Ÿæ”¶æ•›å—ï¼Ÿ2ï¼‰æˆ‘ä»¬å¦‚ä½•è®­ç»ƒä¸€ä¸ªç”¨æˆ·å‹å¥½çš„æ¨¡å‹ï¼Œå®ƒä¸ä»…èƒ½å¤Ÿäº§ç”Ÿæ¸…æ™°è¿è´¯çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œè€Œä¸”è¿˜èƒ½å±•ç¤ºå¼ºå¤§çš„é€šç”¨èƒ½åŠ›ï¼Ÿä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæµç¨‹æ¥è®­ç»ƒDeepSeek-R1ã€‚è¯¥æµç¨‹åŒ…æ‹¬å››ä¸ªé˜¶æ®µï¼Œå¦‚ä¸‹æ‰€è¿°ã€‚

#### Cold Startï¼ˆå†·å¯åŠ¨ï¼‰

Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting models to generate detailed answers with reflection and verification, gathering DeepSeek-R1-Zero outputs in a readable format, and refining the results through post-processing by human annotators.

ä¸ DeepSeek-R1-Zero ä¸åŒï¼Œä¸ºäº†é˜²æ­¢åŸºç¡€æ¨¡å‹åœ¨ RL è®­ç»ƒæ—©æœŸå‡ºç°ä¸ç¨³å®šçš„å†·å¯åŠ¨é˜¶æ®µï¼Œå¯¹äº DeepSeek-R1ï¼Œæˆ‘ä»¬æ„å»ºå¹¶æ”¶é›†å°‘é‡çš„é•¿ CoT æ•°æ®ï¼Œä»¥ä½œä¸ºåˆå§‹ RL å‚ä¸è€…å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†æ”¶é›†æ­¤ç±»æ•°æ®ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡ ç§æ–¹æ³•ï¼šä½¿ç”¨é•¿ CoT çš„å°‘æ ·æœ¬æç¤ºä½œä¸ºç¤ºä¾‹ï¼Œç›´æ¥æç¤ºæ¨¡å‹é€šè¿‡åæ€å’ŒéªŒè¯ç”Ÿæˆè¯¦ç»†ç­”æ¡ˆï¼Œä»¥å¯è¯»æ ¼å¼æ”¶é›† DeepSeek-R1-Zero è¾“å‡ºï¼Œå¹¶é€šè¿‡äººå·¥æ³¨é‡Šè€…çš„åæœŸå¤„ç†æ¥å®Œå–„ç»“æœã€‚

In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data include:

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ”¶é›†äº†æ•°åƒä¸ªå†·å¯åŠ¨æ•°æ®ï¼Œä»¥å¾®è°ƒ DeepSeek-V3-Base ä½œä¸º RL çš„èµ·ç‚¹ã€‚ä¸ DeepSeek-R1-Zero ç›¸æ¯”ï¼Œå†·å¯åŠ¨æ•°æ®çš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š

**Readability**: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable
for reading. Responses may mix multiple languages or lack markdown formatting to
highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,
we design a readable pattern that includes a summary at the end of each response and
filters out responses that are not reader-friendly. Here, we define the output format as
|special_token|<reasoning_process>|special_token|<summary>, where the reasoning
process is the CoT for the query, and the summary is used to summarize the reasoning
results.

**å¯è¯»æ€§**ï¼šDeepSeek-R1-Zeroçš„ä¸€ä¸ªä¸»è¦é™åˆ¶æ˜¯å…¶å†…å®¹é€šå¸¸ä¸é€‚åˆé˜…è¯»ã€‚å“åº”å¯èƒ½æ··åˆå¤šç§è¯­è¨€æˆ–ç¼ºä¹markdownæ ¼å¼æ¥ä¸ºç”¨æˆ·çªå‡ºæ˜¾ç¤ºç­”æ¡ˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨ä¸ºDeepSeek-R1åˆ›å»ºå†·å¯åŠ¨æ•°æ®æ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯è¯»æ€§æ¨¡å¼ï¼Œåœ¨æ¯ä¸ªå“åº”çš„æœ«å°¾åŒ…å«ä¸€ä¸ªæ€»ç»“ï¼Œå¹¶è¿‡æ»¤æ‰ä¸é€‚åˆé˜…è¯»çš„å“åº”ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è¾“å‡ºæ ¼å¼å®šä¹‰ä¸º&#124;special_token&#124;&lt;reasoning_process>&#124;special_token&#124;&lt;summary>ï¼Œå…¶ä¸­æ¨ç†è¿‡ç¨‹æ˜¯é’ˆå¯¹æŸ¥è¯¢çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ï¼Œè€Œæ€»ç»“ç”¨äºæ¦‚æ‹¬æ¨ç†ç»“æœã€‚

**Potential**: By carefully designing the pattern for cold-start data with human priors, we
observe better performance against DeepSeek-R1-Zero. We believe the iterative training is
a better way for reasoning models.

**æ½œåŠ›**ï¼šé€šè¿‡ç²¾å¿ƒè®¾è®¡å…·æœ‰äººç±»å…ˆéªŒçŸ¥è¯†çš„å†·å¯åŠ¨æ•°æ®æ¨¡å¼ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸ DeepSeek-R1-Zero ç›¸æ¯”æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬è®¤ä¸ºè¿­ä»£è®­ç»ƒæ˜¯æ¨ç†æ¨¡å‹çš„æ›´å¥½æ–¹å¼ã€‚

#### Reasoning-oriented Reinforcement Learningï¼ˆé¢å‘æ¨ç†çš„å¼ºåŒ–å­¦ä¹ ï¼‰

After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale
reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses
on enhancing the modelâ€™s reasoning capabilities, particularly in reasoning-intensive tasks such
as coding, mathematics, science, and logic reasoning, which involve well-defined problems with
clear solutions. During the training process, we observe that CoT often exhibits language mixing,
particularly when RL prompts involve multiple languages. To mitigate the issue of language
mixing, we introduce a language consistency reward during RL training, which is calculated
as the proportion of target language words in the CoT. Although ablation experiments show
that such alignment results in a slight degradation in the modelâ€™s performance, this reward
aligns with human preferences, making it more readable. Finally, we combine the accuracy of
reasoning tasks and the reward for language consistency by directly summing them to form the
final reward. We then apply reinforcement learning (RL) training on the fine-tuned model until
it achieves convergence on reasoning tasks.

åœ¨å†·å¯åŠ¨æ•°æ®ä¸Šå¾®è°ƒ DeepSeek-V3-Base åï¼Œæˆ‘ä»¬åº”ç”¨äº†ä¸ DeepSeek-R1-Zero ä¸­ä½¿ç”¨çš„ç›¸åŒçš„å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ã€‚è¿™ä¸ªé˜¶æ®µçš„é‡ç‚¹æ˜¯å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­ï¼Œå¦‚ç¼–ç ã€æ•°å­¦ã€ç§‘å­¦å’Œé€»è¾‘æ¨ç†ï¼Œè¿™äº›ä»»åŠ¡æ¶‰åŠåˆ°å…·æœ‰æ˜ç¡®è§£å†³æ–¹æ¡ˆçš„æ˜ç¡®å®šä¹‰çš„é—®é¢˜ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ° CoT é€šå¸¸ä¼šå‡ºç°è¯­è¨€æ··åˆï¼Œç‰¹åˆ«æ˜¯å½“ RL æç¤ºæ¶‰åŠå¤šç§è¯­è¨€æ—¶ã€‚ä¸ºäº†å‡è½»è¯­è¨€æ··åˆé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ RL è®­ç»ƒæœŸé—´å¼•å…¥äº†ä¸€ç§è¯­è¨€ä¸€è‡´æ€§å¥–åŠ±ï¼Œè¯¥å¥–åŠ±è®¡ç®—ä¸º CoT ä¸­ç›®æ ‡è¯­è¨€å•è¯çš„æ¯”ä¾‹ã€‚å°½ç®¡æ¶ˆèå®éªŒè¡¨æ˜è¿™ç§å¯¹é½ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ç•¥å¾®ä¸‹é™ï¼Œä½†è¿™ç§å¥–åŠ±ç¬¦åˆäººç±»çš„åå¥½ï¼Œä½¿å…¶æ›´æ˜“è¯»ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ¨ç†ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œè¯­è¨€ä¸€è‡´æ€§å¥–åŠ±ç›´æ¥ç›¸åŠ ï¼Œå½¢æˆæœ€ç»ˆå¥–åŠ±ã€‚ç„¶åæˆ‘ä»¬åœ¨å¾®è°ƒæ¨¡å‹ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œç›´åˆ°å®ƒåœ¨æ¨ç†ä»»åŠ¡ä¸Šæ”¶æ•›ã€‚

#### Rejection Sampling and Supervised Fine-Tuningï¼ˆæ‹’ç»æŠ½æ ·å’Œç›‘ç£å¾®è°ƒï¼‰

When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT
(Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which
primarily focuses on reasoning, this stage incorporates data from other domains to enhance the
modelâ€™s capabilities in writing, role-playing, and other general-purpose tasks. Specifically, we
generate the data and fine-tune the model as described below.

å½“é¢å‘æ¨ç†çš„ RL æ”¶æ•›æ—¶ï¼Œæˆ‘ä»¬åˆ©ç”¨ç”Ÿæˆçš„æ£€æŸ¥ç‚¹æ”¶é›† SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰æ•°æ®ï¼Œç”¨äºåç»­è½®æ¬¡ã€‚ä¸æœ€åˆçš„å†·å¯åŠ¨æ•°æ®ä¸åŒï¼Œåè€…ä¸»è¦å…³æ³¨æ¨ç†ï¼Œè¿™ä¸ªé˜¶æ®µåŒ…å«äº†æ¥è‡ªå…¶ä»–é¢†åŸŸçš„æ•°æ®ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨å†™ä½œã€è§’è‰²æ‰®æ¼”å’Œå…¶ä»–é€šç”¨ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”Ÿæˆæ•°æ®å¹¶å¾®è°ƒæ¨¡å‹å¦‚ä¸‹æ‰€è¿°ã€‚

**Reasoning data** We curate reasoning prompts and generate reasoning trajectories by perform-
ing rejection sampling from the checkpoint from the above RL training.In the previous stage, we only included data that could be evaluated using rule-based rewards. However, in this stage, we expand the dataset by incorporating additional data, some of which use a generative reward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For each prompt, we sample multiple responses and retain only the correct ones. In total, we collect about 600k reasoning related training samples.

**æ¨ç†æ•°æ®** æˆ‘ä»¬é€šè¿‡ä»ä¸Šè¿°å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ£€æŸ¥ç‚¹è¿›è¡Œæ‹’ç»æŠ½æ ·æ¥æ•´ç†æ¨ç†æç¤ºå¹¶ç”Ÿæˆæ¨ç†è½¨è¿¹ã€‚åœ¨ä¸Šä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä»…åŒ…å«å¯ä»¥ä½¿ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±è¿›è¡Œè¯„ä¼°çš„æ•°æ®ã€‚ä½†æ˜¯ï¼Œåœ¨æ­¤é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡åˆå¹¶å…¶ä»–æ•°æ®æ¥æ‰©å±•æ•°æ®é›†ï¼Œå…¶ä¸­ä¸€äº›æ•°æ®ä½¿ç”¨ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œå°†åŸºæœ¬äº‹å®å’Œæ¨¡å‹é¢„æµ‹è¾“å…¥ DeepSeek-V3 è¿›è¡Œåˆ¤æ–­ã€‚æ­¤å¤–ï¼Œç”±äºæ¨¡å‹è¾“å‡ºæœ‰æ—¶æ··ä¹±ä¸”éš¾ä»¥é˜…è¯»ï¼Œæˆ‘ä»¬è¿‡æ»¤æ‰äº†æ··åˆè¯­è¨€ã€é•¿æ®µè½å’Œä»£ç å—çš„æ€è·¯é“¾ã€‚å¯¹äºæ¯ä¸ªæç¤ºï¼Œæˆ‘ä»¬ä¼šâ€‹â€‹æŠ½æ ·å¤šä¸ªå“åº”å¹¶ä»…ä¿ç•™æ­£ç¡®çš„å“åº”ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬æ”¶é›†äº†å¤§çº¦ 60 ä¸‡ä¸ªä¸æ¨ç†ç›¸å…³çš„è®­ç»ƒæ ·æœ¬ã€‚

**Non-Reasoning data** For non-reasoning data, such as writing, factual QA, self-cognition, and translation, we adopt the DeepSeek-V3 pipeline and reuse portions of the SFT dataset of DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thought before answering the question by prompting. However, for simpler queries, such as â€œhelloâ€ we do not provide a CoT in response. In the end, we collected a total of approximately 200k training samples that are unrelated to reasoning.

**éæ¨ç†æ•°æ®** å¯¹äºéæ¨ç†æ•°æ®ï¼Œä¾‹å¦‚å†™ä½œã€äº‹å®é—®ç­”ã€è‡ªæˆ‘è®¤çŸ¥å’Œç¿»è¯‘ï¼Œæˆ‘ä»¬é‡‡ç”¨ DeepSeek-V3 æµç¨‹å¹¶é‡ç”¨ DeepSeek-V3 çš„éƒ¨åˆ† SFT æ•°æ®é›†ã€‚å¯¹äºæŸäº›éæ¨ç†ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¼šè°ƒç”¨ DeepSeek-V3 ç”Ÿæˆæ½œåœ¨çš„æ€è·¯é“¾ï¼Œç„¶åå†é€šè¿‡æç¤ºå›ç­”é—®é¢˜ã€‚ä½†æ˜¯ï¼Œå¯¹äºæ›´ç®€å•çš„æŸ¥è¯¢ï¼Œä¾‹å¦‚â€œä½ å¥½â€ï¼Œæˆ‘ä»¬ä¸æä¾› CoT ä½œä¸ºå“åº”ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æ€»å…±æ”¶é›†äº†å¤§çº¦ 20 ä¸‡ä¸ªä¸æ¨ç†æ— å…³çš„è®­ç»ƒæ ·æœ¬ã€‚

We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples.

æˆ‘ä»¬ä½¿ç”¨ä¸Šè¿°ç²¾å¿ƒç­–åˆ’çš„çº¦ 80 ä¸‡ä¸ªæ ·æœ¬çš„æ•°æ®é›†å¯¹ DeepSeek-V3-Base è¿›è¡Œä¸¤ä¸ª epochs çš„å¾®è°ƒã€‚

#### Reinforcement Learning for all Scenariosï¼ˆæ‰€æœ‰åœºæ™¯çš„å¼ºåŒ–å­¦ä¹ ï¼‰

To further align the model with human preferences, we implement a secondary reinforcement
learning stage aimed at improving the modelâ€™s helpfulness and harmlessness while simultane-
ously refining its reasoning capabilities. Specifically, we train the model using a combination
of reward signals and diverse prompt distributions. For reasoning data, we adhere to the
methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the
learning process in math, code, and logical reasoning domains. For general data, we resort to
reward models to capture human preferences in complex and nuanced scenarios. We build
upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and train-
ing prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the
assessment emphasizes the utility and relevance of the response to the user while minimizing
interference with the underlying reasoning process. For harmlessness, we evaluate the entire
response of the model, including both the reasoning process and the summary, to identify and
mitigate any potential risks, biases, or harmful content that may arise during the generation
process. Ultimately, the integration of reward signals and diverse data distributions enables us
to train a model that excels in reasoning while prioritizing helpfulness and harmlessness.

ä¸ºäº†è¿›ä¸€æ­¥ä½¿æ¨¡å‹ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ä¸ªæ—¨åœ¨æé«˜æ¨¡å‹çš„å¸®åŠ©æ€§å’Œæ— å®³æ€§çš„æ¬¡çº§å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼ŒåŒæ—¶å®Œå–„å…¶æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å¥–åŠ±ä¿¡å·å’Œå¤šæ ·åŒ–çš„æç¤ºåˆ†å¸ƒçš„ç»„åˆæ¥è®­ç»ƒæ¨¡å‹ã€‚å¯¹äºæ¨ç†æ•°æ®ï¼Œæˆ‘ä»¬éµå¾ª DeepSeek-R1-Zero ä¸­æ¦‚è¿°çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±æ¥å¼•å¯¼æ•°å­¦ã€ä»£ç å’Œé€»è¾‘æ¨ç†é¢†åŸŸçš„å­¦ä¹ è¿‡ç¨‹ã€‚å¯¹äºä¸€èˆ¬æ•°æ®ï¼Œæˆ‘ä»¬æ±‚åŠ©äºå¥–åŠ±æ¨¡å‹ï¼Œä»¥æ•æ‰å¤æ‚å’Œå¾®å¦™åœºæ™¯ä¸­çš„äººç±»åå¥½ã€‚æˆ‘ä»¬åœ¨ DeepSeek-V3 æµç¨‹çš„åŸºç¡€ä¸Šæ„å»ºï¼Œå¹¶é‡‡ç”¨ç±»ä¼¼çš„åå¥½å¯¹å’Œè®­ç»ƒæç¤ºåˆ†å¸ƒã€‚å¯¹äºå¸®åŠ©æ€§ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæœ€ç»ˆçš„æ€»ç»“ï¼Œç¡®ä¿è¯„ä¼°å¼ºè°ƒå“åº”å¯¹ç”¨æˆ·çš„å®ç”¨æ€§å’Œç›¸å…³æ€§ï¼ŒåŒæ—¶æœ€å¤§ç¨‹åº¦åœ°å‡å°‘å¯¹åº•å±‚æ¨ç†è¿‡ç¨‹çš„å¹²æ‰°ã€‚å¯¹äºæ— å®³æ€§ï¼Œæˆ‘ä»¬è¯„ä¼°æ¨¡å‹çš„æ•´ä¸ªå“åº”ï¼ŒåŒ…æ‹¬æ¨ç†è¿‡ç¨‹å’Œæ€»ç»“ï¼Œä»¥è¯†åˆ«å’Œå‡è½»åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„ä»»ä½•æ½œåœ¨é£é™©ã€åè§æˆ–æœ‰å®³å†…å®¹ã€‚æœ€ç»ˆï¼Œå¥–åŠ±ä¿¡å·å’Œå¤šæ ·åŒ–çš„æ•°æ®åˆ†å¸ƒçš„æ•´åˆä½¿æˆ‘ä»¬èƒ½å¤Ÿè®­ç»ƒå‡ºä¸€ä¸ªåœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¼˜å…ˆè€ƒè™‘å¸®åŠ©æ€§å’Œæ— å®³æ€§ã€‚

### Distillation: Empower Small Models with Reasoning Capabilityï¼ˆè’¸é¦ï¼šèµ‹äºˆå°æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼‰

To equip more efficient smaller models with reasoning capabilities like DeekSeek-R1, we directly
fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using
the 800k samples curated with DeepSeek-R1, as detailed in Â§2.3.3. Our findings indicate that
this straightforward distillation method significantly enhances the reasoning abilities of smaller
models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-
14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct. We select Llama-3.3 because its
reasoning capability is slightly better than that of Llama-3.1.

ä¸ºäº†è®©æ›´é«˜æ•ˆçš„å°æ¨¡å‹å…·å¤‡åƒ DeepSeek-R1 è¿™æ ·çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ DeepSeek-R1 ç²¾å¿ƒç­–åˆ’çš„ 80 ä¸‡ä¸ªæ ·æœ¬å¯¹å¼€æºæ¨¡å‹ï¼ˆå¦‚ Qwenï¼ˆQwenï¼Œ2024bï¼‰å’Œ Llamaï¼ˆAI@Metaï¼Œ2024ï¼‰ï¼‰è¿›è¡Œå¾®è°ƒï¼Œè¯¦è§ Â§2.3.3ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™ç§ç›´æ¥è’¸é¦æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†è¾ƒå°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨çš„åŸºç¡€æ¨¡å‹æ˜¯ Qwen2.5-Math-1.5Bã€Qwen2.5-Math-7Bã€Qwen2.5-14Bã€Qwen2.5-32Bã€Llama-3.1-8B å’Œ Llama-3.3-70B-Instructã€‚æˆ‘ä»¬é€‰æ‹© Llama-3.3ï¼Œå› ä¸ºå®ƒçš„æ¨ç†èƒ½åŠ›ç•¥ä¼˜äº Llama-3.1ã€‚

For distilled models, we apply only SFT and do not include an RL stage, even though
incorporating RL could substantially boost model performance. Our primary goal here is to
demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL
stage to the broader research community.

å¯¹äºè’¸é¦æ¨¡å‹ï¼Œæˆ‘ä»¬ä»…åº”ç”¨ SFTï¼Œä¸åŒ…æ‹¬ RL é˜¶æ®µï¼Œå°½ç®¡ RL é˜¶æ®µå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯å±•ç¤ºè’¸é¦æŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼Œå°† RL é˜¶æ®µçš„æ¢ç´¢ç•™ç»™æ›´å¹¿æ³›çš„ç ”ç©¶ç¤¾åŒºã€‚

## Experimentï¼ˆå®éªŒï¼‰

**Benchmarks** We evaluate models on MMLU (Hendrycks et al., 2020), MMLU-Redux (Gema
et al., 2024), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), and CMMLU (Li et al.,
2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al.,
2023), SimpleQA (OpenAI, 2024c), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI,
112024d), Aider, LiveCodeBench (Jain et al., 2024) (2024-08 â€“ 2025-01), Codeforces 2, Chinese
National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Math-
ematics Examination 2024 (AIME 2024) (MAA, 2024). In addition to standard benchmarks, we
also evaluate our models on open-ended generation tasks using LLMs as judges. Specifically, we
adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li
et al., 2024), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. Here, we
only feed the final summary to evaluation to avoid the length bias. For distilled models, we
report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and
LiveCodeBench.

**åŸºå‡†** æˆ‘ä»¬åœ¨ MMLUï¼ˆHendrycks et al.ï¼Œ2020ï¼‰ã€MMLU-Reduxï¼ˆGema et al.ï¼Œ2024ï¼‰ã€MMLU-Proï¼ˆWang et al.ï¼Œ2024ï¼‰ã€C-Evalï¼ˆHuang et al.ï¼Œ2023ï¼‰å’Œ CMMLUï¼ˆLi et al.ï¼Œ2023ï¼‰ã€IFEvalï¼ˆZhou et al.ï¼Œ2023ï¼‰ã€FRAMESï¼ˆKrishna et al.ï¼Œ2024ï¼‰ã€GPQA Diamondï¼ˆRein et al.ï¼Œ2023ï¼‰ã€SimpleQAï¼ˆOpenAIï¼Œ2024cï¼‰ã€C-SimpleQAï¼ˆHe et al.ï¼Œ2024ï¼‰ã€SWE-Bench Verifiedï¼ˆOpenAIï¼Œ112024dï¼‰ã€Aiderã€LiveCodeBenchï¼ˆJain et al.ï¼Œ2024ï¼‰ï¼ˆ2024-08 â€“ 2025-01ï¼‰ã€Codeforces 2ã€ä¸­å›½å›½å®¶é«˜ä¸­æ•°å­¦å¥¥æ—åŒ¹å…‹ç«èµ›ï¼ˆCNMO 2024ï¼‰å’Œç¾å›½åˆçº§æ•°å­¦è€ƒè¯•2024ï¼ˆAIME 2024ï¼‰ï¼ˆMAAï¼Œ2024ï¼‰ä¸Šè¯„ä¼°æ¨¡å‹ã€‚é™¤äº†æ ‡å‡†åŸºå‡†ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ LLM ä½œä¸ºè¯„å§”åœ¨å¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ä¸Šè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬éµå¾ª AlpacaEval 2.0ï¼ˆDubois et al.ï¼Œ2024ï¼‰å’Œ Arena-Hardï¼ˆLi et al.ï¼Œ2024ï¼‰çš„åŸå§‹é…ç½®ï¼Œè¿™ä¸¤ä¸ªé…ç½®åˆ©ç”¨ GPT-4-Turbo-1106 ä½œä¸ºè¯„å§”è¿›è¡Œæˆå¯¹æ¯”è¾ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªå°†æœ€ç»ˆæ€»ç»“æä¾›ç»™è¯„ä¼°ï¼Œä»¥é¿å…é•¿åº¦åå·®ã€‚å¯¹äºè’¸é¦æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ AIME 2024ã€MATH-500ã€GPQA Diamondã€Codeforces å’Œ LiveCodeBench ä¸ŠæŠ¥å‘Šä»£è¡¨æ€§ç»“æœã€‚

**Evaluation Prompts** Following the setup in DeepSeek-V3, standard benchmarks such as
MMLU, DROP, GPQA Diamond, and SimpleQA are evaluated using prompts from the simple-
evals framework. For MMLU-Redux, we adopt the Zero-Eval prompt format (Lin, 2024) in a
zero-shot setting. In terms of MMLU-Pro, C-Eval and CLUE-WSC, since the original prompts
are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot
may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation
protocols with default prompts provided by their creators. For code and math benchmarks, the
HumanEval-Mul dataset covers eight mainstream programming languages (Python, Java, C++,
C#, JavaScript, TypeScript, PHP, and Bash). Model performance on LiveCodeBench is evaluated
using CoT format, with data collected between August 2024 and January 2025. The Codeforces
dataset is evaluated using problems from 10 Div.2 contests along with expert-crafted test cases,
after which the expected ratings and percentages of competitors are calculated. SWE-Bench
verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related
benchmarks are measured using a "diff" format. DeepSeek-R1 outputs are capped at a maximum
of 32,768 tokens for each benchmark.

**è¯„ä¼°æç¤º** æ ¹æ® DeepSeek-V3 çš„è®¾ç½®ï¼Œä½¿ç”¨ simple-evals æ¡†æ¶çš„æç¤ºæ¥è¯„ä¼°æ ‡å‡†åŸºå‡†ï¼Œå¦‚ MMLUã€DROPã€GPQA Diamond å’Œ SimpleQAã€‚å¯¹äº MMLU-Reduxï¼Œæˆ‘ä»¬åœ¨é›¶å°„å‡»è®¾ç½®ä¸­é‡‡ç”¨ Zero-Eval æç¤ºæ ¼å¼ï¼ˆLinï¼Œ2024ï¼‰ã€‚åœ¨ MMLU-Proã€C-Eval å’Œ CLUE-WSC æ–¹é¢ï¼Œç”±äºåŸå§‹æç¤ºæ˜¯å°‘æ ·æœ¬çš„ï¼Œæˆ‘ä»¬å°†æç¤ºç¨å¾®ä¿®æ”¹ä¸ºé›¶å°„å‡»è®¾ç½®ã€‚å°‘æ ·æœ¬ä¸­çš„ CoT å¯èƒ½ä¼šæŸå®³ DeepSeek-R1 çš„æ€§èƒ½ã€‚å…¶ä»–æ•°æ®é›†éµå¾ªå…¶åˆ›å»ºè€…æä¾›çš„é»˜è®¤æç¤ºçš„åŸå§‹è¯„ä¼°åè®®ã€‚å¯¹äºä»£ç å’Œæ•°å­¦åŸºå‡†ï¼ŒHumanEval-Mul æ•°æ®é›†æ¶µç›–äº†å…«ç§ä¸»æµç¼–ç¨‹è¯­è¨€ï¼ˆPythonã€Javaã€C++ã€C#ã€JavaScriptã€TypeScriptã€PHP å’Œ Bashï¼‰ã€‚ä½¿ç”¨ CoT æ ¼å¼è¯„ä¼° LiveCodeBench ä¸Šçš„æ¨¡å‹æ€§èƒ½ï¼Œæ•°æ®æ”¶é›†æ—¶é—´ä¸º 2024 å¹´ 8 æœˆè‡³ 2025 å¹´ 1 æœˆã€‚Codeforces æ•°æ®é›†ä½¿ç”¨ 10 ä¸ª Div.2 æ¯”èµ›çš„é—®é¢˜ä»¥åŠä¸“å®¶åˆ¶ä½œçš„æµ‹è¯•ç”¨ä¾‹è¿›è¡Œè¯„ä¼°ï¼Œç„¶åè®¡ç®—å‡ºé¢„æœŸçš„è¯„çº§å’Œç«äº‰è€…çš„ç™¾åˆ†æ¯”ã€‚SWE-Bench éªŒè¯ç»“æœæ˜¯é€šè¿‡æ— ä»£ç†æ¡†æ¶ï¼ˆXia et al.ï¼Œ2024ï¼‰è·å¾—çš„ã€‚AIDER ç›¸å…³åŸºå‡†ä½¿ç”¨â€œdiffâ€æ ¼å¼è¿›è¡Œæµ‹é‡ã€‚DeepSeek-R1 çš„è¾“å‡ºåœ¨æ¯ä¸ªåŸºå‡†ä¸Šæœ€å¤šä¸º 32,768 ä¸ªæ ‡è®°ã€‚

**Baselines** We conduct comprehensive evaluations against several strong baselines, including
DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, and OpenAI-o1-1217.
Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-
mance based on official reports. For distilled models, we also compare the open-source model
QwQ-32B-Preview (Qwen, 2024a).

**åŸºçº¿** æˆ‘ä»¬é’ˆå¯¹å‡ ä¸ªå¼ºåŸºçº¿è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬ DeepSeek-V3ã€Claude-Sonnet-3.5-1022ã€GPT-4o-0513ã€OpenAI-o1-mini å’Œ OpenAI-o1-1217ã€‚ç”±äºåœ¨ä¸­å›½å¤§é™†è®¿é—® OpenAI-o1-1217 API æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œæˆ‘ä»¬æ ¹æ®å®˜æ–¹æŠ¥å‘ŠæŠ¥å‘Šå…¶æ€§èƒ½ã€‚å¯¹äºè’¸é¦æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜æ¯”è¾ƒäº†å¼€æºæ¨¡å‹ QwQ-32B-Previewï¼ˆQwenï¼Œ2024aï¼‰ã€‚

**Generation Setup** For all our models, the maximum generation length is set to 32,768 tokens.
For benchmarks requiring sampling, we use a temperature of 0.6, a top-p value of 0.95, and
generate 64 responses per query to estimate pass@1.

**ç”Ÿæˆè®¾ç½®** å¯¹äºæˆ‘ä»¬æ‰€æœ‰çš„æ¨¡å‹ï¼Œæœ€å¤§ç”Ÿæˆé•¿åº¦è®¾ç½®ä¸º 32,768 ä¸ªæ ‡è®°ã€‚å¯¹äºéœ€è¦é‡‡æ ·çš„åŸºå‡†ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¸©åº¦ä¸º 0.6ï¼Œé¡¶éƒ¨-p å€¼ä¸º 0.95ï¼Œå¹¶ä¸ºæ¯ä¸ªæŸ¥è¯¢ç”Ÿæˆ 64 ä¸ªå“åº”ï¼Œä»¥ä¼°è®¡ pass@1ã€‚

### DeepSeek-R1 Evaluationï¼ˆDeepSeek-R1 è¯„ä¼°ï¼‰

For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Di-
amond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This
improvement is primarily attributed to enhanced accuracy in STEM-related questions, where
significant gains are achieved through large-scale reinforcement learning (RL). Additionally,
DeepSeek-R1 excels on FRAMES, a long-context-dependent QA task, showcasing its strong
document analysis capabilities. This highlights the potential of reasoning models in AI-driven
search and data analysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms
DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend
is observed where OpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1
performs worse than DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its
tendency to refuse answering certain queries after safety RL. Without safety RL, DeepSeek-R1
could achieve an accuracy of over 70%.

å¯¹äºæ•™è‚²å¯¼å‘çš„çŸ¥è¯†åŸºå‡†ï¼Œå¦‚ MMLUã€MMLU-Pro å’Œ GPQA Diamondï¼ŒDeepSeek-R1 åœ¨å‡†ç¡®æ€§ä¸Šè¡¨ç°ä¼˜äº DeepSeek-V3ã€‚è¿™ä¸€æ”¹è¿›ä¸»è¦å½’å› äºåœ¨ STEM ç›¸å…³é—®é¢˜ä¸­é€šè¿‡å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°çš„æ˜¾è‘—å¢ç›Šã€‚æ­¤å¤–ï¼ŒDeepSeek-R1 åœ¨ FRAMES ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿™æ˜¯ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡ä¾èµ–çš„ QA ä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ–‡æ¡£åˆ†æèƒ½åŠ›ã€‚è¿™çªæ˜¾äº†æ¨ç†æ¨¡å‹åœ¨ AI é©±åŠ¨çš„æœç´¢å’Œæ•°æ®åˆ†æä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚åœ¨äº‹å®åŸºå‡† SimpleQA ä¸Šï¼ŒDeepSeek-R1 èƒœè¿‡ DeepSeek-V3ï¼Œå±•ç¤ºäº†å…¶å¤„ç†åŸºäºäº‹å®çš„æŸ¥è¯¢çš„èƒ½åŠ›ã€‚åœ¨è¿™ä¸ªåŸºå‡†ä¸Šï¼ŒOpenAI-o1 è¶…è¿‡äº† GPT-4oï¼Œè¡¨ç°å‡ºç±»ä¼¼çš„è¶‹åŠ¿ã€‚ç„¶è€Œï¼ŒDeepSeek-R1 åœ¨ä¸­æ–‡ SimpleQA åŸºå‡†ä¸Šçš„è¡¨ç°ä¸å¦‚ DeepSeek-V3ï¼Œä¸»è¦æ˜¯ç”±äºå…¶åœ¨å®‰å…¨ RL åæ‹’ç»å›ç­”æŸäº›æŸ¥è¯¢çš„å€¾å‘ã€‚æ²¡æœ‰å®‰å…¨ RLï¼ŒDeepSeek-R1 å¯ä»¥è¾¾åˆ°è¶…è¿‡ 70% çš„å‡†ç¡®æ€§ã€‚

![](/images/2025/DeepSeekR1/Table4.png)

`è¡¨4 DeepSeek-R1 ä¸å…¶ä»–ä»£è¡¨æ€§æ¨¡å‹çš„æ¯”è¾ƒã€‚`

DeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a
modelâ€™s ability to follow format instructions. These improvements can be linked to the inclusion
of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL
training. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,
indicating DeepSeek-R1â€™s strengths in writing tasks and open-domain question answering. Its
significant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale
RL, which not only boosts reasoning capabilities but also improves performance across diverse
domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an
average of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that
DeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying
its robustness across multiple tasks.

DeepSeek-R1 åœ¨ IF-Eval ä¸Šä¹Ÿå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æˆç»©ï¼Œè¯¥åŸºå‡†æ—¨åœ¨è¯„ä¼°æ¨¡å‹éµå¾ªæ ¼å¼è¯´æ˜çš„èƒ½åŠ›ã€‚è¿™äº›æ”¹è¿›å¯ä»¥ä¸åœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œ RL è®­ç»ƒçš„æœ€åé˜¶æ®µåŒ…å«éµå¾ªè¯´æ˜æ•°æ®æœ‰å…³ã€‚æ­¤å¤–ï¼Œåœ¨ AlpacaEval2.0 å’Œ ArenaHard ä¸Šè§‚å¯Ÿåˆ°äº†æ˜¾è‘—çš„è¡¨ç°ï¼Œè¡¨æ˜ DeepSeek-R1 åœ¨å†™ä½œä»»åŠ¡å’Œå¼€æ”¾åŸŸé—®ç­”ä¸­çš„ä¼˜åŠ¿ã€‚å®ƒå¯¹ DeepSeek-V3 çš„æ˜¾è‘—è¶…è¶Šå¼ºè°ƒäº†å¤§è§„æ¨¡ RL çš„æ³›åŒ–ä¼˜åŠ¿ï¼Œè¿™ä¸ä»…æå‡äº†æ¨ç†èƒ½åŠ›ï¼Œè¿˜æé«˜äº†è·¨å¤šä¸ªé¢†åŸŸçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒDeepSeek-R1 ç”Ÿæˆçš„æ€»ç»“é•¿åº¦ç®€æ´ï¼ŒArenaHard å¹³å‡ä¸º 689 ä¸ªæ ‡è®°ï¼ŒAlpacaEval 2.0 ä¸º 2,218 ä¸ªå­—ç¬¦ã€‚è¿™è¡¨æ˜ DeepSeek-R1 åœ¨åŸºäº GPT çš„è¯„ä¼°ä¸­é¿å…å¼•å…¥é•¿åº¦åå·®ï¼Œè¿›ä¸€æ­¥å·©å›ºäº†å…¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­çš„ç¨³å¥æ€§ã€‚

On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,
surpassing other models by a large margin. A similar trend is observed on coding algorithm
tasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these
benchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1
on Aider but achieves comparable performance on SWE Verified. We believe the engineering
performance of DeepSeek-R1 will improve in the next version, as the amount of related RL
training data currently remains very limited.

åœ¨æ•°å­¦ä»»åŠ¡ä¸Šï¼ŒDeepSeek-R1 çš„è¡¨ç°ä¸ OpenAI-o1-1217 ç›¸å½“ï¼Œè¿œè¿œè¶…è¿‡å…¶ä»–æ¨¡å‹ã€‚åœ¨ç¼–ç ç®—æ³•ä»»åŠ¡ä¸Šä¹Ÿè§‚å¯Ÿåˆ°ç±»ä¼¼çš„è¶‹åŠ¿ï¼Œä¾‹å¦‚ LiveCodeBench å’Œ Codeforcesï¼Œæ¨ç†ä¸ºé‡ç‚¹çš„æ¨¡å‹ä¸»å¯¼äº†è¿™äº›åŸºå‡†ã€‚åœ¨é¢å‘å·¥ç¨‹çš„ç¼–ç ä»»åŠ¡ä¸­ï¼ŒOpenAI-o1-1217 åœ¨ Aider ä¸Šèƒœè¿‡ DeepSeek-R1ï¼Œä½†åœ¨ SWE éªŒè¯ä¸Šå–å¾—äº†å¯æ¯”çš„è¡¨ç°ã€‚æˆ‘ä»¬ç›¸ä¿¡ DeepSeek-R1 çš„å·¥ç¨‹æ€§èƒ½å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­å¾—åˆ°æ”¹å–„ï¼Œå› ä¸ºç›®å‰ç›¸å…³ RL è®­ç»ƒæ•°æ®çš„æ•°é‡ä»ç„¶éå¸¸æœ‰é™ã€‚

### Distilled Model Evaluationï¼ˆè’¸é¦æ¨¡å‹è¯„ä¼°ï¼‰

![](/images/2025/DeepSeekR1/Table5.png)

`è¡¨5 DeepSeek-R1 è’¸é¦æ¨¡å‹ä¸å…¶ä»–å¯æ¯”æ¨¡å‹åœ¨ä¸æ¨ç†ç›¸å…³çš„åŸºå‡†ä¸Šçš„æ¯”è¾ƒã€‚`

As shown in Table 5, simply distilling DeepSeek-R1â€™s outputs enables the efficient DeepSeek-
R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-
reasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-
Preview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly
exceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-
tion. Additionally, we found that applying RL to these distilled models yields significant further
gains. We believe this warrants further exploration and therefore present only the results of the
simple SFT-distilled models here.

å¦‚è¡¨ 5 æ‰€ç¤ºï¼Œç®€å•åœ°è’¸é¦ DeepSeek-R1 çš„è¾“å‡ºä½¿é«˜æ•ˆçš„ DeepSeek-R1-7Bï¼ˆå³ DeepSeek-R1-Distill-Qwen-7Bï¼Œä»¥ä¸‹ç±»ä¼¼ç¼©å†™ï¼‰åœ¨å„æ–¹é¢å‡èƒœè¿‡ GPT-4o-0513 ç­‰éæ¨ç†æ¨¡å‹ã€‚DeepSeek-R1-14B åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šè¶…è¿‡äº† QwQ-32B-Previewï¼Œè€Œ DeepSeek-R1-32B å’Œ DeepSeek-R1-70B åœ¨å¤§å¤šæ•°åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¿‡äº† o1-miniã€‚è¿™äº›ç»“æœå±•ç¤ºäº†è’¸é¦çš„å¼ºå¤§æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å°† RL åº”ç”¨äºè¿™äº›è’¸é¦æ¨¡å‹ä¼šå¸¦æ¥æ˜¾è‘—çš„è¿›ä¸€æ­¥æ”¶ç›Šã€‚æˆ‘ä»¬è®¤ä¸ºè¿™å€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢ï¼Œå› æ­¤è¿™é‡Œä»…å‘ˆç°ç®€å•çš„ SFT è’¸é¦æ¨¡å‹çš„ç»“æœã€‚

## Discussionï¼ˆè®¨è®ºï¼‰

### Distillation v.s. Reinforcement Learningï¼ˆè’¸é¦ä¸å¼ºåŒ–å­¦ä¹ ï¼‰

![](/images/2025/DeepSeekR1/Table6.png)

`è¡¨6 åœ¨ä¸æ¨ç†ç›¸å…³çš„åŸºå‡†ä¸Šæ¯”è¾ƒè’¸é¦å’Œ RL æ¨¡å‹ã€‚`

In Section 3.2, we can see that by distilling DeepSeek-R1, the small model can achieve
impressive results. However, there is still one question left: can the model achieve comparable
performance through the large-scale RL training discussed in the paper without distillation?

åœ¨ç¬¬ 3.2 èŠ‚ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é€šè¿‡è’¸é¦ DeepSeek-R1ï¼Œå°æ¨¡å‹å¯ä»¥å–å¾—ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œä»ç„¶æœ‰ä¸€ä¸ªé—®é¢˜ï¼šæ¨¡å‹æ˜¯å¦å¯ä»¥é€šè¿‡æœ¬æ–‡è®¨è®ºçš„å¤§è§„æ¨¡ RL è®­ç»ƒå®ç°å¯æ¯”æ€§èƒ½ï¼Œè€Œæ— éœ€è’¸é¦ï¼Ÿ

To answer this question, we conduct large-scale RL training on Qwen-32B-Base using math,
code, and STEM data, training for over 10K steps, resulting in DeepSeek-R1-Zero-Qwen-32B. The
experimental results, shown in Figure 6, demonstrate that the 32B base model, after large-scale
RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-
Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than
DeepSeek-R1-Zero-Qwen-32B across all benchmarks. Therefore, we can draw two conclusions:
First, distilling more powerful models into smaller ones yields excellent results, whereas smaller
models relying on the large-scale RL mentioned in this paper require enormous computational
power and may not even achieve the performance of distillation. Second, while distillation
strategies are both economical and effective, advancing beyond the boundaries of intelligence
may still require more powerful base models and larger-scale reinforcement learning.

ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹ Qwen-32B-Base ä½¿ç”¨æ•°å­¦ã€ä»£ç å’Œ STEM æ•°æ®è¿›è¡Œå¤§è§„æ¨¡ RL è®­ç»ƒï¼Œè®­ç»ƒè¶…è¿‡ 10K æ­¥ï¼Œå¾—åˆ° DeepSeek-R1-Zero-Qwen-32Bã€‚å®éªŒç»“æœå¦‚å›¾ 6 æ‰€ç¤ºï¼Œè¡¨æ˜ 32B åŸºç¡€æ¨¡å‹åœ¨å¤§è§„æ¨¡ RL è®­ç»ƒåï¼Œå®ç°äº†ä¸ QwQ-32B-Preview ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä» DeepSeek-R1 è’¸é¦è€Œæ¥çš„ DeepSeek-R1-Distill-Qwen-32B åœ¨æ‰€æœ‰åŸºå‡†ä¸Šçš„è¡¨ç°æ˜æ˜¾ä¼˜äº DeepSeek-R1-Zero-Qwen-32Bã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºä¸¤ä¸ªç»“è®ºï¼šé¦–å…ˆï¼Œå°†æ›´å¼ºå¤§çš„æ¨¡å‹è’¸é¦ä¸ºæ›´å°çš„æ¨¡å‹ä¼šäº§ç”Ÿå‡ºè‰²çš„ç»“æœï¼Œè€Œä¾èµ–æœ¬æ–‡æåˆ°çš„å¤§è§„æ¨¡ RL çš„è¾ƒå°æ¨¡å‹éœ€è¦å·¨å¤§çš„è®¡ç®—èƒ½åŠ›ï¼Œç”šè‡³å¯èƒ½æ— æ³•è¾¾åˆ°è’¸é¦çš„æ€§èƒ½ã€‚å…¶æ¬¡ï¼Œè™½ç„¶è’¸é¦ç­–ç•¥æ—¢ç»æµåˆæœ‰æ•ˆï¼Œä½†è¦è¶…è¶Šæ™ºèƒ½çš„è¾¹ç•Œå¯èƒ½ä»éœ€è¦æ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹å’Œæ›´å¤§è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ ã€‚

### Unsuccessful Attemptsï¼ˆæœªæˆåŠŸçš„å°è¯•ï¼‰

In the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along
the way. We share our failure experiences here to provide insights, but this does not imply that
these approaches are incapable of developing effective reasoning models.

åœ¨å¼€å‘ DeepSeek-R1 çš„æ—©æœŸé˜¶æ®µï¼Œæˆ‘ä»¬ä¹Ÿåœ¨é€”ä¸­é‡åˆ°äº†å¤±è´¥å’ŒæŒ«æŠ˜ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œåˆ†äº«æˆ‘ä»¬çš„å¤±è´¥ç»éªŒï¼Œä»¥æä¾›è§è§£ï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€è¿™äº›æ–¹æ³•æ— æ³•å¼€å‘å‡ºæœ‰æ•ˆçš„æ¨ç†æ¨¡å‹ã€‚

**Process Reward Model (PRM)** PRM is a reasonable method to guide the model toward better
approaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,
2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-
cess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,
determining whether the current intermediate step is correct is a challenging task. Automated
annotation using models may not yield satisfactory results, while manual annotation is not con-
ducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward
hacking (Gao et al., 2022), and retraining the reward model needs additional training resources
and it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good
ability to rerank the top-N responses generated by the model or assist in guided search (Snell
et al., 2024), its advantages are limited compared to the additional computational overhead it
introduces during large-scale reinforcement learning process in our experiments.

**è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰** PRM æ˜¯å¼•å¯¼æ¨¡å‹æœç€æ›´å¥½çš„æ–¹æ³•è§£å†³æ¨ç†ä»»åŠ¡çš„åˆç†æ–¹æ³•ï¼ˆLightman et al.ï¼Œ2023ï¼›Uesato et al.ï¼Œ2022ï¼›Wang et al.ï¼Œ2023ï¼‰ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼ŒPRM æœ‰ä¸‰ä¸ªä¸»è¦é™åˆ¶å¯èƒ½ä¼šé˜»ç¢å…¶æœ€ç»ˆæˆåŠŸã€‚é¦–å…ˆï¼Œé€šå¸¸æ¨ç†ä¸­éš¾ä»¥æ˜ç¡®å®šä¹‰ç»†ç²’åº¦æ­¥éª¤ã€‚å…¶æ¬¡ï¼Œç¡®å®šå½“å‰ä¸­é—´æ­¥éª¤æ˜¯å¦æ­£ç¡®æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä½¿ç”¨æ¨¡å‹è¿›è¡Œè‡ªåŠ¨æ³¨é‡Šå¯èƒ½ä¸ä¼šäº§ç”Ÿä»¤äººæ»¡æ„çš„ç»“æœï¼Œè€Œæ‰‹åŠ¨æ³¨é‡Šä¸åˆ©äºæ‰©å±•ã€‚ç¬¬ä¸‰ï¼Œä¸€æ—¦å¼•å…¥åŸºäºæ¨¡å‹çš„ PRMï¼Œå®ƒä¸å¯é¿å…åœ°ä¼šå¯¼è‡´å¥–åŠ±é»‘å®¢ï¼ˆGao et al.ï¼Œ2022ï¼‰ï¼Œé‡æ–°è®­ç»ƒå¥–åŠ±æ¨¡å‹éœ€è¦é¢å¤–çš„è®­ç»ƒèµ„æºï¼Œå¹¶ä¸”ä¼šä½¿æ•´ä¸ªè®­ç»ƒæµç¨‹å¤æ‚åŒ–ã€‚æ€»ä¹‹ï¼Œè™½ç„¶ PRM åœ¨é‡æ–°æ’åˆ—æ¨¡å‹ç”Ÿæˆçš„å‰ N ä¸ªå“åº”æˆ–åœ¨å¼•å¯¼æœç´¢ä¸­æä¾›å¸®åŠ©æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ˆSnell et al.ï¼Œ2024ï¼‰ï¼Œä½†ä¸å…¶åœ¨æˆ‘ä»¬çš„å®éªŒä¸­å¼•å…¥çš„é¢å¤–è®¡ç®—å¼€é”€ç›¸æ¯”ï¼Œå…¶ä¼˜åŠ¿æœ‰é™ã€‚

**Monte Carlo Tree Search (MCTS)** Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-
ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time
compute scalability. This approach involves breaking answers into smaller parts to allow the
model to explore the solution space systematically. To facilitate this, we prompt the model to
generate multiple tags that correspond to specific reasoning steps necessary for the search. For
training, we first use collected prompts to find answers via MCTS guided by a pre-trained value
model. Subsequently, we use the resulting question-answer pairs to train both the actor model
and the value model, iteratively refining the process.

**è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰** å— AlphaGoï¼ˆSilver et al.ï¼Œ2017bï¼‰å’Œ AlphaZeroï¼ˆSilver et al.ï¼Œ2017aï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥å¢å¼ºæµ‹è¯•æ—¶çš„è®¡ç®—å¯æ‰©å±•æ€§ã€‚è¿™ç§æ–¹æ³•æ¶‰åŠå°†ç­”æ¡ˆåˆ†è§£ä¸ºè¾ƒå°çš„éƒ¨åˆ†ï¼Œä»¥ä½¿æ¨¡å‹èƒ½å¤Ÿç³»ç»Ÿåœ°æ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´ã€‚ä¸ºäº†ä¿ƒè¿›è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æç¤ºæ¨¡å‹ç”Ÿæˆå¤šä¸ªæ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾å¯¹åº”äºæœç´¢æ‰€éœ€çš„ç‰¹å®šæ¨ç†æ­¥éª¤ã€‚å¯¹äºè®­ç»ƒï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨æ”¶é›†çš„æç¤ºé€šè¿‡ç”±é¢„è®­ç»ƒå€¼æ¨¡å‹å¼•å¯¼çš„ MCTS æ‰¾åˆ°ç­”æ¡ˆã€‚éšåï¼Œæˆ‘ä»¬ä½¿ç”¨ç”Ÿæˆçš„é—®é¢˜-ç­”æ¡ˆå¯¹æ¥è®­ç»ƒ actor æ¨¡å‹å’Œå€¼æ¨¡å‹ï¼Œè¿­ä»£åœ°å®Œå–„è¿™ä¸ªè¿‡ç¨‹ã€‚

However, this approach encounters several challenges when scaling up the training. First,
unlike chess, where the search space is relatively well-defined, token generation presents an
exponentially larger search space. To address this, we set a maximum extension limit for each
node, but this can lead to the model getting stuck in local optima. Second, the value model
directly influences the quality of generation since it guides each step of the search process.
Training a fine-grained value model is inherently difficult, which makes it challenging for the
model to iteratively improve. While AlphaGoâ€™s core success relied on training a value model to
progressively enhance its performance, this principle proves difficult to replicate in our setup
due to the complexities of token generation.

ç„¶è€Œï¼Œå½“æ‰©å±•è®­ç»ƒè§„æ¨¡æ—¶ï¼Œè¿™ç§æ–¹æ³•é‡åˆ°äº†å‡ ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œä¸å›½é™…è±¡æ£‹ä¸åŒï¼Œå…¶ä¸­æœç´¢ç©ºé—´ç›¸å¯¹æ˜ç¡®å®šä¹‰ï¼Œæ ‡è®°ç”Ÿæˆå‘ˆæŒ‡æ•°çº§æ›´å¤§çš„æœç´¢ç©ºé—´ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¾ç½®äº†æœ€å¤§æ‰©å±•é™åˆ¶ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹é™·å…¥å±€éƒ¨æœ€ä¼˜è§£ã€‚å…¶æ¬¡ï¼Œå€¼æ¨¡å‹ç›´æ¥å½±å“ç”Ÿæˆçš„è´¨é‡ï¼Œå› ä¸ºå®ƒæŒ‡å¯¼æœç´¢è¿‡ç¨‹çš„æ¯ä¸€æ­¥ã€‚è®­ç»ƒç»†ç²’åº¦çš„å€¼æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯å›°éš¾çš„ï¼Œè¿™ä½¿å¾—æ¨¡å‹éš¾ä»¥è¿­ä»£åœ°æ”¹è¿›ã€‚è™½ç„¶ AlphaGo çš„æ ¸å¿ƒæˆåŠŸä¾èµ–äºè®­ç»ƒå€¼æ¨¡å‹é€æ­¥æé«˜å…¶æ€§èƒ½ï¼Œä½†ç”±äºæ ‡è®°ç”Ÿæˆçš„å¤æ‚æ€§ï¼Œè¿™ä¸€åŸåˆ™åœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­éš¾ä»¥å¤åˆ¶ã€‚

In conclusion, while MCTS can improve performance during inference when paired with a
pre-trained value model, iteratively boosting model performance through self-search remains a
significant challenge.

æ€»ä¹‹ï¼Œè™½ç„¶ MCTS åœ¨ä¸é¢„è®­ç»ƒå€¼æ¨¡å‹é…å¯¹æ—¶å¯ä»¥æé«˜æ¨ç†æ€§èƒ½ï¼Œä½†é€šè¿‡è‡ªæˆ‘æœç´¢è¿­ä»£æé«˜æ¨¡å‹æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚

## Conclusion, Limitation, and Future Workï¼ˆç»“è®ºã€å±€é™æ€§å’Œæœªæ¥å·¥ä½œï¼‰

In this work, we share our journey in enhancing model reasoning abilities through reinforcement
learning (RL). DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start
data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,
leveraging cold-start data alongside iterative RL fine-tuning. Ultimately, DeepSeek-R1 achieves
performance comparable to OpenAI-o1-1217 on a range of tasks.

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ†äº«äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ—…ç¨‹ã€‚DeepSeek-R1-Zero ä»£è¡¨äº†ä¸€ç§çº¯ RL æ–¹æ³•ï¼Œä¸ä¾èµ–å†·å¯åŠ¨æ•°æ®ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ã€‚DeepSeek-R1 æ›´å¼ºå¤§ï¼Œåˆ©ç”¨å†·å¯åŠ¨æ•°æ®å’Œè¿­ä»£ RL å¾®è°ƒã€‚æœ€ç»ˆï¼ŒDeepSeek-R1 åœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†ä¸ OpenAI-o1-1217 ç›¸å½“çš„æ€§èƒ½ã€‚

We further explore distillation the reasoning capability to small dense models. We use
DeepSeek-R1 as the teacher model to generate 800K data, and fine-tune several small dense
models. The results are promising: DeepSeek-R1-Distill-Qwen-1.5B outperforms GPT-4o and
Claude-3.5-Sonnet on math benchmarks with 28.9% on AIME and 83.9% on MATH. Other dense
models also achieve impressive results, significantly outperforming other instruction-tuned
models based on the same underlying checkpoints.

æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†å°†æ¨ç†èƒ½åŠ›è’¸é¦åˆ°å°å‹ç¨ å¯†æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨ DeepSeek-R1 ä½œä¸ºæ•™å¸ˆæ¨¡å‹ç”Ÿæˆ 80 ä¸‡æ•°æ®ï¼Œå¹¶å¾®è°ƒäº†å‡ ä¸ªå°å‹ç¨ å¯†æ¨¡å‹ã€‚ç»“æœæ˜¯ä»¤äººé¼“èˆçš„ï¼šDeepSeek-R1-Distill-Qwen-1.5B åœ¨æ•°å­¦åŸºå‡†ä¸Šçš„è¡¨ç°ä¼˜äº GPT-4o å’Œ Claude-3.5-Sonnetï¼ŒAIME ä¸Šä¸º 28.9%ï¼ŒMATH ä¸Šä¸º 83.9%ã€‚å…¶ä»–ç¨ å¯†æ¨¡å‹ä¹Ÿå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œæ˜æ˜¾ä¼˜äºåŸºäºç›¸åŒåŸºç¡€æ£€æŸ¥ç‚¹çš„å…¶ä»–åŸºäºæŒ‡ä»¤è°ƒæ•´çš„æ¨¡å‹ã€‚

In the future, we plan to invest in research across the following directions for DeepSeek-R1.

æœªæ¥ï¼Œæˆ‘ä»¬è®¡åˆ’åœ¨ä»¥ä¸‹æ–¹å‘ä¸Šä¸º DeepSeek-R1 è¿›è¡Œç ”ç©¶ã€‚

- **General Capability**: Currently, the capabilities of DeepSeek-R1 fall short of DeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and json output. Moving forward, we plan to explore how leveraging long CoT to enhance tasks in these fields.
- **é€šç”¨èƒ½åŠ›**ï¼šç›®å‰ï¼ŒDeepSeek-R1 åœ¨å‡½æ•°è°ƒç”¨ã€å¤šè½®ã€å¤æ‚è§’è‰²æ‰®æ¼”å’Œ json è¾“å‡ºç­‰ä»»åŠ¡ä¸­çš„èƒ½åŠ›ä¸åŠ DeepSeek-V3ã€‚æœªæ¥ï¼Œæˆ‘ä»¬è®¡åˆ’æ¢ç´¢å¦‚ä½•åˆ©ç”¨é•¿ CoT æ¥å¢å¼ºè¿™äº›é¢†åŸŸçš„ä»»åŠ¡ã€‚
- **Language Mixing**: DeepSeek-R1 is currently optimized for Chinese and English, which may result in language mixing issues when handling queries in other languages. For instance, DeepSeek-R1 might use English for reasoning and responses, even if the query is in a language other than English or Chinese. We aim to address this limitation in future updates.
- **è¯­è¨€æ··åˆ**ï¼šDeepSeek-R1 ç›®å‰é’ˆå¯¹ä¸­æ–‡å’Œè‹±æ–‡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œè¿™å¯èƒ½å¯¼è‡´å¤„ç†å…¶ä»–è¯­è¨€æŸ¥è¯¢æ—¶å‡ºç°è¯­è¨€æ··åˆé—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒDeepSeek-R1 å¯èƒ½åœ¨æŸ¥è¯¢ä¸æ˜¯è‹±æ–‡æˆ–ä¸­æ–‡çš„è¯­è¨€æ—¶ä½¿ç”¨è‹±æ–‡è¿›è¡Œæ¨ç†å’Œå“åº”ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥çš„æ›´æ–°ä¸­è§£å†³è¿™ä¸ªå±€é™æ€§ã€‚
- **Prompting Engineering**: When evaluating DeepSeek-R1, we observe that it is sensitive to prompts. Few-shot prompting consistently degrades its performance. Therefore, we recommend users directly describe the problem and specify the output format using a zero-shot setting for optimal results.
- **æç¤ºå·¥ç¨‹**ï¼šåœ¨è¯„ä¼° DeepSeek-R1 æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å®ƒå¯¹æç¤ºå¾ˆæ•æ„Ÿã€‚å°‘æ ·æœ¬æç¤ºä¼šæŒç»­é™ä½å…¶æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å»ºè®®ç”¨æˆ·ç›´æ¥æè¿°é—®é¢˜ï¼Œå¹¶ä½¿ç”¨é›¶å°„å‡»è®¾ç½®æŒ‡å®šè¾“å‡ºæ ¼å¼ä»¥è·å¾—æœ€ä½³ç»“æœã€‚
- **Software Engineering Tasks**: Due to the long evaluation times, which impact the efficiency of the RL process, large-scale RL has not been applied extensively in software engineering tasks. As a result, DeepSeek-R1 has not demonstrated a huge improvement over DeepSeek-V3 on software engineering benchmarks. Future versions will address this by implementing reject sampling on software engineering data or incorporating asynchronous evaluations during the RL process to improve efficiency.
- **è½¯ä»¶å·¥ç¨‹ä»»åŠ¡**ï¼šç”±äºé•¿æ—¶é—´çš„è¯„ä¼°æ—¶é—´å½±å“äº† RL è¿‡ç¨‹çš„æ•ˆç‡ï¼Œå¤§è§„æ¨¡ RL åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­å¹¶æ²¡æœ‰å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚å› æ­¤ï¼ŒDeepSeek-R1 åœ¨è½¯ä»¶å·¥ç¨‹åŸºå‡†ä¸Šå¹¶æ²¡æœ‰æ¯” DeepSeek-V3 æ˜¾è‘—æ”¹è¿›ã€‚æœªæ¥ç‰ˆæœ¬å°†é€šè¿‡åœ¨è½¯ä»¶å·¥ç¨‹æ•°æ®ä¸Šå®ç°æ‹’ç»é‡‡æ ·æˆ–åœ¨ RL è¿‡ç¨‹ä¸­å¼•å…¥å¼‚æ­¥è¯„ä¼°æ¥æé«˜æ•ˆç‡ã€‚
