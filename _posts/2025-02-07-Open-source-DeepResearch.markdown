---
layout: single
title:  "Open-source DeepResearch â€“ Freeing our search agents"
date:   2025-02-07 10:00:00 +0800
categories: DeepResearch Agent
tags: [DeepResearch, Agent, HuggingFace]
---

- [Open-source DeepResearch](https://huggingface.co/blog/open-deep-research)

## TLDR

Yesterday, OpenAI released [Deep Research](https://openai.com/index/introducing-deep-research/), a system that browses the web to summarize content and answer questions based on the summary. The system is impressive and blew our minds when we tried it for the first time.

æ˜¨å¤©ï¼ŒOpenAI å‘å¸ƒäº† [Deep Research](https://openai.com/index/introducing-deep-research/)ï¼Œè¿™æ˜¯ä¸€ä¸ªæµè§ˆç½‘é¡µä»¥æ€»ç»“å†…å®¹å¹¶æ ¹æ®æ€»ç»“å›ç­”é—®é¢˜çš„ç³»ç»Ÿã€‚å½“æˆ‘ä»¬ç¬¬ä¸€æ¬¡å°è¯•æ—¶ï¼Œè¿™ä¸ªç³»ç»Ÿç»™æˆ‘ä»¬ç•™ä¸‹äº†æ·±åˆ»çš„å°è±¡ã€‚

One of the main results in the blog post is a strong improvement of performances on the [General AI Assistants benchmark (GAIA)](https://huggingface.co/gaia-benchmark), a benchmark weâ€™ve been playing with recently as well, where they successfully reached near 67% correct answers on 1-shot on average, and 47.6% on especially challenging â€œlevel 3â€ questions that involve multiple steps of reasoning and tool usage (see below for a presentation of GAIA).

åšå®¢æ–‡ç« ä¸­çš„ä¸»è¦ç»“æœä¹‹ä¸€æ˜¯åœ¨[é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹åŸºå‡†ï¼ˆGAIAï¼‰](https://huggingface.co/gaia-benchmark)ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬æœ€è¿‘ä¹Ÿåœ¨ç©çš„åŸºå‡†ï¼Œä»–ä»¬æˆåŠŸåœ°åœ¨å¹³å‡ 1 æ¬¡å°è¯•ä¸­è¾¾åˆ°äº†è¿‘ 67% çš„æ­£ç¡®ç­”æ¡ˆï¼Œç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„â€œ3 çº§â€é—®é¢˜ä¸Šè¾¾åˆ°äº† 47.6%ï¼Œè¿™äº›é—®é¢˜æ¶‰åŠå¤šæ­¥æ¨ç†å’Œå·¥å…·ä½¿ç”¨ï¼ˆè¯·å‚è§ä¸‹æ–‡å…³äº GAIA çš„ä»‹ç»ï¼‰ã€‚

DeepResearch is composed of an LLM (which can be selected from the current list of LLMs provided by OpenAI, 4o, o1, o3, etc) and an internal â€œagentic frameworkâ€ which guide the LLM to use tools like web search and organize its actions in steps.

DeepResearch ç”±ä¸€ä¸ª LLMï¼ˆå¯ä»¥ä» OpenAI æä¾›çš„å½“å‰ LLM åˆ—è¡¨ä¸­é€‰æ‹©ï¼Œ4oã€o1ã€o3 ç­‰ï¼‰å’Œä¸€ä¸ªå†…éƒ¨â€œä»£ç†æ¡†æ¶â€ç»„æˆï¼Œè¯¥æ¡†æ¶æŒ‡å¯¼ LLM ä½¿ç”¨è¯¸å¦‚ç½‘ç»œæœç´¢ä¹‹ç±»çš„å·¥å…·ï¼Œå¹¶å°†å…¶æ“ä½œç»„ç»‡æˆæ­¥éª¤ã€‚

While powerful LLMs are now freely available in open-source (see e.g. [the recent DeepSeek R1 model](https://huggingface.co/deepseek-ai/DeepSeek-R1)), OpenAI didnâ€™t disclose much about the agentic framework underlying Deep Researchâ€¦

è™½ç„¶å¼ºå¤§çš„ LLM ç°åœ¨å¯ä»¥åœ¨å¼€æºä¸­å…è´¹è·å¾—ï¼ˆä¾‹å¦‚[æœ€è¿‘çš„ DeepSeek R1 æ¨¡å‹](https://huggingface.co/deepseek-ai/DeepSeek-R1)ï¼‰ï¼Œä½† OpenAI å¹¶æ²¡æœ‰é€éœ²æœ‰å…³ Deep Research åº•å±‚ä»£ç†æ¡†æ¶çš„å¤ªå¤šä¿¡æ¯...

So we decided to embark on a 24-hour mission to reproduce their results and open-source the needed framework along the way!

å› æ­¤ï¼Œæˆ‘ä»¬å†³å®šå¼€å§‹ä¸€é¡¹ä¸ºæœŸ 24 å°æ—¶çš„ä»»åŠ¡ï¼Œä»¥é‡ç°ä»–ä»¬çš„ç»“æœï¼Œå¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­å¼€æºæ‰€éœ€çš„æ¡†æ¶ï¼

The clock is ticking, letâ€™s go! â±ï¸

æ—¶é’Ÿæ»´ç­”ä½œå“ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼â±ï¸


## What are Agent frameworks and why they matter?ï¼ˆä»£ç†æ¡†æ¶æ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆé‡è¦ï¼Ÿï¼‰

An Agent framework is a layer on top of an LLM to make said LLM execute actions (like browse the web or read PDF documents), and organize its operations in a series of steps. For a quick intro to agents, check [this great interview by Andrew Ng](https://youtu.be/sal78ACtGTc?feature=shared&t=52) and our [introduction blog post](https://huggingface.co/blog/smolagents) to the smolagents library. For a more detailed dive in agents you can subscribe to our agents course that starts in just a few days: [link here](https://huggingface.us17.list-manage.com/subscribe?u=7f57e683fa28b51bfc493d048&id=9ed45a3ef6).

ä»£ç†æ¡†æ¶æ˜¯åœ¨ LLM ä¹‹ä¸Šçš„ä¸€å±‚ï¼Œä½¿è¯¥ LLM æ‰§è¡Œæ“ä½œï¼ˆå¦‚æµè§ˆç½‘é¡µæˆ–é˜…è¯» PDF æ–‡æ¡£ï¼‰ï¼Œå¹¶å°†å…¶æ“ä½œç»„ç»‡æˆä¸€ç³»åˆ—æ­¥éª¤ã€‚è¦å¿«é€Ÿäº†è§£ä»£ç†ï¼Œè¯·æŸ¥çœ‹[Andrew Ng çš„è¿™æ¬¡å¾ˆæ£’çš„é‡‡è®¿](https://youtu.be/sal78ACtGTc?feature=shared&t=52)ä»¥åŠæˆ‘ä»¬çš„[ä»‹ç»åšå®¢æ–‡ç« ](https://huggingface.co/blog/smolagents)åˆ° smolagents åº“ã€‚è¦æ·±å…¥äº†è§£ä»£ç†ï¼Œæ‚¨å¯ä»¥è®¢é˜…æˆ‘ä»¬çš„ä»£ç†è¯¾ç¨‹ï¼Œè¯¥è¯¾ç¨‹å°†åœ¨å‡ å¤©åå¼€å§‹ï¼š[é“¾æ¥åœ¨è¿™é‡Œ](https://huggingface.us17.list-manage.com/subscribe?u=7f57e683fa28b51bfc493d048&id=9ed45a3ef6)ã€‚

Almost everyone has already experienced how powerful LLMs can be simply by playing with chatbots.. However, what not everyone is aware of yet is that integrating these LLMs into agentic systems can give them real superpowers!

å‡ ä¹æ¯ä¸ªäººéƒ½å·²ç»é€šè¿‡ä¸èŠå¤©æœºå™¨äººç©è€ä½“éªŒåˆ°äº† LLM çš„å¼ºå¤§ä¹‹å¤„ã€‚ç„¶è€Œï¼Œè¿˜ä¸æ˜¯æ¯ä¸ªäººéƒ½æ„è¯†åˆ°çš„æ˜¯ï¼Œå°†è¿™äº› LLM é›†æˆåˆ°ä»£ç†ç³»ç»Ÿä¸­å¯ä»¥èµ‹äºˆå®ƒä»¬çœŸæ­£çš„è¶…èƒ½åŠ›ï¼

Here is a recent example comparing the performance of a few frontier LLMs with and without an agentic framework (in this case the simple [smolagents](https://github.com/huggingface/smolagents) library) - using an agentic framework bumps performance by up to 60 points!

è¿™æ˜¯ä¸€ä¸ªæœ€è¿‘çš„ä¾‹å­ï¼Œæ¯”è¾ƒäº†ä¸€äº›å‰æ²¿ LLM çš„æ€§èƒ½ï¼Œæœ‰çš„æœ‰ä»£ç†æ¡†æ¶ï¼Œæœ‰çš„æ²¡æœ‰ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯ç®€å•çš„ [smolagents](https://github.com/huggingface/smolagents) åº“ï¼‰- ä½¿ç”¨ä»£ç†æ¡†æ¶å¯ä»¥å°†æ€§èƒ½æé«˜å¤šè¾¾ 60 åˆ†ï¼

![](/images/2025/OpenDeepResearch/Figure1.png)

In fact, OpenAI also highlighted in [its release blogpost](https://openai.com/index/introducing-deep-research/) how Deep Research performed dramatically better than standalone LLMs on the knowledge-intensive "[Humanityâ€™s Last Exam](https://huggingface.co/datasets/cais/hle)" benchmark.

äº‹å®ä¸Šï¼ŒOpenAI åœ¨[å‘å¸ƒåšå®¢æ–‡ç« ](https://openai.com/index/introducing-deep-research/)ä¸­ä¹Ÿå¼ºè°ƒäº† Deep Research åœ¨çŸ¥è¯†å¯†é›†å‹çš„â€œ[äººç±»æœ€åä¸€æ¬¡è€ƒè¯•](https://huggingface.co/datasets/cais/hle)â€åŸºå‡†ä¸Šçš„è¡¨ç°è¿œè¿œä¼˜äºç‹¬ç«‹çš„ LLMã€‚

So, what happens when we integrate our current top LLM in an agentic framework, to work toward an `open-DeepResearch` ?

é‚£ä¹ˆï¼Œå½“æˆ‘ä»¬å°†å½“å‰çš„é¡¶çº§ LLM é›†æˆåˆ°ä»£ç†æ¡†æ¶ä¸­æ—¶ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Œä»¥ä¾¿æœç€ `open-DeepResearch` è¿ˆè¿›ï¼Ÿ

A quick note: Weâ€™ll benchmark our results on the same GAIA challenge but keep in mind that this is a work in progress. DeepResearch is a massive achievement and its open reproduction will take time. In particular, full parity will require improved browser use and interaction like OpenAI Operator is providing, i.e. beyond the current text-only web interaction we explore in this first step.

ä¸€ä¸ªå¿«é€Ÿçš„è¯´æ˜ï¼šæˆ‘ä»¬å°†åœ¨ç›¸åŒçš„ GAIA æŒ‘æˆ˜ä¸Šå¯¹æˆ‘ä»¬çš„ç»“æœè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä½†è¯·è®°ä½è¿™æ˜¯ä¸€ä¸ªæ­£åœ¨è¿›è¡Œçš„å·¥ä½œã€‚DeepResearch æ˜¯ä¸€ä¸ªå·¨å¤§çš„æˆå°±ï¼Œå…¶å¼€æ”¾æ€§å¤åˆ¶å°†éœ€è¦æ—¶é—´ã€‚ç‰¹åˆ«æ˜¯ï¼Œå®Œå…¨çš„å¯¹ç­‰æ€§å°†éœ€è¦æ”¹è¿›çš„æµè§ˆå™¨ä½¿ç”¨å’Œäº¤äº’ï¼Œå°±åƒ OpenAI Operator æä¾›çš„é‚£æ ·ï¼Œå³è¶…å‡ºæˆ‘ä»¬åœ¨è¿™ä¸€ç¬¬ä¸€æ­¥ä¸­æ¢ç´¢çš„å½“å‰çº¯æ–‡æœ¬ç½‘ç»œäº¤äº’ã€‚

Letâ€™s first understand the scope of the challenge: GAIA.

é¦–å…ˆè®©æˆ‘ä»¬äº†è§£æŒ‘æˆ˜çš„èŒƒå›´ï¼šGAIAã€‚


## The GAIA benchmark

[GAIA](https://huggingface.co/datasets/gaia-benchmark/GAIA) is arguably the most comprehensive benchmark for agents. Its questions are very difficult and hit on many challenges of LLM-based systems. Here is an example of a hard question:

[GAIA](https://huggingface.co/datasets/gaia-benchmark/GAIA) å¯èƒ½æ˜¯æœ€å…¨é¢çš„ä»£ç†åŸºå‡†ã€‚å®ƒçš„é—®é¢˜éå¸¸å›°éš¾ï¼Œæ¶‰åŠåˆ° LLM ç³»ç»Ÿçš„è®¸å¤šæŒ‘æˆ˜ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªéš¾é¢˜çš„ä¾‹å­ï¼š

> "Which of the fruits shown in the 2008 painting "Embroidery from Uzbekistan" were served as part of the October 1949 breakfast menu for the ocean liner that was later used as a floating prop for the film "The Last Voyage"? Give the items as a comma-separated list, ordering them in clockwise order based on their arrangement in the painting starting from the 12 o'clock position. Use the plural form of each fruit."

> â€œ2008 å¹´çš„ç”»ä½œã€Šä¹Œå…¹åˆ«å…‹æ–¯å¦çš„åˆºç»£ã€‹ä¸­å±•ç¤ºçš„æ°´æœä¸­ï¼Œå“ªäº›æ°´æœä½œä¸º 1949 å¹´ 10 æœˆçš„æ—©é¤èœå•çš„ä¸€éƒ¨åˆ†ä¾›åº”ç»™äº†åæ¥è¢«ç”¨ä½œç”µå½±ã€Šæœ€åçš„èˆªè¡Œã€‹ä¸­çš„æ¼‚æµ®é“å…·çš„å®¢è½®ï¼Ÿå°†è¿™äº›é¡¹ç›®ä½œä¸ºé€—å·åˆ†éš”çš„åˆ—è¡¨ç»™å‡ºï¼Œæ ¹æ®ç”»ä½œä¸­çš„æ’åˆ—é¡ºåºï¼Œä» 12 ç‚¹é’Ÿä½ç½®å¼€å§‹ï¼ŒæŒ‰é¡ºæ—¶é’ˆé¡ºåºæ’åˆ—ã€‚ä½¿ç”¨æ¯ç§æ°´æœçš„å¤æ•°å½¢å¼ã€‚â€

You can see this question involves several challenges:ï¼ˆä½ å¯ä»¥çœ‹åˆ°è¿™ä¸ªé—®é¢˜æ¶‰åŠåˆ°äº†å‡ ä¸ªæŒ‘æˆ˜ï¼šï¼‰

- Answering in a constrained format,ï¼ˆä»¥å—é™åˆ¶çš„æ ¼å¼å›ç­”ï¼‰
- Using multimodal capabilities (to extract the fruits from the image),ï¼ˆä½¿ç”¨å¤šæ¨¡æ€åŠŸèƒ½ï¼ˆä»å›¾åƒä¸­æå–æ°´æœï¼‰ï¼‰
- Gathering several pieces of information, some depending on others:ï¼ˆæ”¶é›†å‡ ä¸ªä¿¡æ¯ç‰‡æ®µï¼Œæœ‰äº›å–å†³äºå…¶ä»–ä¿¡æ¯ï¼šï¼‰
    - Identifying the fruits on the pictureï¼ˆè¯†åˆ«å›¾ç‰‡ä¸Šçš„æ°´æœï¼‰
    - Finding which ocean liner was used as a floating prop for â€œThe Last Voyageâ€ï¼ˆæ‰¾åˆ°å“ªè‰˜å®¢è½®è¢«ç”¨ä½œã€Šæœ€åçš„èˆªè¡Œã€‹çš„æ¼‚æµ®é“å…·ï¼‰
    - Finding the October 1949 breakfast menu for the above ocean linerï¼ˆæ‰¾åˆ°ä¸Šè¿°å®¢è½®çš„ 1949 å¹´ 10 æœˆæ—©é¤èœå•ï¼‰
- Chaining together a problem-solving trajectory in the correct order.ï¼ˆæŒ‰æ­£ç¡®é¡ºåºå°†é—®é¢˜è§£å†³è½¨è¿¹é“¾æ¥åœ¨ä¸€èµ·ã€‚ï¼‰

Solving this requires both high-level planning abilities and rigorous execution, which are two areas where LLMs struggle when used alone.

è§£å†³è¿™ä¸ªé—®é¢˜éœ€è¦**é«˜æ°´å¹³çš„è§„åˆ’èƒ½åŠ›**å’Œ**ä¸¥æ ¼çš„æ‰§è¡Œ**ï¼Œè¿™æ˜¯ LLM å•ç‹¬ä½¿ç”¨æ—¶å›°éš¾çš„ä¸¤ä¸ªé¢†åŸŸã€‚

So itâ€™s an excellent test set for agent systems!

å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªä¼˜ç§€çš„ä»£ç†ç³»ç»Ÿæµ‹è¯•é›†ï¼

On GAIAâ€™s [public leaderboard](https://huggingface.co/spaces/gaia-benchmark/leaderboard), GPT-4 does not even reach 7% on the validation set when used without any agentic setup. On the other side of the spectrum, with Deep Research, OpenAI reached 67.36% score on the validation set, so an order of magnitude better! (Though we donâ€™t know how they would actually fare on the private test set.)

åœ¨ GAIA çš„[å…¬å…±æ’è¡Œæ¦œ](https://huggingface.co/spaces/gaia-benchmark/leaderboard)ä¸Šï¼ŒGPT-4 åœ¨æ²¡æœ‰ä»»ä½•ä»£ç†è®¾ç½®çš„æƒ…å†µä¸‹ç”šè‡³æ²¡æœ‰è¾¾åˆ°éªŒè¯é›†çš„ 7%ã€‚å¦ä¸€æ–¹é¢ï¼Œé€šè¿‡ Deep Researchï¼ŒOpenAI åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°äº† 67.36% çš„åˆ†æ•°ï¼Œå› æ­¤å¥½äº†ä¸€ä¸ªæ•°é‡çº§ï¼ï¼ˆå°½ç®¡æˆ‘ä»¬ä¸çŸ¥é“ä»–ä»¬åœ¨ç§æœ‰æµ‹è¯•é›†ä¸Šçš„å®é™…è¡¨ç°ã€‚ï¼‰

Letâ€™s see if we can do better with open source tools!

è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦å¯ä»¥ä½¿ç”¨å¼€æºå·¥å…·åšå¾—æ›´å¥½ï¼


## Building an open Deep Research

### Using a CodeAgent

The first improvement over traditional AI agent systems weâ€™ll tackle is to use a so-called â€œcode agentâ€. As shown by [Wang et al. (2024)](https://huggingface.co/papers/2402.01030), letting the agent express its actions in code has several advantages, but most notably that code is specifically designed to express complex sequences of actions.

æˆ‘ä»¬å°†é¦–å…ˆè§£å†³ä¼ ç»Ÿ AI ä»£ç†ç³»ç»Ÿçš„ä¸€ä¸ªæ”¹è¿›ï¼Œå³ä½¿ç”¨æ‰€è°“çš„â€œä»£ç ä»£ç†â€ã€‚æ­£å¦‚[Wang ç­‰äººï¼ˆ2024ï¼‰](https://huggingface.co/papers/2402.01030)æ‰€ç¤ºï¼Œè®©ä»£ç†ä»¥ä»£ç å½¢å¼è¡¨è¾¾å…¶æ“ä½œå…·æœ‰å‡ ä¸ªä¼˜ç‚¹ï¼Œä½†æœ€é‡è¦çš„æ˜¯ä»£ç ä¸“é—¨è®¾è®¡ç”¨äºè¡¨è¾¾å¤æ‚çš„æ“ä½œåºåˆ—ã€‚

Consider this example given by Wang et al.:

è€ƒè™‘ Wang ç­‰äººç»™å‡ºçš„è¿™ä¸ªä¾‹å­ï¼š

![](/images/2025/OpenDeepResearch/Figure2.png)

This highlights several advantages of using code:ï¼ˆè¿™çªå‡ºäº†ä½¿ç”¨ä»£ç çš„å‡ ä¸ªä¼˜ç‚¹ï¼šï¼‰

- Code actions are much more concise than JSON.ï¼ˆä»£ç æ“ä½œæ¯” JSON æ›´ç®€æ´ã€‚ï¼‰
    - Need to run 4 parallel streams of 5 consecutive actions ? In JSON, you would need to generate 20 JSON blobs, each in their separate step; in Code itâ€™s only 1 step.
    - éœ€è¦è¿è¡Œ 4 ä¸ªå¹¶è¡Œæµçš„ 5 ä¸ªè¿ç»­æ“ä½œï¼Ÿåœ¨ JSON ä¸­ï¼Œæ‚¨éœ€è¦ç”Ÿæˆ 20 ä¸ª JSON blobï¼Œæ¯ä¸ªåœ¨å…¶å•ç‹¬çš„æ­¥éª¤ä¸­ï¼›åœ¨ä»£ç ä¸­åªéœ€è¦ 1 æ­¥ã€‚
    - On average, the paper shows that Code actions require 30% fewer steps than JSON, which amounts to an equivalent reduction in the tokens generated. Since LLM calls are often the dimensioning cost of agent systems, it means your agent system runs are ~30% cheaper.
    - å¹³å‡è€Œè¨€ï¼Œè¯¥è®ºæ–‡æ˜¾ç¤ºï¼Œä»£ç æ“ä½œæ¯” JSON æ“ä½œéœ€è¦çš„æ­¥éª¤å°‘ 30%ï¼Œè¿™ç›¸å½“äºç”Ÿæˆçš„ä»¤ç‰Œæ•°é‡å‡å°‘ã€‚ç”±äº LLM è°ƒç”¨é€šå¸¸æ˜¯ä»£ç†ç³»ç»Ÿçš„ç»´åº¦æˆæœ¬ï¼Œè¿™æ„å‘³ç€æ‚¨çš„ä»£ç†ç³»ç»Ÿè¿è¡Œæˆæœ¬é™ä½äº†çº¦ 30%ã€‚
- Code enables to re-use tools from common librariesï¼ˆä»£ç å¯ä»¥é‡ç”¨å¸¸è§åº“ä¸­çš„å·¥å…·ï¼‰
- Better performance in benchmarks, due to two reasons:ï¼ˆç”±äºä¸¤ä¸ªåŸå› ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´å¥½ï¼šï¼‰
    - More intuitive way to express actionsï¼ˆæ›´ç›´è§‚åœ°è¡¨è¾¾æ“ä½œçš„æ–¹å¼ï¼‰
    - Extensive exposure of LLMs to code in trainingï¼ˆLLMs åœ¨è®­ç»ƒä¸­å¹¿æ³›æ¥è§¦ä»£ç ï¼‰

The advantages above were confirmed by our experiments on the [agent_reasoning_benchmark](https://github.com/aymeric-roucher/agent_reasoning_benchmark).

ä¸Šè¿°ä¼˜ç‚¹å·²ç»é€šè¿‡æˆ‘ä»¬åœ¨ [agent_reasoning_benchmark](https://github.com/aymeric-roucher/agent_reasoning_benchmark) ä¸Šçš„å®éªŒå¾—åˆ°äº†è¯å®ã€‚

From building `smolagents` we can also cite a notable additional advantage, which is a better handling of state: this is very useful for multimodal tasks in particular. Need to store this image/audio/other for later use? No problem, just assign it as a variable in your state and you can re-use it 4 steps later if needed. In JSON you would have to let the LLM name it in a dictionary key and trust the LLM will later understand that it can still use it.

ä»æ„å»º `smolagents` æˆ‘ä»¬è¿˜å¯ä»¥å¼•ç”¨ä¸€ä¸ªæ˜¾è‘—çš„é¢å¤–ä¼˜åŠ¿ï¼Œå³æ›´å¥½åœ°å¤„ç†çŠ¶æ€ï¼šè¿™å¯¹äºå¤šæ¨¡æ€ä»»åŠ¡ç‰¹åˆ«æœ‰ç”¨ã€‚éœ€è¦å­˜å‚¨è¿™ä¸ªå›¾åƒ/éŸ³é¢‘/å…¶ä»–ä»¥ä¾›ä»¥åä½¿ç”¨ï¼Ÿæ²¡é—®é¢˜ï¼Œåªéœ€å°†å…¶åˆ†é…ä¸ºçŠ¶æ€ä¸­çš„å˜é‡ï¼Œå¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥åœ¨ 4 æ­¥åé‡æ–°ä½¿ç”¨å®ƒã€‚åœ¨ JSON ä¸­ï¼Œæ‚¨å¿…é¡»è®© LLM åœ¨å­—å…¸é”®ä¸­å‘½åå®ƒï¼Œå¹¶ç›¸ä¿¡ LLM ä»¥åä¼šæ˜ç™½å®ƒä»ç„¶å¯ä»¥ä½¿ç”¨å®ƒã€‚

### Making the right tools ğŸ› ï¸

Now we need to provide the agent with the right set of tools.

ç°åœ¨æˆ‘ä»¬éœ€è¦ä¸ºä»£ç†æä¾›æ­£ç¡®çš„å·¥å…·é›†ã€‚

1. A web browser. While a fully fledged web browser interaction like [Operator](https://openai.com/index/introducing-operator/) will be needed to reach full performance, we started with an extremely simple text-based web browser for now for our first proof-of-concept. You can find the code [here](https://github.com/huggingface/smolagents/blob/gaia-submission-r1/examples/open_deep_research/scripts/text_web_browser.py).

    ä¸€ä¸ªç½‘ç»œæµè§ˆå™¨ã€‚è™½ç„¶éœ€è¦åƒ[Operator](https://openai.com/index/introducing-operator/)è¿™æ ·çš„å®Œæ•´ç½‘ç»œæµè§ˆå™¨äº¤äº’æ‰èƒ½è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä½†æˆ‘ä»¬ç›®å‰é¦–å…ˆä»ä¸€ä¸ªæå…¶ç®€å•çš„åŸºäºæ–‡æœ¬çš„ç½‘ç»œæµè§ˆå™¨å¼€å§‹ï¼Œç”¨äºæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæ¦‚å¿µéªŒè¯ã€‚æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/smolagents/blob/gaia-submission-r1/examples/open_deep_research/scripts/text_web_browser.py)æ‰¾åˆ°ä»£ç ã€‚

2. A simple text inspector, to be able to **read a bunch of text file format**, find it [here](https://github.com/huggingface/smolagents/blob/gaia-submission-r1/examples/open_deep_research/scripts/text_inspector_tool.py).

    ä¸€ä¸ªç®€å•çš„æ–‡æœ¬æ£€æŸ¥å™¨ï¼Œç”¨äº**é˜…è¯»ä¸€å †æ–‡æœ¬æ–‡ä»¶æ ¼å¼**ï¼Œå¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/huggingface/smolagents/blob/gaia-submission-r1/examples/open_deep_research/scripts/text_inspector_tool.py)æ‰¾åˆ°ã€‚

These tools were taken from the excellent [Magentic-One](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/) agent by Microsoft Research, kudos to them! We didnâ€™t change them much, as our goal was to get as high a performance as we can with the lowest complexity possible.

è¿™äº›å·¥å…·å–è‡ªå¾®è½¯ç ”ç©¶çš„å‡ºè‰²çš„[Magentic-One](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)ä»£ç†ï¼Œå‘ä»–ä»¬è‡´æ•¬ï¼æˆ‘ä»¬æ²¡æœ‰åšå¤ªå¤šæ”¹åŠ¨ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»¥å°½å¯èƒ½ä½çš„å¤æ‚æ€§è·å¾—å°½å¯èƒ½é«˜çš„æ€§èƒ½ã€‚

Here is a short roadmap of improvements which we feel would really improve these toolsâ€™ performance (feel free to open a PR and contribute!):

è¿™æ˜¯æˆ‘ä»¬è®¤ä¸ºå°†çœŸæ­£æé«˜è¿™äº›å·¥å…·æ€§èƒ½çš„ä¸€äº›æ”¹è¿›çš„ç®€çŸ­è·¯çº¿å›¾ï¼ˆæ¬¢è¿æ‰“å¼€ PR å¹¶è´¡çŒ®ï¼ï¼‰ï¼š

- extending the number of file formats which can be read.ï¼ˆæ‰©å±•å¯ä»¥è¯»å–çš„æ–‡ä»¶æ ¼å¼æ•°é‡ã€‚ï¼‰
- proposing a more fine-grained handling of files.ï¼ˆæå‡ºæ›´ç»†ç²’åº¦çš„æ–‡ä»¶å¤„ç†ã€‚ï¼‰
- replacing the web browser with a vision-based one, which weâ€™ve started doing [here](https://github.com/huggingface/smolagents/blob/gaia-submission-r1/src/smolagents/vision_web_browser.py).ï¼ˆç”¨åŸºäºè§†è§‰çš„ç½‘ç»œæµè§ˆå™¨æ›¿æ¢ï¼Œæˆ‘ä»¬å·²ç»å¼€å§‹åœ¨[è¿™é‡Œ](https://github.com/huggingface/smolagents/blob/gaia-submission-r1/src/smolagents/vision_web_browser.py)åšäº†ã€‚ï¼‰


## Results ğŸ…
In our 24h+ reproduction sprint, weâ€™ve already seen steady improvements in the performance of our agent on GAIA!

åœ¨æˆ‘ä»¬ 24 å°æ—¶ä»¥ä¸Šçš„å¤åˆ¶å†²åˆºä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°æˆ‘ä»¬çš„ä»£ç†åœ¨ GAIA ä¸Šçš„æ€§èƒ½ç¨³æ­¥æé«˜ï¼

Weâ€™ve quickly gone up from the previous SoTA with an open framework, around 46% for Magentic-One, to our [current performance of 55.15% on the validation set](https://huggingface.co/spaces/gaia-benchmark/leaderboard).

æˆ‘ä»¬å¾ˆå¿«ä»ä»¥å‰çš„ SoTAï¼Œå¤§çº¦ 46% çš„ Magentic-Oneï¼Œä¸Šå‡åˆ°æˆ‘ä»¬[ç›®å‰åœ¨éªŒè¯é›†ä¸Šçš„ 55.15% çš„æ€§èƒ½](https://huggingface.co/spaces/gaia-benchmark/leaderboard)ã€‚

This bump in performance is due mostly to letting our agents write their actions in code! Indeed, when switching to a standard agent that writes actions in JSON instead of code, performance of the same setup is instantly degraded to 33% average on the validation set.

è¿™ç§æ€§èƒ½æå‡ä¸»è¦æ˜¯å› ä¸ºè®©æˆ‘ä»¬çš„ä»£ç†ä»¥ä»£ç å½¢å¼ç¼–å†™å…¶æ“ä½œï¼å®é™…ä¸Šï¼Œå½“åˆ‡æ¢åˆ°ä¸€ä¸ªæ ‡å‡†ä»£ç†ï¼Œè¯¥ä»£ç†ä»¥ JSON è€Œä¸æ˜¯ä»£ç å½¢å¼ç¼–å†™æ“ä½œæ—¶ï¼Œç›¸åŒè®¾ç½®çš„æ€§èƒ½ç«‹å³é™çº§åˆ°éªŒè¯é›†ä¸Šçš„å¹³å‡ 33%ã€‚

[Here is the final agentic system.](https://github.com/huggingface/smolagents/tree/gaia-submission-r1/examples/open_deep_research)

[è¿™æ˜¯æœ€ç»ˆçš„ä»£ç†ç³»ç»Ÿã€‚](https://github.com/huggingface/smolagents/tree/gaia-submission-r1/examples/open_deep_research)

Weâ€™ve set up [a live demo](https://m-ric-open-deep-research.hf.space/) here for you to try it out!

æˆ‘ä»¬åœ¨è¿™é‡Œä¸ºæ‚¨è®¾ç½®äº†[ä¸€ä¸ªå®æ—¶æ¼”ç¤º](https://m-ric-open-deep-research.hf.space/)ï¼Œä¾›æ‚¨å°è¯•ï¼

However, this is only the beginning, and there are a lot of things to improve! Our open tools can be made better, the smolagents framework can also be tuned, and weâ€™d love to explore the performance of better open models to support the agent.

ç„¶è€Œï¼Œè¿™åªæ˜¯ä¸€ä¸ªå¼€å§‹ï¼Œè¿˜æœ‰å¾ˆå¤šäº‹æƒ…éœ€è¦æ”¹è¿›ï¼æˆ‘ä»¬çš„å¼€æ”¾å·¥å…·å¯ä»¥åšå¾—æ›´å¥½ï¼Œsmolagents æ¡†æ¶ä¹Ÿå¯ä»¥è°ƒæ•´ï¼Œæˆ‘ä»¬å¾ˆä¹æ„æ¢ç´¢æ›´å¥½çš„å¼€æ”¾æ¨¡å‹çš„æ€§èƒ½ï¼Œä»¥æ”¯æŒä»£ç†ã€‚

We welcome the community to come join us in this endeavour, so we can leverage the power of open research together to build a great open-source agentic framework! It would allow anyone to run a DeepResearch-like agent at home, with their favorite models, using a completely local and customized approach!

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºåŠ å…¥æˆ‘ä»¬çš„åŠªåŠ›ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å…±åŒåˆ©ç”¨å¼€æ”¾ç ”ç©¶çš„åŠ›é‡æ¥æ„å»ºä¸€ä¸ªä¼Ÿå¤§çš„å¼€æºä»£ç†æ¡†æ¶ï¼è¿™å°†å…è®¸ä»»ä½•äººåœ¨å®¶ä¸­è¿è¡Œç±»ä¼¼ DeepResearch çš„ä»£ç†ï¼Œä½¿ç”¨ä»–ä»¬å–œæ¬¢çš„æ¨¡å‹ï¼Œé‡‡ç”¨å®Œå…¨æœ¬åœ°åŒ–å’Œå®šåˆ¶åŒ–çš„æ–¹æ³•ï¼


## Community Reproductionsï¼ˆç¤¾åŒºå¤åˆ¶ï¼‰

While we were working on this and focusing on GAIA, other great open implementations of Deep Research emerged from the community, specifically from

å½“æˆ‘ä»¬åœ¨è¿™æ–¹é¢å·¥ä½œå¹¶ä¸“æ³¨äº GAIA æ—¶ï¼Œå…¶ä»–ä¼Ÿå¤§çš„ Deep Research å¼€æ”¾å®ç°ä»ç¤¾åŒºä¸­å‡ºç°ï¼Œç‰¹åˆ«æ˜¯æ¥è‡ª

- [dzhng](https://x.com/dzhng/status/1886603396578484630),
- [assafelovic](https://github.com/assafelovic/gpt-researcher),
- [nickscamara](https://github.com/nickscamara/open-deep-research),
- [jina-ai](https://github.com/jina-ai/node-DeepResearch) and
- [mshumer](https://x.com/mattshumer_/status/1886558939434664404).

Each of these implementations use different libraries for indexing data, browsing the web and querying LLMs. In this project, we would like to **reproduce the benchmarks presented by OpenAI (pass@1 average score), benchmark and document our findings with switching to open LLMs (like DeepSeek R1), using vision LMs, benchmark traditional tool calling against code-native agents.**

è¿™äº›å®ç°ä½¿ç”¨ä¸åŒçš„åº“æ¥ç´¢å¼•æ•°æ®ã€æµè§ˆç½‘ç»œå’ŒæŸ¥è¯¢ LLMsã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›**å¤åˆ¶ OpenAI æå‡ºçš„åŸºå‡†ï¼ˆpass@1 å¹³å‡åˆ†ï¼‰ï¼Œå¹¶ä½¿ç”¨å¼€æ”¾ LLMsï¼ˆå¦‚ DeepSeek R1ï¼‰ï¼Œä½¿ç”¨è§†è§‰ LMsï¼Œä½¿ç”¨ä¼ ç»Ÿå·¥å…·è°ƒç”¨ä¸ä»£ç æœ¬åœ°ä»£ç†è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶è®°å½•æˆ‘ä»¬çš„å‘ç°ã€‚**


## Most important next stepsï¼ˆæœ€é‡è¦çš„ä¸‹ä¸€æ­¥ï¼‰

OpenAIâ€™s Deep Research is probably boosted by the excellent web browser that they introduced with [Operator](https://openai.com/index/introducing-operator/).

OpenAI çš„ Deep Research å¯èƒ½å—ç›Šäºä»–ä»¬åœ¨[Operator](https://openai.com/index/introducing-operator/)ä¸­å¼•å…¥çš„å‡ºè‰²ç½‘ç»œæµè§ˆå™¨ã€‚

So weâ€™re tackling that next! In a more general problem: weâ€™re going to build GUI agents, i.e. â€œagents that view your screen and can act directly with mouse & keyboardâ€. If youâ€™re excited about this project, and want to help everyone get access to such cool capabilities through open source, weâ€™d love to get your contribution!

æ‰€ä»¥ä¸‹ä¸€æ­¥æˆ‘ä»¬è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼åœ¨ä¸€ä¸ªæ›´ä¸€èˆ¬çš„é—®é¢˜ä¸Šï¼šæˆ‘ä»¬å°†æ„å»º GUI ä»£ç†ï¼Œå³â€œæŸ¥çœ‹æ‚¨çš„å±å¹•å¹¶å¯ä»¥ç›´æ¥ä½¿ç”¨é¼ æ ‡å’Œé”®ç›˜æ“ä½œçš„ä»£ç†â€ã€‚å¦‚æœæ‚¨å¯¹è¿™ä¸ªé¡¹ç›®æ„Ÿåˆ°å…´å¥‹ï¼Œå¹¶å¸Œæœ›é€šè¿‡å¼€æºå¸®åŠ©æ¯ä¸ªäººè·å¾—è¿™äº›å¾ˆé…·çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å¾ˆä¹æ„å¾—åˆ°æ‚¨çš„è´¡çŒ®ï¼

Weâ€™re also [hiring a full time engineer](https://apply.workable.com/huggingface/j/AF1D4E3FEB/) to help us work on this and more, apply if youâ€™re interested ğŸ™‚

æˆ‘ä»¬è¿˜[æ‹›è˜å…¨èŒå·¥ç¨‹å¸ˆ](https://apply.workable.com/huggingface/j/AF1D4E3FEB/)æ¥å¸®åŠ©æˆ‘ä»¬å¼€å±•è¿™é¡¹å·¥ä½œä»¥åŠæ›´å¤šå·¥ä½œï¼Œå¦‚æœæ‚¨æ„Ÿå…´è¶£ï¼Œè¯·ç”³è¯· ğŸ™‚

- To get started with Open Deep Research, try the examples [here](https://github.com/huggingface/smolagents/tree/gaia-submission-r1/examples/open_deep_research).ï¼ˆè¦å¼€å§‹ä½¿ç”¨ Open Deep Researchï¼Œè¯·å°è¯•[è¿™é‡Œ](https://github.com/huggingface/smolagents/tree/gaia-submission-r1/examples/open_deep_research)çš„ç¤ºä¾‹ã€‚ï¼‰
- Check the [smolagents](https://github.com/huggingface/smolagents) repo.ï¼ˆæŸ¥çœ‹[smolagents](https://github.com/huggingface/smolagents)å­˜å‚¨åº“ã€‚ï¼‰
- Read more about smolagents [docs](https://huggingface.co/docs/smolagents/index), [introduction blog post](https://huggingface.co/blog/smolagents).ï¼ˆé˜…è¯»æœ‰å…³ smolagents çš„æ›´å¤š[æ–‡æ¡£](https://huggingface.co/docs/smolagents/index)ï¼Œ[ä»‹ç»åšå®¢æ–‡ç« ](https://huggingface.co/blog/smolagents)ã€‚ï¼‰
